{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57b6854",
   "metadata": {},
   "source": [
    "# 개발용 코드 프레임\n",
    "\n",
    "### 구성(셀)\n",
    "1. 모듈 임포트\n",
    "- 한번 실행하고 삭제\n",
    "2. 기본 설정\n",
    "필요한 모듈 임포트와 로깅 설정 담당. 가급적 수정하지 말것.\n",
    "3. DevAnalyzer 클래스\n",
    "아래 두 개의 메서드를 완성하는 것이 목표!\n",
    "- _filter_data\n",
    "- _generate_analysis_result\n",
    "4. 테스트용 코드\n",
    "- 사용하고자 하는 task_id를 직접 입력하고, 원하는 테스트의 주석만 해제하여 사용.\n",
    "- task_id 이외에 직접적인 수정 하지 말 것.\n",
    "\n",
    "### 디버깅\n",
    "- 코드를 작성한 후 테스트해보고 싶으면 **restart** -> **run all** 하면 됨.\n",
    "- 우선은 최대한 로그를 찍어두었으니 나오는 코드를 활용할 것.\n",
    "- 개발할때도 로거를 이용해서 디버깅하길\n",
    "- 출력도 찍었지만 동일 디렉토리에 analyzer_debug.log가 생성되니 참고하시길\n",
    "- 로그 파일은 1번셀 실행시 매번 초기화되니 주의."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee115e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unknown-data in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unknown-data) (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unknown-data) (2.3.3)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unknown-data) (4.25.1)\n",
      "Requirement already satisfied: boto3 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unknown-data) (1.40.43)\n",
      "Requirement already satisfied: sqlalchemy>=2.0.0 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unknown-data) (2.0.43)\n",
      "Requirement already satisfied: psycopg2-binary in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from unknown-data) (2.9.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.3.0->unknown-data) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.3.0->unknown-data) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=1.3.0->unknown-data) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->unknown-data) (1.17.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sqlalchemy>=2.0.0->unknown-data) (3.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sqlalchemy>=2.0.0->unknown-data) (4.15.0)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.43 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from boto3->unknown-data) (1.40.43)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from boto3->unknown-data) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from boto3->unknown-data) (0.14.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from botocore<1.41.0,>=1.40.43->boto3->unknown-data) (2.5.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema->unknown-data) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema->unknown-data) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema->unknown-data) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\kisia\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonschema->unknown-data) (0.27.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade unknown-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcff2beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 로그 파일 저장 위치: c:\\Users\\KISIA\\Desktop\\기초분석\\analyzer_debug.log\n",
      "🔄 로그 파일이 초기화되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from pandas import DataFrame\n",
    "\n",
    "from testAnalyzer import TestAnalyzer, BackendClient, Category, ResultDataFrames, ResultDataFrame\n",
    "from Analyzer import BehaviorType\n",
    "\n",
    "# 로그 파일 경로 설정 (현재 작업 디렉토리에 저장)\n",
    "log_file_path = \"analyzer_debug.log\"\n",
    "\n",
    "# 기존 핸들러들 제거 (중복 방지)\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# 로깅 설정 추가 (콘솔 + 파일)\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),  # 노트북 출력으로 로그 표시\n",
    "        logging.FileHandler(log_file_path, mode='w', encoding='utf-8')  # 파일 초기화 후 저장\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 루트 로거 레벨 설정\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "print(f\"📄 로그 파일 저장 위치: {os.path.abspath(log_file_path)}\")\n",
    "print(f\"🔄 로그 파일이 초기화되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e8dae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09174f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Set\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "class DevAnalyzer(TestAnalyzer):\n",
    "    def __init__(self, backend_client: BackendClient, time_filter_months: int = 7) -> None:\n",
    "        super().__init__(backend_client)\n",
    "        # DevAnalyzer 전용 로거 생성\n",
    "        self.dev_logger = logging.getLogger(\"DevAnaly\")\n",
    "        # 필터링된 데이터 저장을 위한 디렉토리 설정\n",
    "        self.output_dir = \"filtered_data\"\n",
    "        self._create_output_directory()\n",
    "        \n",
    "        # 브라우저 파일 화이트리스트 설정 (실제 파일명 형태)\n",
    "        self.browser_whitelist: Set[str] = {\n",
    "            \"binary\", \"browser_collected_files\", \"browser_discovered_profiles\",\n",
    "            \"Chrome.keyword_search_terms\", \"Edge.keyword_search_terms\",\n",
    "            \"Chrome.keywords\", \"Edge.keywords\",\n",
    "            \"Chrome.urls\", \"Edge.urls\",\n",
    "            \"Chrome.visits\", \"Edge.visits\", \n",
    "            \"Chrome.visited_links\", \"Edge.visited_links\",\n",
    "            \"Chrome.autofill\", \"Edge.autofill\",\n",
    "            # \"Chrome.autofill_profiles\", \"Edge.autofill_profiles\",\n",
    "            \"Chrome.addresses\", \"Edge.addresses\",\n",
    "            \"Chrome.autofill_sync_metadata\", \"Edge.autofill_sync_metadata\",\n",
    "            \"Chrome.sync_entities_metadata\", \"Edge.sync_entities_metadata\", \n",
    "            \"Chrome.downloads\", \"Edge.downloads\",\n",
    "            \"Chrome.downloads_url_chains\", \"Edge.downloads_url_chains\",\n",
    "            \"Chrome.logins\", \"Edge.logins\"\n",
    "        }\n",
    "        \n",
    "        # 광고/추적 도메인 설정\n",
    "        self.ad_tracking_domains: List[str] = [\n",
    "            'doubleclick.net', 'googlesyndication.com', 'adnxs.com', \n",
    "            'google-analytics.com', 'scorecardresearch.com', 'facebook.net', \n",
    "            'akamaihd.net', 'cloudfront.net', 'gstatic.com'\n",
    "        ]\n",
    "        \n",
    "        # 시간 필터링 기간 설정 (개월 수)\n",
    "        self.time_filter_months: int = time_filter_months\n",
    "\n",
    "        # 파일별 특정 시간 컬럼 필터링 규칙\n",
    "        self.time_filter_config: Dict[str, str] = {\n",
    "            \"binary\": \"timestamp\",\n",
    "            \"browser_collected_files\": \"timestamp\",\n",
    "            \"Chrome.keywords\": \"last_visited\",\n",
    "            \"Edge.keywords\": \"last_visited\",\n",
    "            \"Chrome.urls\": \"last_visit_time\",\n",
    "            \"Edge.urls\": \"last_visit_time\",\n",
    "            \"Chrome.visits\": \"visit_time\",\n",
    "            \"Edge.visits\": \"visit_time\",\n",
    "            \"Chrome.autofill\": \"date_last_used\",\n",
    "            \"Edge.autofill\": \"date_last_used\",\n",
    "            \"Chrome.downloads\": \"end_time\",\n",
    "            \"Edge.downloads\": \"end_time\",\n",
    "            \"Chrome.logins\": \"date_last_used\",\n",
    "            \"Edge.logins\": \"date_last_used\",\n",
    "            \"Chrome.addresses\": \"use_date\",\n",
    "            \"Edge.addresses\": \"use_date\",\n",
    "            \"mft_deleted_files\": \"deletion_time\",\n",
    "            \"recycle_bin_files\": \"deleted_time\",\n",
    "            \"lnk_files\": \"target_info__target_times__access\",\n",
    "            \"prefetch_files\": \"last_run_time_1\",\n",
    "            \"usb_devices\": \"setupapi_info__last_connection_time\",\n",
    "            \"KakaoTalk.files\": \"last_modified\",\n",
    "            \"Discord.files\": \"last_modified\"\n",
    "        }\n",
    "    \n",
    "    def _create_output_directory(self) -> None:\n",
    "        \"\"\"필터링된 데이터 저장을 위한 디렉토리 생성\"\"\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "            self.dev_logger.info(f\"📁 [DEV] Created output directory: {self.output_dir}\")\n",
    "    \n",
    "    def _save_filtered_data(self, category: Category, df_results: ResultDataFrames) -> str:\n",
    "        \"\"\"필터링된 데이터를 CSV 파일로 저장\"\"\"\n",
    "        # 빈 데이터인 경우 저장하지 않음\n",
    "        if not df_results or not df_results.data:\n",
    "            self.dev_logger.info(f\"⏭️ [DEV] No data to save for category: {category.name}\")\n",
    "            return \"\"\n",
    "        \n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        category_dir = os.path.join(self.output_dir, f\"{category.name.lower()}_{timestamp}\")\n",
    "        \n",
    "        saved_count = 0\n",
    "        skipped_count = 0\n",
    "        empty_count = 0\n",
    "        \n",
    "        # 먼저 저장할 파일이 있는지 확인\n",
    "        files_to_save = []\n",
    "        for result in df_results.data:\n",
    "            # 브라우저 카테고리의 경우 화이트리스트 확인\n",
    "            if category == Category.BROWSER and result.name not in self.browser_whitelist:\n",
    "                self.dev_logger.debug(f\"⏭️ [DEV] Skipping browser file (not in whitelist): {result.name}\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # 빈 데이터인 경우 건너뛰기\n",
    "            if result.data.empty:\n",
    "                self.dev_logger.info(f\"⏭️ [DEV] Skipping empty file: {result.name} (0 rows)\")\n",
    "                empty_count += 1\n",
    "                continue\n",
    "            \n",
    "            files_to_save.append(result)\n",
    "        \n",
    "        # 저장할 파일이 없으면 디렉토리 생성하지 않음\n",
    "        if not files_to_save:\n",
    "            self.dev_logger.info(f\"⏭️ [DEV] No files to save for category: {category.name} (all files empty or skipped)\")\n",
    "            return \"\"\n",
    "        \n",
    "        # 디렉토리 생성\n",
    "        if not os.path.exists(category_dir):\n",
    "            os.makedirs(category_dir)\n",
    "            self.dev_logger.info(f\"📁 [DEV] Created category directory: {category_dir}\")\n",
    "        \n",
    "        # 파일 저장\n",
    "        for result in files_to_save:\n",
    "            # 파일명에서 특수문자 제거 및 안전한 파일명 생성\n",
    "            safe_filename = re.sub(r'[/\\\\:*?\"<>|]', '_', result.name)\n",
    "            if not safe_filename.endswith('.csv'):\n",
    "                safe_filename += '.csv'\n",
    "            \n",
    "            file_path = os.path.join(category_dir, safe_filename)\n",
    "            \n",
    "            try:\n",
    "                result.data.to_csv(file_path, index=False, encoding='utf-8-sig')\n",
    "                self.dev_logger.info(f\"💾 [DEV] Saved filtered data: {file_path} ({len(result.data)} rows)\")\n",
    "                saved_count += 1\n",
    "            except Exception as e:\n",
    "                self.dev_logger.error(f\"❌ [DEV] Failed to save {file_path}: {str(e)}\")\n",
    "        \n",
    "        self.dev_logger.info(f\"✅ [DEV] Saved {saved_count} files, skipped {skipped_count} files, empty {empty_count} files to: {category_dir}\")\n",
    "        return category_dir\n",
    "    \n",
    "    def _filter_data(self, category: Category, df_results: ResultDataFrames) -> ResultDataFrames:\n",
    "        \"\"\"\n",
    "        데이터 필터링 처리. 카테고리별로 받아서 데이터를 처리한다.\n",
    "        \"\"\"\n",
    "        self.dev_logger.info(f\"🔧 [DEV] Starting _filter_data for category: {category.name}\")\n",
    "        \n",
    "        # 빈 데이터인 경우 조기 반환\n",
    "        if not df_results or not df_results.data:\n",
    "            self.dev_logger.info(f\"⏭️ [DEV] No data to filter for category: {category.name}\")\n",
    "            return df_results\n",
    "        \n",
    "        # 필터링 통계 초기화\n",
    "        category_original_rows = 0\n",
    "        category_filtered_rows = 0\n",
    "        \n",
    "        for result in df_results.data:\n",
    "            original_count = len(result.data)\n",
    "            category_original_rows += original_count\n",
    "            \n",
    "            # 1. 시간 필터링\n",
    "            result.data = self._apply_time_filtering(result)\n",
    "            after_time_filter = len(result.data)\n",
    "            if original_count != after_time_filter:\n",
    "                self.dev_logger.info(f\"⏰ [DEV] Time filtering for {result.name}: {original_count} -> {after_time_filter} rows\")\n",
    "            \n",
    "            # 2. 카테고리별 열/행 필터링\n",
    "            match category:\n",
    "                case Category.USB:\n",
    "                    result.data = self._filter_usb_data(result)\n",
    "                case Category.LNK:\n",
    "                    result.data = self._filter_lnk_data(result)\n",
    "                case Category.MESSENGER:\n",
    "                    result.data = self._filter_messenger_data(result)\n",
    "                case Category.PREFETCH:\n",
    "                    result.data = self._filter_prefetch_data(result)\n",
    "                case Category.DELETED:\n",
    "                    result.data = self._filter_deleted_data(result)\n",
    "                case Category.BROWSER:\n",
    "                    result.data = self._filter_browser_data(result)\n",
    "                case _:\n",
    "                    self.dev_logger.warning(f\"No specific filter for category {category.name}. Applying default limit.\")\n",
    "                    result.data = result.data.head(10)\n",
    "            \n",
    "            final_count = len(result.data)\n",
    "            category_filtered_rows += final_count\n",
    "            \n",
    "            self.dev_logger.debug(f\"🔧 [DEV] Filtered {result.name} data: {original_count} -> {final_count} rows\")\n",
    "        \n",
    "        # 카테고리별 필터링 통계 로깅\n",
    "        reduction = category_original_rows - category_filtered_rows\n",
    "        reduction_percent = (reduction / category_original_rows * 100) if category_original_rows > 0 else 0\n",
    "        \n",
    "        self.dev_logger.info(f\"📊 [DEV] {category.name} filtering summary: {category_original_rows:,} -> {category_filtered_rows:,} rows (reduction: {reduction:,} rows, {reduction_percent:.1f}%)\")\n",
    "        \n",
    "        # 전체 통계 업데이트\n",
    "        if not hasattr(self, '_total_original_rows'):\n",
    "            self._total_original_rows = 0\n",
    "            self._total_filtered_rows = 0\n",
    "        \n",
    "        self._total_original_rows += category_original_rows\n",
    "        self._total_filtered_rows += category_filtered_rows\n",
    "        \n",
    "        # 필터링된 데이터를 CSV 파일로 저장\n",
    "        self._save_filtered_data(category, df_results)\n",
    "        \n",
    "        self.dev_logger.info(f\"✅ [DEV] Completed _filter_data for category: {category.name}\")\n",
    "        return df_results\n",
    "\n",
    "    def _apply_time_filtering(self, result: ResultDataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        지정된 개월 수 이전부터 현재까지의 데이터만 유지하는 시간 필터링.\n",
    "        파일별로 지정된 시간 컬럼이 있으면 해당 컬럼을 사용하고, 없으면 일반적인 시간 컬럼을 찾아 필터링.\n",
    "        \"\"\"\n",
    "        df = result.data\n",
    "        file_name = result.name\n",
    "\n",
    "        if df.empty:\n",
    "            return df\n",
    "\n",
    "        current_time = datetime.now()\n",
    "        cutoff_date = current_time - relativedelta(months=self.time_filter_months)\n",
    "        \n",
    "        self.dev_logger.debug(\n",
    "            f\"⏰ [DEV] Time filtering for '{file_name}': Keeping data from \"\n",
    "            f\"{cutoff_date.strftime('%Y-%m-%d')} to {current_time.strftime('%Y-%m-%d')}\"\n",
    "        )\n",
    "\n",
    "        target_columns = []\n",
    "        \n",
    "        # 1. 파일별 특정 시간 컬럼 규칙 확인\n",
    "        specific_time_col = self.time_filter_config.get(file_name)\n",
    "        if specific_time_col and specific_time_col in df.columns:\n",
    "            self.dev_logger.info(f\"🎯 [DEV] Found specific time column for '{file_name}': '{specific_time_col}'\")\n",
    "            target_columns.append(specific_time_col)\n",
    "        else:\n",
    "            # 2. 특정 규칙이 없거나 컬럼이 없으면, 일반적인 시간 컬럼 탐색\n",
    "            if specific_time_col:\n",
    "                 self.dev_logger.warning(f\"⚠️ [DEV] Specific time column '{specific_time_col}' not found in '{file_name}'. Falling back to general search.\")\n",
    "            \n",
    "            time_keywords = ['time', 'date', 'created', 'modified', 'access', 'deletion', 'mtime', 'ctime', 'timestamp']\n",
    "            target_columns = [\n",
    "                col for col in df.columns \n",
    "                if isinstance(col, str) and any(keyword in col.lower() for keyword in time_keywords)\n",
    "            ]\n",
    "\n",
    "        if not target_columns:\n",
    "            self.dev_logger.warning(f\"ℹ️ [DEV] No time columns found for '{file_name}', skipping time filtering.\")\n",
    "            return df\n",
    "\n",
    "        # 시간 필터링 적용 (OR 조건)\n",
    "        mask = pd.Series([False] * len(df), index=df.index)\n",
    "        \n",
    "        for col in target_columns:\n",
    "            try:\n",
    "                # addresses 파일의 use_date 컬럼 특별 처리 (Unix timestamp)\n",
    "                if 'addresses' in file_name and col == 'use_date':\n",
    "                    self.dev_logger.debug(f\"🔍 [DEV] Converting Unix timestamp in {file_name}\")\n",
    "                    # Unix timestamp를 datetime으로 변환\n",
    "                    temp_dates = self._smart_datetime_conversion(df[col], col)\n",
    "                # cookies 파일의 WebKit timestamp 특별 처리\n",
    "                elif 'cookies' in file_name and col.endswith('_utc'):\n",
    "                    self.dev_logger.debug(f\"🔍 [DEV] Converting WebKit timestamp in {file_name}\")\n",
    "                    # WebKit timestamp (마이크로초)를 datetime으로 변환\n",
    "                    temp_dates = self._smart_datetime_conversion(df[col], col)\n",
    "                # 기타 WebKit timestamp 형식 처리 (큰 숫자 값들)\n",
    "                elif col.endswith('_utc') and df[col].dtype in ['int64', 'float64']:\n",
    "                    # 값이 매우 큰 경우 WebKit timestamp로 간주\n",
    "                    sample_values = df[col].dropna().head(3)\n",
    "                    if len(sample_values) > 0 and sample_values.iloc[0] > 1e15:  # WebKit timestamp 범위\n",
    "                        self.dev_logger.debug(f\"🔍 [DEV] Converting WebKit timestamp for {col}\")\n",
    "                        temp_dates = self._smart_datetime_conversion(df[col], col)\n",
    "                    else:\n",
    "                        # 숫자형 데이터인 경우 Unix timestamp로 시도\n",
    "                        temp_dates = self._smart_datetime_conversion(df[col], col)\n",
    "                else:\n",
    "                    # 일반적인 datetime 변환 - 먼저 데이터 타입과 샘플 값 확인\n",
    "                    temp_dates = self._smart_datetime_conversion(df[col], col)\n",
    "                \n",
    "                # 타임존 정보가 있는 경우에만 tz_localize 적용\n",
    "                if hasattr(temp_dates.dt, 'tz') and temp_dates.dt.tz is not None:\n",
    "                    temp_dates = temp_dates.dt.tz_localize(None)\n",
    "                \n",
    "                # 유효한 날짜이고, cutoff_date 이후인 데이터만 선택\n",
    "                valid_mask = temp_dates.notna() & (temp_dates >= cutoff_date)\n",
    "                mask |= valid_mask\n",
    "                \n",
    "                if valid_mask.any():\n",
    "                    self.dev_logger.debug(\n",
    "                        f\"⏰ [DEV] Column '{col}' contributed {valid_mask.sum()} valid rows for filtering.\"\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                self.dev_logger.warning(f\"⚠️ [DEV] Could not filter by column '{col}' in '{file_name}': {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # 필터링된 데이터프레임 반환\n",
    "        return df[mask]\n",
    "\n",
    "    def _smart_datetime_conversion(self, series: pd.Series, column_name: str) -> pd.Series:\n",
    "        \"\"\"\n",
    "        스마트한 datetime 변환 - 데이터 타입과 샘플 값을 분석하여 최적의 변환 방법 선택\n",
    "        \"\"\"\n",
    "        # Categorical 데이터 처리 (먼저 처리)\n",
    "        if isinstance(series.dtype, pd.CategoricalDtype):\n",
    "            self.dev_logger.debug(f\"🔍 [DEV] Converting categorical data to string for {column_name}\")\n",
    "            series = pd.Series(series.astype(str), index=series.index if hasattr(series, 'index') else None)\n",
    "        \n",
    "        if series.empty:\n",
    "            return pd.Series([], dtype='datetime64[ns]')\n",
    "        \n",
    "        # 샘플 값들 확인 (NaN 제외)\n",
    "        sample_values = series.dropna().head(5)\n",
    "        if sample_values.empty:\n",
    "            return pd.Series([pd.NaT] * len(series), index=series.index)\n",
    "        \n",
    "        # 숫자형 데이터인 경우 - 더 엄격한 체크\n",
    "        if pd.api.types.is_numeric_dtype(series) and not pd.api.types.is_string_dtype(series):\n",
    "            # Unix timestamp 범위 확인 (1970-2038년)\n",
    "            min_val = sample_values.min()\n",
    "            max_val = sample_values.max()\n",
    "            \n",
    "            if min_val > 1e15:  # WebKit timestamp (마이크로초)\n",
    "                self.dev_logger.debug(f\"🔍 [DEV] Detected WebKit timestamp for {column_name}\")\n",
    "                return pd.to_datetime(series, unit='us', errors='coerce')\n",
    "            elif min_val > 1e9:  # Unix timestamp (초)\n",
    "                self.dev_logger.debug(f\"🔍 [DEV] Detected Unix timestamp for {column_name}\")\n",
    "                return pd.to_datetime(series, unit='s', errors='coerce')\n",
    "            elif min_val > 1e6:  # 밀리초 timestamp\n",
    "                self.dev_logger.debug(f\"🔍 [DEV] Detected millisecond timestamp for {column_name}\")\n",
    "                return pd.to_datetime(series, unit='ms', errors='coerce')\n",
    "        \n",
    "        # 문자열 데이터이거나 숫자형이지만 범위에 맞지 않는 경우\n",
    "        sample_str = str(sample_values.iloc[0])\n",
    "        \n",
    "        # ISO 형식 확인\n",
    "        if 'T' in sample_str and ('+' in sample_str or 'Z' in sample_str):\n",
    "            self.dev_logger.debug(f\"🔍 [DEV] Detected ISO format for {column_name}\")\n",
    "            return pd.to_datetime(series, format='ISO8601', errors='coerce')\n",
    "        \n",
    "        # 일반적인 날짜 형식들 시도\n",
    "        common_formats = [\n",
    "            '%Y-%m-%d %H:%M:%S',\n",
    "            '%Y-%m-%d',\n",
    "            '%m/%d/%Y %H:%M:%S',\n",
    "            '%m/%d/%Y',\n",
    "            '%d/%m/%Y %H:%M:%S',\n",
    "            '%d/%m/%Y',\n",
    "            '%Y-%m-%d %H:%M:%S.%f',\n",
    "            '%Y-%m-%d %H:%M:%S.%f%z'\n",
    "        ]\n",
    "        \n",
    "        for fmt in common_formats:\n",
    "            try:\n",
    "                # 샘플 값으로 형식 테스트\n",
    "                test_val = pd.to_datetime(sample_str, format=fmt, errors='coerce')\n",
    "                if not pd.isna(test_val):\n",
    "                    self.dev_logger.debug(f\"🔍 [DEV] Detected format '{fmt}' for {column_name}\")\n",
    "                    return pd.to_datetime(series, format=fmt, errors='coerce')\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # 모든 형식이 실패하면 기본 변환 (경고 억제)\n",
    "        import warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", UserWarning)\n",
    "            warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "            self.dev_logger.debug(f\"🔍 [DEV] Using fallback datetime conversion for {column_name}\")\n",
    "            return pd.to_datetime(series, errors='coerce')\n",
    "\n",
    "    def _filter_browser_data(self, result: ResultDataFrame) -> pd.DataFrame:\n",
    "        \"\"\"브라우저 데이터에 대한 필터링 규칙\"\"\"\n",
    "        self.dev_logger.info(f\"🔎 [DEV] Applying 'BROWSER' filter to {result.name}\")\n",
    "        df = result.data.copy()\n",
    "        \n",
    "        # 화이트리스트에 없는 파일은 건너뛰기\n",
    "        if result.name not in self.browser_whitelist:\n",
    "            self.dev_logger.debug(f\"⏭️ [DEV] Skipping browser file (not in whitelist): {result.name}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # 파일명에서 접두사 제거\n",
    "        file_name = result.name.split('.', 1)[1] if result.name.startswith(('Chrome.', 'Edge.')) else result.name\n",
    "        self.dev_logger.debug(f\"📁 [DEV] Processing browser file: {file_name}\")\n",
    "        \n",
    "        # 1. 열 필터링\n",
    "        columns_to_drop = self._get_browser_columns_to_drop(file_name)\n",
    "        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "        \n",
    "        if existing_columns_to_drop:\n",
    "            df.drop(columns=existing_columns_to_drop, inplace=True)\n",
    "            self.dev_logger.debug(f\"Dropped columns for {file_name}: {existing_columns_to_drop}\")\n",
    "        \n",
    "        # 로그인 파일의 경우 첫 번째 컬럼(인덱스)도 제거\n",
    "        if file_name == \"logins\" and len(df.columns) > 0:\n",
    "            df.drop(columns=[df.columns[0]], inplace=True)\n",
    "        \n",
    "        # 2. 행 필터링\n",
    "        df = self._apply_browser_row_filtering(df, file_name)\n",
    "        \n",
    "        self.dev_logger.info(f\"🌐 [DEV] Filtered {file_name}: {len(result.data)} -> {len(df)} rows\")\n",
    "        return df\n",
    "    \n",
    "    def _apply_browser_row_filtering(self, df: pd.DataFrame, file_name: str) -> pd.DataFrame:\n",
    "        \"\"\"브라우저 데이터에 대한 행 필터링 규칙 적용\"\"\"\n",
    "        original_count = len(df)\n",
    "        \n",
    "        if file_name == \"visits\":\n",
    "            if 'visit_duration' in df.columns:\n",
    "                df['visit_duration'] = pd.to_numeric(df['visit_duration'], errors='coerce')\n",
    "                df = df[df['visit_duration'] != 0]\n",
    "                self.dev_logger.info(f\"🗑️ [DEV] Removed rows with visit_duration=0\")\n",
    "        \n",
    "        elif file_name == \"urls\":\n",
    "            if 'visit_count' in df.columns:\n",
    "                df['visit_count'] = pd.to_numeric(df['visit_count'], errors='coerce')\n",
    "                df = df[df['visit_count'] > 0]\n",
    "            \n",
    "            if 'url' in df.columns:\n",
    "                pattern = '|'.join(self.ad_tracking_domains)\n",
    "                df = df[~df['url'].str.contains(pattern, case=False, na=False)]\n",
    "                self.dev_logger.info(f\"🗑️ [DEV] Removed ad/tracking domains\")\n",
    "        \n",
    "        elif file_name == \"downloads\":\n",
    "            # 완료된 다운로드만\n",
    "            if 'state' in df.columns:\n",
    "                df = df[df['state'].astype(str) == '1']\n",
    "            \n",
    "            # 임시 파일 제거\n",
    "            if 'target_path' in df.columns:\n",
    "                temp_pattern = r'\\.(?:tmp|crdownload)$'  # non-capturing group 사용\n",
    "                df = df[~df['target_path'].str.contains(temp_pattern, case=False, na=False)]\n",
    "            \n",
    "            # 빈 행 제거\n",
    "            df = df.dropna(how='all')\n",
    "            if 'target_path' in df.columns:\n",
    "                df = df[df['target_path'].notna() & (df['target_path'].astype(str).str.strip() != '')]\n",
    "            if 'url' in df.columns:\n",
    "                df = df[df['url'].notna() & (df['url'].astype(str).str.strip() != '')]\n",
    "        \n",
    "        elif file_name == \"browser_collected_files\":\n",
    "            if 'data_type' in df.columns and 'download_state' in df.columns:\n",
    "                df = df[(df['data_type'] == 'downloads') & (df['download_state'] == 'completed')]\n",
    "        \n",
    "        elif file_name == \"sync_entities_metadata\":\n",
    "            # 삭제된 항목 제거\n",
    "            if 'is_deleted' in df.columns:\n",
    "                df = df[df['is_deleted'] != True]\n",
    "            \n",
    "            # 폴더가 아닌 실제 데이터만 유지\n",
    "            if 'is_folder' in df.columns:\n",
    "                df = df[df['is_folder'] != True]\n",
    "            \n",
    "            # 빈 행 제거\n",
    "            df = df.dropna(how='all')\n",
    "        \n",
    "        if original_count != len(df):\n",
    "            self.dev_logger.info(f\"🗑️ [DEV] Row filtering: {original_count} -> {len(df)} rows\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _get_browser_columns_to_drop(self, file_name: str) -> List[str]:\n",
    "        \"\"\"브라우저 파일명에 따른 제거할 컬럼 목록 반환\"\"\"\n",
    "        column_map = {\n",
    "            \"binary\": ['error', 'success', 'file_type'],\n",
    "            \"browser_collected_files\": ['error', 'success', 'file_type'],\n",
    "            \"browser_discovered_profiles\": ['exists'],\n",
    "            \"keyword_search_terms\": ['normalized_term'],\n",
    "            \"keywords\": [\n",
    "                'url_hash', 'input_encodings', 'image_url_post_params', \n",
    "                'search_url_post_params', 'suggest_url_post_params',\n",
    "                'alternate_urls', 'safe_for_autoreplace', 'created_from_play_api',\n",
    "                'image_url', 'favicon_url'\n",
    "            ],\n",
    "            \"urls\": ['favicon_id'],\n",
    "            \"visits\": ['incremented_omnibox_typed_score', 'app_id'],\n",
    "            \"autofill\": ['value_lower'],\n",
    "            \"addresses\": ['language_code'],\n",
    "            \"downloads\": ['transient', 'http_method', 'etag', 'last_response_headers'],\n",
    "            \"logins\": [\n",
    "                'password_value', 'username_element', 'password_element', \n",
    "                'submit_element', 'display_name', 'icon_url', 'federation_url', \n",
    "                'skip_zero_click', 'generation_upload_status', \n",
    "                'possible_username_pairs', 'moving_blocked_for_list'\n",
    "            ],\n",
    "            \"sync_entities_metadata\": [\n",
    "                'model_type', 'storage_key', 'client_tag_hash', 'server_id',\n",
    "                'specifics_hash', 'base_specifics_hash', 'parent_id', 'version',\n",
    "                'mtime', 'ctime', 'server_version', 'is_deleted', 'is_folder',\n",
    "                'unique_position', 'is_bookmark', 'is_folder', 'is_unsynced'\n",
    "            ]\n",
    "        }\n",
    "        return column_map.get(file_name, [])\n",
    "\n",
    "    def _filter_deleted_data(self, result: ResultDataFrame) -> pd.DataFrame:\n",
    "        \"\"\"삭제된 파일 데이터에 대한 필터링\"\"\"\n",
    "        self.dev_logger.info(f\"🔎 [DEV] Applying 'DELETED' filter to {result.name}\")\n",
    "        df = result.data.copy()\n",
    "        \n",
    "        # 열 필터링\n",
    "        columns_to_drop = [\n",
    "            'access_time_timestamp', 'creation_time_timestamp', \n",
    "            'deletion_time_timestamp', 'modification_time_timestamp', \n",
    "            'deleted_time_timestamp', 'is_directory', 'parse_status', \n",
    "            'recycle_bin', 'recycle_bin_version', 'Unnamed: 0', 'file_index'\n",
    "        ]\n",
    "        \n",
    "        existing_columns = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_columns:\n",
    "            df.drop(columns=existing_columns, inplace=True)\n",
    "        \n",
    "        # 행 필터링\n",
    "        df = self._apply_deleted_row_filtering(df, result.name)\n",
    "        return df\n",
    "    \n",
    "    def _apply_deleted_row_filtering(self, df: pd.DataFrame, file_name: str) -> pd.DataFrame:\n",
    "        \"\"\"삭제된 파일 데이터에 대한 행 필터링\"\"\"\n",
    "        if 'mft_deleted' not in str(file_name).lower():\n",
    "            return df\n",
    "        \n",
    "        original_count = len(df)\n",
    "        \n",
    "        # 시스템 파일 제거 ($ 접두사)\n",
    "        file_col = 'file_name' if 'file_name' in df.columns else 'FileName'\n",
    "        if file_col in df.columns:\n",
    "            df = df[~df[file_col].str.startswith('$', na=False)]\n",
    "        \n",
    "        # 시스템 경로 제거\n",
    "        path_col = 'full_path' if 'full_path' in df.columns else 'ParentPath'\n",
    "        if path_col in df.columns:\n",
    "            system_paths = [\n",
    "                r'C:\\\\Windows', r'C:\\\\Program Files', \n",
    "                r'C:\\\\ProgramData', r'C:\\\\\\$Recycle\\.Bin'\n",
    "            ]\n",
    "            pattern = '|'.join(system_paths)\n",
    "            df = df[~df[path_col].str.contains(pattern, case=False, na=False, regex=True)]\n",
    "            \n",
    "            # 시스템 로그 파일 제거\n",
    "            if file_col in df.columns:\n",
    "                system_files = ['bootex.log', 'LOG', 'setup.log', 'install.log']\n",
    "                system_pattern = '|'.join(system_files)\n",
    "                df = df[~df[file_col].str.contains(system_pattern, case=False, na=False)]\n",
    "        \n",
    "        if original_count != len(df):\n",
    "            self.dev_logger.info(f\"🗑️ [DEV] Filtered: {original_count} -> {len(df)} rows\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _filter_lnk_data(self, result: ResultDataFrame) -> pd.DataFrame:\n",
    "        \"\"\"LNK 데이터 필터링\"\"\"\n",
    "        self.dev_logger.info(f\"🔎 [DEV] Applying 'LNK' filter to {result.name}\")\n",
    "        df = result.data.copy()\n",
    "        \n",
    "        # 열 필터링\n",
    "        static_columns = [\n",
    "            'header__header_size', 'header__header_size_hex', \n",
    "            'link_info__link_info_header_size', 'header__hot_key', \n",
    "            'header__icon_index', 'link_info__volume_info__drive_type',\n",
    "            'link_info__volume_info__drive_type_formatted'\n",
    "        ]\n",
    "        \n",
    "        pattern_columns = [\n",
    "            col for col in df.columns \n",
    "            if isinstance(col, str) and (\n",
    "                col.endswith(('_time_raw', '_hex', '_timestamp')) or \n",
    "                col.startswith('link_info__offsets__')\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        columns_to_drop = static_columns + pattern_columns\n",
    "        existing_columns = [col for col in columns_to_drop if col in df.columns]\n",
    "        \n",
    "        if existing_columns:\n",
    "            df.drop(columns=existing_columns, inplace=True)\n",
    "        \n",
    "        # 행 필터링\n",
    "        df = self._apply_lnk_row_filtering(df, result.name)\n",
    "        return df\n",
    "    \n",
    "    def _apply_lnk_row_filtering(self, df: pd.DataFrame, file_name: str) -> pd.DataFrame:\n",
    "        \"\"\"LNK 행 필터링\"\"\"\n",
    "        if 'lnk_files' not in str(file_name).lower():\n",
    "            return df\n",
    "        \n",
    "        # 시스템 파일 제거\n",
    "        target_col = next((col for col in ['Target_Path', 'target_info__target_path'] \n",
    "                          if col in df.columns), None)\n",
    "        \n",
    "        if target_col:\n",
    "            system_keywords = [\n",
    "                'Windows', 'System32', 'ProgramData', 'AppData', \n",
    "                'Update', 'Setup', 'Installer', 'MicrosoftEdge', \n",
    "                'Chrome', 'Temp', 'Cache'\n",
    "            ]\n",
    "            pattern = '|'.join(system_keywords)\n",
    "            df = df[~df[target_col].str.contains(pattern, case=False, na=False)]\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _filter_messenger_data(self, result: ResultDataFrame) -> pd.DataFrame:\n",
    "        \"\"\"메신저 데이터 필터링\"\"\"\n",
    "        self.dev_logger.info(f\"🔎 [DEV] Applying 'MESSENGER' filter to {result.name}\")\n",
    "        df = result.data.copy()\n",
    "        \n",
    "        # 열 필터링\n",
    "        columns_to_drop = [\n",
    "            'created_timestamp', 'last_modified_timestamp', 'relative_path',\n",
    "            'is_valid_file', 'messenger_type', 'file_index'\n",
    "        ]\n",
    "        \n",
    "        existing_columns = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_columns:\n",
    "            df.drop(columns=existing_columns, inplace=True)\n",
    "        \n",
    "        # 행 필터링\n",
    "        df = self._apply_messenger_row_filtering(df, result.name)\n",
    "        \n",
    "        messenger_type = \"Discord\" if \"Discord\" in result.name else \"KakaoTalk\" if \"KakaoTalk\" in result.name else \"Unknown\"\n",
    "        self.dev_logger.info(f\"📱 [DEV] Processing {messenger_type}: {len(df)} rows\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def _apply_messenger_row_filtering(self, df: pd.DataFrame, result_name: str) -> pd.DataFrame:\n",
    "        \"\"\"메신저 데이터 행 필터링\"\"\"\n",
    "        if 'file_name' not in df.columns:\n",
    "            self.dev_logger.warning(\"⚠️ 'file_name' column not found, skipping extension filtering.\")\n",
    "            return df\n",
    "        \n",
    "        # 제외할 확장자\n",
    "        exclude_extensions = (\n",
    "            \".exe\", \".dll\", \".sys\", \".ico\", \".cur\", \".msi\", \".bat\", \".sh\",\n",
    "            \".ttf\", \".otf\", \".editorconfig\", \".eslintrc\", \".npmignore\"\n",
    "        )\n",
    "        \n",
    "        initial_count = len(df)\n",
    "        mask = ~df['file_name'].str.lower().str.endswith(exclude_extensions, na=False)\n",
    "        filtered_df = df[mask]\n",
    "        \n",
    "        self.dev_logger.info(\n",
    "            f\"Filtered out {initial_count - len(filtered_df)} rows \"\n",
    "            f\"based on {len(exclude_extensions)} excluded extensions.\"\n",
    "        )\n",
    "        \n",
    "        return filtered_df\n",
    "\n",
    "    def _filter_prefetch_data(self, result: ResultDataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Prefetch 데이터 필터링\"\"\"\n",
    "        self.dev_logger.info(f\"🔎 [DEV] Applying 'PREFETCH' filter to {result.name}\")\n",
    "        df = result.data.copy()\n",
    "        \n",
    "        columns_to_drop = [\n",
    "            'structure__executable_file_name', 'file_index', \n",
    "            'structure__signature', 'structure__format_version', \n",
    "            'is_compressed', 'parse_success', 'error_message',\n",
    "            'structure__trace_chains_offset', 'structure__volumes_info_offset',\n",
    "            'structure__filename_strings_size', 'structure__filename_strings_offset'\n",
    "        ]\n",
    "        \n",
    "        existing_columns = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_columns:\n",
    "            df.drop(columns=existing_columns, inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _filter_usb_data(self, result: ResultDataFrame) -> pd.DataFrame:\n",
    "        \"\"\"USB 데이터 필터링\"\"\"\n",
    "        self.dev_logger.info(f\"🔎 [DEV] Applying 'USB' filter to {result.name}\")\n",
    "        df = result.data.copy()\n",
    "        \n",
    "        static_columns = [\n",
    "            'volume_info__file_system', 'volume_info__mount_point', \n",
    "            'volume_info__volume_guid', 'volume_info__drive_letter', \n",
    "            'volume_info__volume_label', 'setupapi_info__serial_number', \n",
    "            'device_metadata__serial_number', 'setupapi_info__product_id', \n",
    "            'setupapi_info__volume_name', 'data_sources__has_usb_data', \n",
    "            'data_sources__has_usbstor_data', 'data_sources__has_setupapi_data', \n",
    "            'primary_source', 'portable_device_info__friendly_name'\n",
    "        ]\n",
    "        \n",
    "        pattern_columns = [col for col in df.columns if col.startswith('connection_times__')]\n",
    "        columns_to_drop = static_columns + pattern_columns\n",
    "        \n",
    "        existing_columns = [col for col in columns_to_drop if col in df.columns]\n",
    "        if existing_columns:\n",
    "            df.drop(columns=existing_columns, inplace=True)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    \n",
    "    \n",
    "    def _load_data(self, category: Category) -> ResultDataFrames:\n",
    "        \"\"\"카테고리별 데이터 로드 - 파일이 없으면 건너뛰기\"\"\"\n",
    "        try:\n",
    "            self.dev_logger.debug(f\"Starting data load for category: {category.name}\")\n",
    "            \n",
    "            # helper를 사용하여 데이터 로드 및 인코딩 처리\n",
    "            df_results = self.helper.get_encoded_results(self.task_id, category)\n",
    "            \n",
    "            if not df_results:\n",
    "                self.dev_logger.warning(f\"No dataframes found for category {category.name} and task {self.task_id}\")\n",
    "                # 빈 ResultDataFrames 반환\n",
    "                return ResultDataFrames(data=[])\n",
    "            \n",
    "            self.dev_logger.debug(f\"Successfully loaded {len(df_results.data)} dataframes for category: {category.name}\")\n",
    "            return df_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.dev_logger.warning(f\"⚠️ [DEV] Failed to load data for category {category.name}: {e}\")\n",
    "            self.dev_logger.info(f\"⏭️ [DEV] Skipping category {category.name} - no data available\")\n",
    "            # 빈 ResultDataFrames 반환하여 건너뛰기\n",
    "            return ResultDataFrames(data=[])\n",
    "\n",
    "    def _generate_analysis_result(self):\n",
    "        \"\"\"\n",
    "        self.created_artifacts를 활용하여 분류결과를 생성함.\n",
    "        결과는 상속받은 클래스에서 정의된 아래 self.analyze_results에 업데이트할 것.\n",
    "\n",
    "        self.analyze_results = {\n",
    "            behavior: {\n",
    "                \"job_id\": self.job_id,\n",
    "                \"task_id\": self.task_id,\n",
    "                \"behavior\": behavior.name,\n",
    "                \"analysis_summary\": \"\",\n",
    "                \"risk_level\": \"\",\n",
    "                \"artifact_ids\": []\n",
    "            } for behavior in BehaviorType\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.dev_logger.info(\"🔧 [DEV] Starting _generate_analysis_result\")\n",
    "        \n",
    "        # 부모 클래스의 메서드 먼저 실행\n",
    "        super()._generate_analysis_result()\n",
    "        \n",
    "        # created_artifacts가 비어있으면 조기 종료\n",
    "        if not self.created_artifacts:\n",
    "            self.dev_logger.warning(\"⚠️ [DEV] No artifacts created, skipping analysis result generation\")\n",
    "            return\n",
    "        \n",
    "        # 각 행위 유형별로 아티팩트 분석\n",
    "        for behavior_type in BehaviorType:\n",
    "            self.dev_logger.debug(f\"🔍 [DEV] Analyzing behavior type: {behavior_type.name}\")\n",
    "            \n",
    "            # 해당 행위와 관련된 아티팩트 수집\n",
    "            related_artifacts = self._get_artifacts_by_behavior(behavior_type)\n",
    "            \n",
    "            if not related_artifacts:\n",
    "                self.dev_logger.debug(f\"ℹ️ [DEV] No artifacts found for {behavior_type.name}\")\n",
    "                continue\n",
    "            \n",
    "            # 아티팩트 ID 목록 추출\n",
    "            artifact_ids = [artifact.get('id') or artifact.get('artifact_id') \n",
    "                        for artifact in related_artifacts \n",
    "                        if artifact.get('id') or artifact.get('artifact_id')]\n",
    "            \n",
    "            # 분석 요약 생성\n",
    "            analysis_summary = self._create_analysis_summary(behavior_type, related_artifacts)\n",
    "            \n",
    "            # 위험도 평가\n",
    "            risk_level = self._evaluate_risk_level(behavior_type, related_artifacts)\n",
    "            \n",
    "            # 결과 업데이트\n",
    "            if behavior_type in self.analyze_results:\n",
    "                self.analyze_results[behavior_type].update({\n",
    "                    \"artifact_ids\": artifact_ids,\n",
    "                    \"analysis_summary\": analysis_summary,\n",
    "                    \"risk_level\": risk_level,\n",
    "                    \"artifact_count\": len(artifact_ids)\n",
    "                })\n",
    "                \n",
    "                self.dev_logger.info(\n",
    "                    f\"✅ [DEV] Updated {behavior_type.name}: \"\n",
    "                    f\"{len(artifact_ids)} artifacts, risk={risk_level}\"\n",
    "                )\n",
    "        \n",
    "        # 전체 분석 통계 로깅\n",
    "        self._log_analysis_statistics()\n",
    "        \n",
    "        self.dev_logger.info(\"✅ [DEV] Completed _generate_analysis_result\")\n",
    "\n",
    "\n",
    "    def _get_artifacts_by_behavior(self, behavior_type: BehaviorType) -> List[dict]:\n",
    "        \"\"\"특정 행위 유형과 관련된 아티팩트 필터링\"\"\"\n",
    "        related_artifacts = []\n",
    "        \n",
    "        for artifact in self.created_artifacts:\n",
    "            # 아티팩트의 카테고리나 메타데이터를 기반으로 행위 유형 매칭\n",
    "            artifact_behavior = artifact.get('behavior_type') or artifact.get('category')\n",
    "            \n",
    "            # 직접 매칭\n",
    "            if artifact_behavior == behavior_type:\n",
    "                related_artifacts.append(artifact)\n",
    "                continue\n",
    "            \n",
    "            # 카테고리 기반 매핑\n",
    "            if self._is_artifact_related_to_behavior(artifact, behavior_type):\n",
    "                related_artifacts.append(artifact)\n",
    "        \n",
    "        return related_artifacts\n",
    "\n",
    "\n",
    "    def _is_artifact_related_to_behavior(self, artifact: dict, behavior_type: BehaviorType) -> bool:\n",
    "        \"\"\"아티팩트가 특정 행위 유형과 관련이 있는지 판단\"\"\"\n",
    "        category = artifact.get('category', '').lower()\n",
    "        artifact_type = artifact.get('type', '').lower()\n",
    "        file_name = artifact.get('file_name', '').lower()\n",
    "        \n",
    "        # 행위 유형별 매핑 규칙\n",
    "        behavior_mappings = {\n",
    "            BehaviorType.USB_USAGE: ['usb', 'external_device', 'removable'],\n",
    "            BehaviorType.FILE_ACCESS: ['lnk', 'shortcut', 'recent', 'jump_list'],\n",
    "            BehaviorType.WEB_BROWSING: ['browser', 'chrome', 'edge', 'firefox', 'url', 'download'],\n",
    "            BehaviorType.MESSENGER_USAGE: ['messenger', 'kakao', 'discord', 'telegram', 'chat'],\n",
    "            BehaviorType.PROGRAM_EXECUTION: ['prefetch', 'execution', 'process', 'application'],\n",
    "            BehaviorType.FILE_DELETION: ['deleted', 'recycle', 'mft_deleted', 'removed'],\n",
    "            BehaviorType.DATA_EXFILTRATION: ['download', 'transfer', 'upload', 'export'],\n",
    "        }\n",
    "        \n",
    "        # 해당 행위 유형의 키워드 확인\n",
    "        keywords = behavior_mappings.get(behavior_type, [])\n",
    "        \n",
    "        return any(keyword in category or keyword in artifact_type or keyword in file_name \n",
    "                for keyword in keywords)\n",
    "\n",
    "\n",
    "    def _create_analysis_summary(self, behavior_type: BehaviorType, artifacts: List[dict]) -> str:\n",
    "        \"\"\"행위 유형별 분석 요약 생성\"\"\"\n",
    "        artifact_count = len(artifacts)\n",
    "        \n",
    "        # 행위 유형별 요약 템플릿\n",
    "        summary_templates = {\n",
    "            BehaviorType.USB_USAGE: self._summarize_usb_usage,\n",
    "            BehaviorType.FILE_ACCESS: self._summarize_file_access,\n",
    "            BehaviorType.WEB_BROWSING: self._summarize_web_browsing,\n",
    "            BehaviorType.MESSENGER_USAGE: self._summarize_messenger_usage,\n",
    "            BehaviorType.PROGRAM_EXECUTION: self._summarize_program_execution,\n",
    "            BehaviorType.FILE_DELETION: self._summarize_file_deletion,\n",
    "            BehaviorType.DATA_EXFILTRATION: self._summarize_data_exfiltration,\n",
    "        }\n",
    "        \n",
    "        # 해당 행위에 맞는 요약 함수 실행\n",
    "        summarize_func = summary_templates.get(behavior_type)\n",
    "        if summarize_func:\n",
    "            return summarize_func(artifacts)\n",
    "        \n",
    "        # 기본 요약\n",
    "        return f\"Found {artifact_count} artifact(s) related to {behavior_type.name}\"\n",
    "\n",
    "\n",
    "    def _summarize_usb_usage(self, artifacts: List[dict]) -> str:\n",
    "        \"\"\"USB 사용 분석 요약\"\"\"\n",
    "        device_count = len(set(a.get('device_id') for a in artifacts if a.get('device_id')))\n",
    "        connection_count = sum(a.get('connection_count', 1) for a in artifacts)\n",
    "        \n",
    "        return (f\"Detected {device_count} unique USB device(s) with \"\n",
    "                f\"{connection_count} total connection(s). \"\n",
    "                f\"Analysis based on {len(artifacts)} artifact(s).\")\n",
    "\n",
    "\n",
    "    def _summarize_file_access(self, artifacts: List[dict]) -> str:\n",
    "        \"\"\"파일 접근 분석 요약\"\"\"\n",
    "        file_types = set()\n",
    "        for artifact in artifacts:\n",
    "            file_name = artifact.get('file_name', '')\n",
    "            if '.' in file_name:\n",
    "                ext = file_name.rsplit('.', 1)[-1].lower()\n",
    "                file_types.add(ext)\n",
    "        \n",
    "        accessed_files = len(artifacts)\n",
    "        return (f\"Analyzed {accessed_files} file access record(s) \"\n",
    "                f\"across {len(file_types)} file type(s): {', '.join(sorted(file_types)[:5])}\")\n",
    "\n",
    "\n",
    "    def _summarize_web_browsing(self, artifacts: List[dict]) -> str:\n",
    "        \"\"\"웹 브라우징 분석 요약\"\"\"\n",
    "        url_count = sum(1 for a in artifacts if 'url' in str(a.get('type', '')).lower())\n",
    "        download_count = sum(1 for a in artifacts if 'download' in str(a.get('type', '')).lower())\n",
    "        \n",
    "        return (f\"Analyzed {len(artifacts)} web browsing artifact(s): \"\n",
    "                f\"{url_count} URL visit(s), {download_count} download(s)\")\n",
    "\n",
    "\n",
    "    def _summarize_messenger_usage(self, artifacts: List[dict]) -> str:\n",
    "        \"\"\"메신저 사용 분석 요약\"\"\"\n",
    "        messenger_types = set(a.get('messenger_type') for a in artifacts if a.get('messenger_type'))\n",
    "        file_count = len(artifacts)\n",
    "        \n",
    "        if messenger_types:\n",
    "            messengers = ', '.join(sorted(messenger_types))\n",
    "            return f\"Detected {file_count} messenger file(s) from: {messengers}\"\n",
    "        \n",
    "        return f\"Detected {file_count} messenger-related file(s)\"\n",
    "\n",
    "\n",
    "    def _summarize_program_execution(self, artifacts: List[dict]) -> str:\n",
    "        \"\"\"프로그램 실행 분석 요약\"\"\"\n",
    "        programs = set(a.get('program_name') or a.get('executable_name') \n",
    "                    for a in artifacts \n",
    "                    if a.get('program_name') or a.get('executable_name'))\n",
    "        \n",
    "        return (f\"Identified {len(programs)} unique program(s) executed. \"\n",
    "                f\"Total {len(artifacts)} execution record(s).\")\n",
    "\n",
    "\n",
    "    def _summarize_file_deletion(self, artifacts: List[dict]) -> str:\n",
    "        \"\"\"파일 삭제 분석 요약\"\"\"\n",
    "        deleted_count = len(artifacts)\n",
    "        total_size = sum(a.get('file_size', 0) for a in artifacts)\n",
    "        \n",
    "        size_mb = total_size / (1024 * 1024) if total_size > 0 else 0\n",
    "        return (f\"Found {deleted_count} deleted file(s). \"\n",
    "                f\"Total size: {size_mb:.2f} MB\")\n",
    "\n",
    "\n",
    "    def _summarize_data_exfiltration(self, artifacts: List[dict]) -> str:\n",
    "        \"\"\"데이터 유출 분석 요약\"\"\"\n",
    "        transfer_count = len(artifacts)\n",
    "        suspicious_count = sum(1 for a in artifacts if a.get('is_suspicious', False))\n",
    "        \n",
    "        return (f\"Detected {transfer_count} data transfer event(s). \"\n",
    "                f\"{suspicious_count} flagged as potentially suspicious.\")\n",
    "\n",
    "\n",
    "    def _evaluate_risk_level(self, behavior_type: BehaviorType, artifacts: List[dict]) -> str:\n",
    "        \"\"\"위험도 평가\"\"\"\n",
    "        artifact_count = len(artifacts)\n",
    "        \n",
    "        # 행위 유형별 기본 위험도\n",
    "        base_risk = {\n",
    "            BehaviorType.DATA_EXFILTRATION: 'HIGH',\n",
    "            BehaviorType.FILE_DELETION: 'MEDIUM',\n",
    "            BehaviorType.USB_USAGE: 'MEDIUM',\n",
    "            BehaviorType.WEB_BROWSING: 'LOW',\n",
    "            BehaviorType.MESSENGER_USAGE: 'LOW',\n",
    "            BehaviorType.FILE_ACCESS: 'LOW',\n",
    "            BehaviorType.PROGRAM_EXECUTION: 'LOW',\n",
    "        }\n",
    "        \n",
    "        risk = base_risk.get(behavior_type, 'LOW')\n",
    "        \n",
    "        # 아티팩트 수에 따른 위험도 조정\n",
    "        if artifact_count > 100:\n",
    "            if risk == 'LOW':\n",
    "                risk = 'MEDIUM'\n",
    "            elif risk == 'MEDIUM':\n",
    "                risk = 'HIGH'\n",
    "        elif artifact_count > 50:\n",
    "            if risk == 'LOW':\n",
    "                risk = 'MEDIUM'\n",
    "        \n",
    "        # 의심스러운 패턴 감지\n",
    "        suspicious_count = sum(1 for a in artifacts if a.get('is_suspicious', False))\n",
    "        if suspicious_count > artifact_count * 0.3:  # 30% 이상이 의심스러운 경우\n",
    "            if risk == 'LOW':\n",
    "                risk = 'MEDIUM'\n",
    "            elif risk == 'MEDIUM':\n",
    "                risk = 'HIGH'\n",
    "        \n",
    "        return risk\n",
    "\n",
    "\n",
    "    def _log_analysis_statistics(self):\n",
    "        \"\"\"전체 분석 통계 로깅 - 업그레이드된 버전\"\"\"\n",
    "        total_artifacts = len(self.created_artifacts)\n",
    "        behaviors_with_data = sum(1 for result in self.analyze_results.values() \n",
    "                                if result.get('artifact_count', 0) > 0)\n",
    "        \n",
    "        high_risk_count = sum(1 for result in self.analyze_results.values() \n",
    "                            if result.get('risk_level') == 'HIGH')\n",
    "        medium_risk_count = sum(1 for result in self.analyze_results.values() \n",
    "                            if result.get('risk_level') == 'MEDIUM')\n",
    "        low_risk_count = sum(1 for result in self.analyze_results.values() \n",
    "                            if result.get('risk_level') == 'LOW')\n",
    "        \n",
    "        # 카테고리별 통계 계산\n",
    "        category_stats = {}\n",
    "        for artifact in self.created_artifacts:\n",
    "            category = artifact.get('category', 'Unknown')\n",
    "            if category not in category_stats:\n",
    "                category_stats[category] = {'count': 0, 'risk_levels': []}\n",
    "            category_stats[category]['count'] += 1\n",
    "            category_stats[category]['risk_levels'].append(artifact.get('risk_level', 'UNKNOWN'))\n",
    "        \n",
    "        # 필터링 효과 통계 (원본 데이터와 비교)\n",
    "        total_original_rows = getattr(self, '_total_original_rows', 0)\n",
    "        total_filtered_rows = getattr(self, '_total_filtered_rows', 0)\n",
    "        filtering_reduction = total_original_rows - total_filtered_rows\n",
    "        filtering_percentage = (filtering_reduction / total_original_rows * 100) if total_original_rows > 0 else 0\n",
    "        \n",
    "        # 상세 통계 로깅\n",
    "        self.dev_logger.info(\"=\" * 80)\n",
    "        self.dev_logger.info(\"📊 [DEV] Enhanced Analysis Statistics Summary\")\n",
    "        self.dev_logger.info(\"=\" * 80)\n",
    "        \n",
    "        # 기본 통계\n",
    "        self.dev_logger.info(\"🔍 Basic Statistics:\")\n",
    "        self.dev_logger.info(f\"  • Total Artifacts: {total_artifacts:,}\")\n",
    "        self.dev_logger.info(f\"  • Behaviors with Data: {behaviors_with_data}/{len(BehaviorType)}\")\n",
    "        self.dev_logger.info(f\"  • Data Coverage: {(behaviors_with_data/len(BehaviorType)*100):.1f}%\")\n",
    "        \n",
    "        # 위험도 분포\n",
    "        self.dev_logger.info(\"\\n⚠️ Risk Level Distribution:\")\n",
    "        self.dev_logger.info(f\"  • High Risk: {high_risk_count} behaviors\")\n",
    "        self.dev_logger.info(f\"  • Medium Risk: {medium_risk_count} behaviors\")\n",
    "        self.dev_logger.info(f\"  • Low Risk: {low_risk_count} behaviors\")\n",
    "        \n",
    "        # 필터링 효과\n",
    "        if total_original_rows > 0:\n",
    "            self.dev_logger.info(\"\\n📉 Filtering Effectiveness:\")\n",
    "            self.dev_logger.info(f\"  • Original Data: {total_original_rows:,} rows\")\n",
    "            self.dev_logger.info(f\"  • Filtered Data: {total_filtered_rows:,} rows\")\n",
    "            self.dev_logger.info(f\"  • Reduction: {filtering_reduction:,} rows ({filtering_percentage:.1f}%)\")\n",
    "        \n",
    "        # 카테고리별 통계\n",
    "        if category_stats:\n",
    "            self.dev_logger.info(\"\\n📁 Category-wise Statistics:\")\n",
    "            for category, stats in sorted(category_stats.items()):\n",
    "                risk_distribution = {}\n",
    "                for risk in stats['risk_levels']:\n",
    "                    risk_distribution[risk] = risk_distribution.get(risk, 0) + 1\n",
    "                \n",
    "                risk_str = \", \".join([f\"{risk}: {count}\" for risk, count in risk_distribution.items()])\n",
    "                self.dev_logger.info(f\"  • {category}: {stats['count']} artifacts ({risk_str})\")\n",
    "        \n",
    "        # 행위별 상세 통계\n",
    "        self.dev_logger.info(\"\\n🎯 Behavior-wise Details:\")\n",
    "        for behavior_type, result in self.analyze_results.items():\n",
    "            artifact_count = result.get('artifact_count', 0)\n",
    "            if artifact_count > 0:\n",
    "                risk_level = result.get('risk_level', 'UNKNOWN')\n",
    "                analysis_summary = result.get('analysis_summary', 'No summary available')\n",
    "                \n",
    "                # 위험도에 따른 이모지\n",
    "                risk_emoji = {\n",
    "                    'HIGH': '🔴',\n",
    "                    'MEDIUM': '🟡', \n",
    "                    'LOW': '🟢',\n",
    "                    'UNKNOWN': '⚪'\n",
    "                }.get(risk_level, '⚪')\n",
    "                \n",
    "                self.dev_logger.info(f\"  {risk_emoji} {behavior_type.name}:\")\n",
    "                self.dev_logger.info(f\"    • Artifacts: {artifact_count}\")\n",
    "                self.dev_logger.info(f\"    • Risk Level: {risk_level}\")\n",
    "                self.dev_logger.info(f\"    • Summary: {analysis_summary[:100]}{'...' if len(analysis_summary) > 100 else ''}\")\n",
    "        \n",
    "        # 성능 통계\n",
    "        processing_time = getattr(self, '_processing_time', 0)\n",
    "        if processing_time > 0:\n",
    "            self.dev_logger.info(f\"\\n⏱️ Performance:\")\n",
    "            self.dev_logger.info(f\"  • Processing Time: {processing_time:.2f} seconds\")\n",
    "            if total_artifacts > 0:\n",
    "                self.dev_logger.info(f\"  • Artifacts per Second: {total_artifacts/processing_time:.2f}\")\n",
    "        \n",
    "        self.dev_logger.info(\"=\" * 80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0395dad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-16 02:47:09,360 - testAnalyzer - INFO - 🚀 [TEST] Starting final test for task: session-20251002-050523-6de09ba4\n",
      "2025-10-16 02:47:09,362 - Analyzer - DEBUG - 🔄 Processing category: BROWSER\n",
      "2025-10-16 02:47:09,363 - DevAnaly - DEBUG - Starting data load for category: BROWSER\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit urls.last_visit_time to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit visits.visit_time to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit downloads.start_time to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit downloads.end_time to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit downloads.last_access_time to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit logins.date_created to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit logins.date_last_used to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit logins.date_received to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit logins.date_password_modified to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit logins.date_last_filled to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit insecure_credentials.create_time to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit cookies.creation_utc to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit cookies.last_access_utc to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit cookies.expires_utc to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit cookies.last_update_utc to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted Unix autofill.date_created to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted Unix autofill.date_last_used to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit keywords.date_created to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit keywords.last_visited to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit keywords.last_modified to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit urls.last_visit_time to datetime\n",
      "[2025-10-16 02:47:27] DataEncoder - Converted WebKit visits.visit_time to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit downloads.start_time to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit downloads.end_time to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit downloads.last_access_time to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit logins.date_created to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit logins.date_last_used to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit logins.date_received to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit logins.date_password_modified to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit cookies.creation_utc to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit cookies.last_access_utc to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit cookies.expires_utc to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit cookies.last_update_utc to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted Unix autofill.date_created to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted Unix autofill.date_last_used to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit keywords.date_created to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit keywords.last_visited to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit keywords.last_modified to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit urls.last_visit_time to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit visits.visit_time to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit downloads.start_time to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit downloads.end_time to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit downloads.last_access_time to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit logins.date_created to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit logins.date_last_used to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit logins.date_received to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit logins.date_password_modified to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit logins.date_last_filled to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit insecure_credentials.create_time to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit cookies.creation_utc to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit cookies.last_access_utc to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit cookies.expires_utc to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit cookies.last_update_utc to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted Unix autofill.date_created to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted Unix autofill.date_last_used to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit keywords.date_created to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit keywords.last_visited to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit keywords.last_modified to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit urls.last_visit_time to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit visits.visit_time to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit downloads.start_time to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit downloads.end_time to datetime\n",
      "[2025-10-16 02:47:28] DataEncoder - Converted WebKit downloads.last_access_time to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit logins.date_created to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit logins.date_last_used to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit logins.date_received to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit logins.date_password_modified to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit logins.date_last_filled to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit insecure_credentials.create_time to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit cookies.creation_utc to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit cookies.last_access_utc to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit cookies.expires_utc to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit cookies.last_update_utc to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted Unix autofill.date_created to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted Unix autofill.date_last_used to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit keywords.date_created to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit keywords.last_visited to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit keywords.last_modified to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit urls.last_visit_time to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit visits.visit_time to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit downloads.start_time to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit downloads.end_time to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit downloads.last_access_time to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit logins.date_created to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit logins.date_last_used to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit logins.date_received to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit logins.date_password_modified to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit logins.date_last_filled to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit cookies.creation_utc to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit cookies.last_access_utc to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit cookies.expires_utc to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit cookies.last_update_utc to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted Unix autofill.date_created to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted Unix autofill.date_last_used to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit keywords.date_created to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit keywords.last_visited to datetime\n",
      "[2025-10-16 02:47:29] DataEncoder - Converted WebKit keywords.last_modified to datetime\n",
      "2025-10-16 02:47:29,452 - DevAnaly - DEBUG - Successfully loaded 60 dataframes for category: BROWSER\n",
      "2025-10-16 02:47:29,454 - Analyzer - DEBUG - Filtering data for category: BROWSER\n",
      "2025-10-16 02:47:29,455 - DevAnaly - INFO - 🔧 [DEV] Starting _filter_data for category: BROWSER\n",
      "2025-10-16 02:47:29,457 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'browser_statistics': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:29,458 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'browser_statistics', skipping time filtering.\n",
      "2025-10-16 02:47:29,459 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to browser_statistics\n",
      "2025-10-16 02:47:29,461 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): browser_statistics\n",
      "2025-10-16 02:47:29,463 - DevAnaly - DEBUG - 🔧 [DEV] Filtered browser_statistics data: 1 -> 0 rows\n",
      "2025-10-16 02:47:29,464 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.urls': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:29,465 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'Chrome.urls': 'last_visit_time'\n",
      "2025-10-16 02:47:29,478 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_visit_time\n",
      "2025-10-16 02:47:29,494 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_visit_time' contributed 14384 valid rows for filtering.\n",
      "2025-10-16 02:47:29,498 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Chrome.urls: 14829 -> 14384 rows\n",
      "2025-10-16 02:47:29,500 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.urls\n",
      "2025-10-16 02:47:29,504 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: urls\n",
      "2025-10-16 02:47:29,832 - DevAnaly - INFO - 🗑️ [DEV] Removed ad/tracking domains\n",
      "2025-10-16 02:47:29,834 - DevAnaly - INFO - 🌐 [DEV] Filtered urls: 14384 -> 14384 rows\n",
      "2025-10-16 02:47:29,835 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.urls data: 14829 -> 14384 rows\n",
      "2025-10-16 02:47:29,837 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.visits': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:29,839 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'Chrome.visits': 'visit_time'\n",
      "2025-10-16 02:47:29,848 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for visit_time\n",
      "2025-10-16 02:47:29,863 - DevAnaly - DEBUG - ⏰ [DEV] Column 'visit_time' contributed 31505 valid rows for filtering.\n",
      "2025-10-16 02:47:29,881 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Chrome.visits: 32258 -> 31505 rows\n",
      "2025-10-16 02:47:29,883 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.visits\n",
      "2025-10-16 02:47:29,905 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: visits\n",
      "2025-10-16 02:47:29,917 - DevAnaly - DEBUG - Dropped columns for visits: ['incremented_omnibox_typed_score', 'app_id']\n",
      "2025-10-16 02:47:29,956 - DevAnaly - INFO - 🗑️ [DEV] Removed rows with visit_duration=0\n",
      "2025-10-16 02:47:29,958 - DevAnaly - INFO - 🗑️ [DEV] Row filtering: 31505 -> 29146 rows\n",
      "2025-10-16 02:47:29,961 - DevAnaly - INFO - 🌐 [DEV] Filtered visits: 31505 -> 29146 rows\n",
      "2025-10-16 02:47:29,967 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.visits data: 32258 -> 29146 rows\n",
      "2025-10-16 02:47:29,969 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.segments': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:29,970 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Chrome.segments', skipping time filtering.\n",
      "2025-10-16 02:47:29,971 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.segments\n",
      "2025-10-16 02:47:29,973 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.segments\n",
      "2025-10-16 02:47:29,975 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.segments data: 48 -> 0 rows\n",
      "2025-10-16 02:47:29,976 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.downloads': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:29,977 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'Chrome.downloads': 'end_time'\n",
      "2025-10-16 02:47:29,981 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for end_time\n",
      "2025-10-16 02:47:29,986 - DevAnaly - DEBUG - ⏰ [DEV] Column 'end_time' contributed 165 valid rows for filtering.\n",
      "2025-10-16 02:47:29,989 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Chrome.downloads: 590 -> 165 rows\n",
      "2025-10-16 02:47:29,990 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.downloads\n",
      "2025-10-16 02:47:29,993 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: downloads\n",
      "2025-10-16 02:47:29,995 - DevAnaly - DEBUG - Dropped columns for downloads: ['transient', 'http_method', 'etag']\n",
      "2025-10-16 02:47:30,000 - DevAnaly - INFO - 🌐 [DEV] Filtered downloads: 165 -> 165 rows\n",
      "2025-10-16 02:47:30,001 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.downloads data: 590 -> 165 rows\n",
      "2025-10-16 02:47:30,003 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.segment_usage': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,006 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for time_slot\n",
      "2025-10-16 02:47:30,021 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Chrome.segment_usage: 138 -> 0 rows\n",
      "2025-10-16 02:47:30,023 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.segment_usage\n",
      "2025-10-16 02:47:30,025 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.segment_usage\n",
      "2025-10-16 02:47:30,026 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.segment_usage data: 138 -> 0 rows\n",
      "2025-10-16 02:47:30,027 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.visited_links': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,029 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Chrome.visited_links', skipping time filtering.\n",
      "2025-10-16 02:47:30,030 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.visited_links\n",
      "2025-10-16 02:47:30,032 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: visited_links\n",
      "2025-10-16 02:47:30,033 - DevAnaly - INFO - 🌐 [DEV] Filtered visited_links: 3155 -> 3155 rows\n",
      "2025-10-16 02:47:30,034 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.visited_links data: 3155 -> 3155 rows\n",
      "2025-10-16 02:47:30,035 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.content_annotations': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,036 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Chrome.content_annotations', skipping time filtering.\n",
      "2025-10-16 02:47:30,037 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.content_annotations\n",
      "2025-10-16 02:47:30,044 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.content_annotations\n",
      "2025-10-16 02:47:30,050 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.content_annotations data: 28100 -> 0 rows\n",
      "2025-10-16 02:47:30,051 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.context_annotations': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,053 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Chrome.context_annotations', skipping time filtering.\n",
      "2025-10-16 02:47:30,054 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.context_annotations\n",
      "2025-10-16 02:47:30,066 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.context_annotations\n",
      "2025-10-16 02:47:30,075 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.context_annotations data: 30569 -> 0 rows\n",
      "2025-10-16 02:47:30,078 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.downloads_url_chains': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,079 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Chrome.downloads_url_chains', skipping time filtering.\n",
      "2025-10-16 02:47:30,080 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.downloads_url_chains\n",
      "2025-10-16 02:47:30,082 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: downloads_url_chains\n",
      "2025-10-16 02:47:30,083 - DevAnaly - INFO - 🌐 [DEV] Filtered downloads_url_chains: 717 -> 717 rows\n",
      "2025-10-16 02:47:30,084 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.downloads_url_chains data: 717 -> 717 rows\n",
      "2025-10-16 02:47:30,085 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.keyword_search_terms': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,087 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Chrome.keyword_search_terms', skipping time filtering.\n",
      "2025-10-16 02:47:30,088 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.keyword_search_terms\n",
      "2025-10-16 02:47:30,090 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: keyword_search_terms\n",
      "2025-10-16 02:47:30,092 - DevAnaly - DEBUG - Dropped columns for keyword_search_terms: ['normalized_term']\n",
      "2025-10-16 02:47:30,093 - DevAnaly - INFO - 🌐 [DEV] Filtered keyword_search_terms: 2357 -> 2357 rows\n",
      "2025-10-16 02:47:30,094 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.keyword_search_terms data: 2357 -> 2357 rows\n",
      "2025-10-16 02:47:30,096 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.stats': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,100 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for update_time\n",
      "2025-10-16 02:47:30,104 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Chrome.stats: 2 -> 0 rows\n",
      "2025-10-16 02:47:30,105 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.stats\n",
      "2025-10-16 02:47:30,106 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.stats\n",
      "2025-10-16 02:47:30,108 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.stats data: 2 -> 0 rows\n",
      "2025-10-16 02:47:30,110 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.logins': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,111 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'Chrome.logins': 'date_last_used'\n",
      "2025-10-16 02:47:30,115 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for date_last_used\n",
      "2025-10-16 02:47:30,120 - DevAnaly - DEBUG - ⏰ [DEV] Column 'date_last_used' contributed 84 valid rows for filtering.\n",
      "2025-10-16 02:47:30,123 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Chrome.logins: 809 -> 84 rows\n",
      "2025-10-16 02:47:30,124 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.logins\n",
      "2025-10-16 02:47:30,126 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: logins\n",
      "2025-10-16 02:47:30,129 - DevAnaly - DEBUG - Dropped columns for logins: ['password_value', 'username_element', 'password_element', 'submit_element', 'display_name', 'icon_url', 'federation_url', 'skip_zero_click', 'generation_upload_status', 'possible_username_pairs']\n",
      "2025-10-16 02:47:30,131 - DevAnaly - INFO - 🌐 [DEV] Filtered logins: 84 -> 84 rows\n",
      "2025-10-16 02:47:30,132 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.logins data: 809 -> 84 rows\n",
      "2025-10-16 02:47:30,133 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.sync_model_metadata': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,135 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Chrome.sync_model_metadata', skipping time filtering.\n",
      "2025-10-16 02:47:30,136 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.sync_model_metadata\n",
      "2025-10-16 02:47:30,137 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.sync_model_metadata\n",
      "2025-10-16 02:47:30,139 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.sync_model_metadata data: 4 -> 0 rows\n",
      "2025-10-16 02:47:30,141 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.insecure_credentials': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,145 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for create_time\n",
      "2025-10-16 02:47:30,147 - DevAnaly - DEBUG - ⏰ [DEV] Column 'create_time' contributed 2 valid rows for filtering.\n",
      "2025-10-16 02:47:30,150 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Chrome.insecure_credentials: 14 -> 2 rows\n",
      "2025-10-16 02:47:30,151 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.insecure_credentials\n",
      "2025-10-16 02:47:30,152 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.insecure_credentials\n",
      "2025-10-16 02:47:30,154 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.insecure_credentials data: 14 -> 0 rows\n",
      "2025-10-16 02:47:30,156 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.sync_entities_metadata': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,157 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Chrome.sync_entities_metadata', skipping time filtering.\n",
      "2025-10-16 02:47:30,159 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.sync_entities_metadata\n",
      "2025-10-16 02:47:30,160 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: sync_entities_metadata\n",
      "2025-10-16 02:47:30,162 - DevAnaly - DEBUG - Dropped columns for sync_entities_metadata: ['storage_key']\n",
      "2025-10-16 02:47:30,165 - DevAnaly - INFO - 🌐 [DEV] Filtered sync_entities_metadata: 809 -> 809 rows\n",
      "2025-10-16 02:47:30,167 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.sync_entities_metadata data: 809 -> 809 rows\n",
      "2025-10-16 02:47:30,168 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.cookies': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,171 - DevAnaly - DEBUG - 🔍 [DEV] Converting WebKit timestamp in Chrome.cookies\n",
      "2025-10-16 02:47:30,175 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_access_utc\n",
      "2025-10-16 02:47:30,189 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_access_utc' contributed 2978 valid rows for filtering.\n",
      "2025-10-16 02:47:30,190 - DevAnaly - DEBUG - 🔍 [DEV] Converting WebKit timestamp in Chrome.cookies\n",
      "2025-10-16 02:47:30,194 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_update_utc\n",
      "2025-10-16 02:47:30,206 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_update_utc' contributed 2702 valid rows for filtering.\n",
      "2025-10-16 02:47:30,212 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Chrome.cookies: 7372 -> 2978 rows\n",
      "2025-10-16 02:47:30,213 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.cookies\n",
      "2025-10-16 02:47:30,216 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.cookies\n",
      "2025-10-16 02:47:30,219 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.cookies data: 7372 -> 0 rows\n",
      "2025-10-16 02:47:30,220 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.autofill': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,222 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'Chrome.autofill': 'date_last_used'\n",
      "2025-10-16 02:47:30,226 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for date_last_used\n",
      "2025-10-16 02:47:30,231 - DevAnaly - DEBUG - ⏰ [DEV] Column 'date_last_used' contributed 817 valid rows for filtering.\n",
      "2025-10-16 02:47:30,235 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Chrome.autofill: 1807 -> 817 rows\n",
      "2025-10-16 02:47:30,236 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.autofill\n",
      "2025-10-16 02:47:30,238 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: autofill\n",
      "2025-10-16 02:47:30,241 - DevAnaly - DEBUG - Dropped columns for autofill: ['value_lower']\n",
      "2025-10-16 02:47:30,243 - DevAnaly - INFO - 🌐 [DEV] Filtered autofill: 817 -> 817 rows\n",
      "2025-10-16 02:47:30,244 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.autofill data: 1807 -> 817 rows\n",
      "2025-10-16 02:47:30,245 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.keywords': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,247 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'Chrome.keywords': 'last_visited'\n",
      "2025-10-16 02:47:30,251 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_visited\n",
      "2025-10-16 02:47:30,254 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_visited' contributed 11 valid rows for filtering.\n",
      "2025-10-16 02:47:30,257 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Chrome.keywords: 55 -> 11 rows\n",
      "2025-10-16 02:47:30,259 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.keywords\n",
      "2025-10-16 02:47:30,260 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: keywords\n",
      "2025-10-16 02:47:30,262 - DevAnaly - DEBUG - Dropped columns for keywords: ['url_hash', 'input_encodings', 'image_url_post_params', 'search_url_post_params', 'suggest_url_post_params', 'alternate_urls', 'safe_for_autoreplace', 'created_from_play_api', 'image_url', 'favicon_url']\n",
      "2025-10-16 02:47:30,263 - DevAnaly - INFO - 🌐 [DEV] Filtered keywords: 11 -> 11 rows\n",
      "2025-10-16 02:47:30,265 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.keywords data: 55 -> 11 rows\n",
      "2025-10-16 02:47:30,266 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.credit_cards': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,270 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for use_date\n",
      "2025-10-16 02:47:30,275 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for date_modified\n",
      "2025-10-16 02:47:30,278 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Chrome.credit_cards: 2 -> 0 rows\n",
      "2025-10-16 02:47:30,279 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.credit_cards\n",
      "2025-10-16 02:47:30,281 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.credit_cards\n",
      "2025-10-16 02:47:30,283 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.credit_cards data: 2 -> 0 rows\n",
      "2025-10-16 02:47:30,284 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.token_service': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,286 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Chrome.token_service', skipping time filtering.\n",
      "2025-10-16 02:47:30,287 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.token_service\n",
      "2025-10-16 02:47:30,288 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.token_service\n",
      "2025-10-16 02:47:30,290 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.token_service data: 6 -> 0 rows\n",
      "2025-10-16 02:47:30,292 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.autofill_sync_metadata': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,293 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Chrome.autofill_sync_metadata', skipping time filtering.\n",
      "2025-10-16 02:47:30,294 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.autofill_sync_metadata\n",
      "2025-10-16 02:47:30,296 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: autofill_sync_metadata\n",
      "2025-10-16 02:47:30,298 - DevAnaly - INFO - 🌐 [DEV] Filtered autofill_sync_metadata: 1809 -> 1809 rows\n",
      "2025-10-16 02:47:30,299 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.autofill_sync_metadata data: 1809 -> 1809 rows\n",
      "2025-10-16 02:47:30,301 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.autofill_model_type_state': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,302 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Chrome.autofill_model_type_state', skipping time filtering.\n",
      "2025-10-16 02:47:30,303 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.autofill_model_type_state\n",
      "2025-10-16 02:47:30,304 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.autofill_model_type_state\n",
      "2025-10-16 02:47:30,306 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.autofill_model_type_state data: 25 -> 0 rows\n",
      "2025-10-16 02:47:30,307 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.visit_source': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,309 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Chrome.visit_source', skipping time filtering.\n",
      "2025-10-16 02:47:30,310 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.visit_source\n",
      "2025-10-16 02:47:30,312 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.visit_source\n",
      "2025-10-16 02:47:30,315 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.visit_source data: 27134 -> 0 rows\n",
      "2025-10-16 02:47:30,316 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.history_sync_metadata': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,318 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Chrome.history_sync_metadata', skipping time filtering.\n",
      "2025-10-16 02:47:30,319 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.history_sync_metadata\n",
      "2025-10-16 02:47:30,321 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.history_sync_metadata\n",
      "2025-10-16 02:47:30,322 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.history_sync_metadata data: 1 -> 0 rows\n",
      "2025-10-16 02:47:30,324 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.addresses': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,325 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'Chrome.addresses': 'use_date'\n",
      "2025-10-16 02:47:30,327 - DevAnaly - DEBUG - 🔍 [DEV] Converting Unix timestamp in Chrome.addresses\n",
      "2025-10-16 02:47:30,330 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for use_date\n",
      "2025-10-16 02:47:30,333 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Chrome.addresses: 1 -> 0 rows\n",
      "2025-10-16 02:47:30,335 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.addresses\n",
      "2025-10-16 02:47:30,336 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: addresses\n",
      "2025-10-16 02:47:30,338 - DevAnaly - DEBUG - Dropped columns for addresses: ['language_code']\n",
      "2025-10-16 02:47:30,339 - DevAnaly - INFO - 🌐 [DEV] Filtered addresses: 0 -> 0 rows\n",
      "2025-10-16 02:47:30,340 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.addresses data: 1 -> 0 rows\n",
      "2025-10-16 02:47:30,342 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.local_stored_cvc': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,346 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_updated_timestamp\n",
      "2025-10-16 02:47:30,349 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Chrome.local_stored_cvc: 1 -> 0 rows\n",
      "2025-10-16 02:47:30,350 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.local_stored_cvc\n",
      "2025-10-16 02:47:30,352 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.local_stored_cvc\n",
      "2025-10-16 02:47:30,354 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.local_stored_cvc data: 1 -> 0 rows\n",
      "2025-10-16 02:47:30,355 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.address_type_tokens': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,357 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Chrome.address_type_tokens', skipping time filtering.\n",
      "2025-10-16 02:47:30,358 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.address_type_tokens\n",
      "2025-10-16 02:47:30,360 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.address_type_tokens\n",
      "2025-10-16 02:47:30,361 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.address_type_tokens data: 37 -> 0 rows\n",
      "2025-10-16 02:47:30,363 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Chrome.payments_customer_data': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,364 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Chrome.payments_customer_data', skipping time filtering.\n",
      "2025-10-16 02:47:30,365 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Chrome.payments_customer_data\n",
      "2025-10-16 02:47:30,367 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.payments_customer_data\n",
      "2025-10-16 02:47:30,368 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Chrome.payments_customer_data data: 1 -> 0 rows\n",
      "2025-10-16 02:47:30,370 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.urls': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,371 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'Edge.urls': 'last_visit_time'\n",
      "2025-10-16 02:47:30,375 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_visit_time\n",
      "2025-10-16 02:47:30,379 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_visit_time' contributed 56 valid rows for filtering.\n",
      "2025-10-16 02:47:30,381 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.urls: 58 -> 56 rows\n",
      "2025-10-16 02:47:30,383 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.urls\n",
      "2025-10-16 02:47:30,385 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: urls\n",
      "2025-10-16 02:47:30,389 - DevAnaly - INFO - 🗑️ [DEV] Removed ad/tracking domains\n",
      "2025-10-16 02:47:30,391 - DevAnaly - INFO - 🌐 [DEV] Filtered urls: 56 -> 56 rows\n",
      "2025-10-16 02:47:30,393 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.urls data: 58 -> 56 rows\n",
      "2025-10-16 02:47:30,394 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.visits': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,396 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'Edge.visits': 'visit_time'\n",
      "2025-10-16 02:47:30,400 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for visit_time\n",
      "2025-10-16 02:47:30,403 - DevAnaly - DEBUG - ⏰ [DEV] Column 'visit_time' contributed 93 valid rows for filtering.\n",
      "2025-10-16 02:47:30,406 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.visits\n",
      "2025-10-16 02:47:30,407 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: visits\n",
      "2025-10-16 02:47:30,409 - DevAnaly - DEBUG - Dropped columns for visits: ['incremented_omnibox_typed_score', 'app_id']\n",
      "2025-10-16 02:47:30,413 - DevAnaly - INFO - 🗑️ [DEV] Removed rows with visit_duration=0\n",
      "2025-10-16 02:47:30,414 - DevAnaly - INFO - 🗑️ [DEV] Row filtering: 93 -> 84 rows\n",
      "2025-10-16 02:47:30,416 - DevAnaly - INFO - 🌐 [DEV] Filtered visits: 93 -> 84 rows\n",
      "2025-10-16 02:47:30,417 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.visits data: 93 -> 84 rows\n",
      "2025-10-16 02:47:30,419 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.segments': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,420 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Edge.segments', skipping time filtering.\n",
      "2025-10-16 02:47:30,421 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.segments\n",
      "2025-10-16 02:47:30,422 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.segments\n",
      "2025-10-16 02:47:30,424 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.segments data: 1 -> 0 rows\n",
      "2025-10-16 02:47:30,426 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.downloads': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,427 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'Edge.downloads': 'end_time'\n",
      "2025-10-16 02:47:30,432 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for end_time\n",
      "2025-10-16 02:47:30,438 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.downloads: 322 -> 0 rows\n",
      "2025-10-16 02:47:30,440 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.downloads\n",
      "2025-10-16 02:47:30,442 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: downloads\n",
      "2025-10-16 02:47:30,444 - DevAnaly - DEBUG - Dropped columns for downloads: ['transient', 'http_method', 'etag']\n",
      "2025-10-16 02:47:30,450 - DevAnaly - INFO - 🌐 [DEV] Filtered downloads: 0 -> 0 rows\n",
      "2025-10-16 02:47:30,452 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.downloads data: 322 -> 0 rows\n",
      "2025-10-16 02:47:30,453 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.segment_usage': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,457 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for time_slot\n",
      "2025-10-16 02:47:30,461 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.segment_usage: 2 -> 0 rows\n",
      "2025-10-16 02:47:30,463 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.segment_usage\n",
      "2025-10-16 02:47:30,464 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.segment_usage\n",
      "2025-10-16 02:47:30,465 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.segment_usage data: 2 -> 0 rows\n",
      "2025-10-16 02:47:30,467 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.visited_links': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,468 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Edge.visited_links', skipping time filtering.\n",
      "2025-10-16 02:47:30,470 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.visited_links\n",
      "2025-10-16 02:47:30,471 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: visited_links\n",
      "2025-10-16 02:47:30,473 - DevAnaly - INFO - 🌐 [DEV] Filtered visited_links: 10 -> 10 rows\n",
      "2025-10-16 02:47:30,474 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.visited_links data: 10 -> 10 rows\n",
      "2025-10-16 02:47:30,475 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.content_annotations': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,477 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Edge.content_annotations', skipping time filtering.\n",
      "2025-10-16 02:47:30,478 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.content_annotations\n",
      "2025-10-16 02:47:30,479 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.content_annotations\n",
      "2025-10-16 02:47:30,481 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.content_annotations data: 80 -> 0 rows\n",
      "2025-10-16 02:47:30,483 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.context_annotations': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,485 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Edge.context_annotations', skipping time filtering.\n",
      "2025-10-16 02:47:30,486 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.context_annotations\n",
      "2025-10-16 02:47:30,489 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.context_annotations\n",
      "2025-10-16 02:47:30,491 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.context_annotations data: 84 -> 0 rows\n",
      "2025-10-16 02:47:30,493 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.downloads_url_chains': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,494 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Edge.downloads_url_chains', skipping time filtering.\n",
      "2025-10-16 02:47:30,496 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.downloads_url_chains\n",
      "2025-10-16 02:47:30,497 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: downloads_url_chains\n",
      "2025-10-16 02:47:30,499 - DevAnaly - INFO - 🌐 [DEV] Filtered downloads_url_chains: 338 -> 338 rows\n",
      "2025-10-16 02:47:30,500 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.downloads_url_chains data: 338 -> 338 rows\n",
      "2025-10-16 02:47:30,501 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.keyword_search_terms': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,503 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Edge.keyword_search_terms', skipping time filtering.\n",
      "2025-10-16 02:47:30,504 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.keyword_search_terms\n",
      "2025-10-16 02:47:30,506 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: keyword_search_terms\n",
      "2025-10-16 02:47:30,508 - DevAnaly - DEBUG - Dropped columns for keyword_search_terms: ['normalized_term']\n",
      "2025-10-16 02:47:30,510 - DevAnaly - INFO - 🌐 [DEV] Filtered keyword_search_terms: 5 -> 5 rows\n",
      "2025-10-16 02:47:30,511 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.keyword_search_terms data: 5 -> 5 rows\n",
      "2025-10-16 02:47:30,513 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.stats': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,517 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for update_time\n",
      "2025-10-16 02:47:30,520 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.stats: 5 -> 0 rows\n",
      "2025-10-16 02:47:30,522 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.stats\n",
      "2025-10-16 02:47:30,523 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.stats\n",
      "2025-10-16 02:47:30,525 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.stats data: 5 -> 0 rows\n",
      "2025-10-16 02:47:30,531 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.logins': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,532 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'Edge.logins': 'date_last_used'\n",
      "2025-10-16 02:47:30,537 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for date_last_used\n",
      "2025-10-16 02:47:30,541 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.logins: 60 -> 0 rows\n",
      "2025-10-16 02:47:30,542 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.logins\n",
      "2025-10-16 02:47:30,544 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: logins\n",
      "2025-10-16 02:47:30,546 - DevAnaly - DEBUG - Dropped columns for logins: ['password_value', 'username_element', 'password_element', 'submit_element', 'display_name', 'icon_url', 'federation_url', 'skip_zero_click', 'generation_upload_status', 'possible_username_pairs']\n",
      "2025-10-16 02:47:30,548 - DevAnaly - INFO - 🌐 [DEV] Filtered logins: 0 -> 0 rows\n",
      "2025-10-16 02:47:30,550 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.logins data: 60 -> 0 rows\n",
      "2025-10-16 02:47:30,551 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.breached': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,553 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for last_seen_time\n",
      "2025-10-16 02:47:30,557 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_seen_time\n",
      "2025-10-16 02:47:30,562 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for last_checked_time\n",
      "2025-10-16 02:47:30,565 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_checked_time\n",
      "2025-10-16 02:47:30,570 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for breach_ic_last_shown_time\n",
      "2025-10-16 02:47:30,574 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for breach_ic_last_shown_time\n",
      "2025-10-16 02:47:30,580 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.breached: 38 -> 0 rows\n",
      "2025-10-16 02:47:30,581 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.breached\n",
      "2025-10-16 02:47:30,583 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.breached\n",
      "2025-10-16 02:47:30,585 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.breached data: 38 -> 0 rows\n",
      "2025-10-16 02:47:30,586 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.logins_edge_extended': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,588 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Edge.logins_edge_extended', skipping time filtering.\n",
      "2025-10-16 02:47:30,589 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.logins_edge_extended\n",
      "2025-10-16 02:47:30,591 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.logins_edge_extended\n",
      "2025-10-16 02:47:30,593 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.logins_edge_extended data: 60 -> 0 rows\n",
      "2025-10-16 02:47:30,594 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.cookies': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,596 - DevAnaly - DEBUG - 🔍 [DEV] Converting WebKit timestamp in Edge.cookies\n",
      "2025-10-16 02:47:30,600 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_access_utc\n",
      "2025-10-16 02:47:30,604 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_access_utc' contributed 205 valid rows for filtering.\n",
      "2025-10-16 02:47:30,606 - DevAnaly - DEBUG - 🔍 [DEV] Converting WebKit timestamp in Edge.cookies\n",
      "2025-10-16 02:47:30,610 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_update_utc\n",
      "2025-10-16 02:47:30,614 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_update_utc' contributed 182 valid rows for filtering.\n",
      "2025-10-16 02:47:30,617 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.cookies: 242 -> 205 rows\n",
      "2025-10-16 02:47:30,619 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.cookies\n",
      "2025-10-16 02:47:30,621 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.cookies\n",
      "2025-10-16 02:47:30,623 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.cookies data: 242 -> 0 rows\n",
      "2025-10-16 02:47:30,624 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.autofill': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,626 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'Edge.autofill': 'date_last_used'\n",
      "2025-10-16 02:47:30,630 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for date_last_used\n",
      "2025-10-16 02:47:30,633 - DevAnaly - DEBUG - ⏰ [DEV] Column 'date_last_used' contributed 5 valid rows for filtering.\n",
      "2025-10-16 02:47:30,635 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.autofill: 22 -> 5 rows\n",
      "2025-10-16 02:47:30,637 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.autofill\n",
      "2025-10-16 02:47:30,638 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: autofill\n",
      "2025-10-16 02:47:30,640 - DevAnaly - DEBUG - Dropped columns for autofill: ['value_lower']\n",
      "2025-10-16 02:47:30,642 - DevAnaly - INFO - 🌐 [DEV] Filtered autofill: 5 -> 5 rows\n",
      "2025-10-16 02:47:30,643 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.autofill data: 22 -> 5 rows\n",
      "2025-10-16 02:47:30,645 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.keywords': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,646 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'Edge.keywords': 'last_visited'\n",
      "2025-10-16 02:47:30,649 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.keywords: 9 -> 0 rows\n",
      "2025-10-16 02:47:30,651 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.keywords\n",
      "2025-10-16 02:47:30,653 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: keywords\n",
      "2025-10-16 02:47:30,655 - DevAnaly - DEBUG - Dropped columns for keywords: ['url_hash', 'input_encodings', 'image_url_post_params', 'search_url_post_params', 'suggest_url_post_params', 'alternate_urls', 'safe_for_autoreplace', 'created_from_play_api', 'image_url', 'favicon_url']\n",
      "2025-10-16 02:47:30,657 - DevAnaly - INFO - 🌐 [DEV] Filtered keywords: 0 -> 0 rows\n",
      "2025-10-16 02:47:30,658 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.keywords data: 9 -> 0 rows\n",
      "2025-10-16 02:47:30,660 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.addresses': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,661 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'Edge.addresses': 'use_date'\n",
      "2025-10-16 02:47:30,663 - DevAnaly - DEBUG - 🔍 [DEV] Converting Unix timestamp in Edge.addresses\n",
      "2025-10-16 02:47:30,666 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for use_date\n",
      "2025-10-16 02:47:30,670 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.addresses: 4 -> 0 rows\n",
      "2025-10-16 02:47:30,671 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.addresses\n",
      "2025-10-16 02:47:30,673 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: addresses\n",
      "2025-10-16 02:47:30,675 - DevAnaly - DEBUG - Dropped columns for addresses: ['language_code']\n",
      "2025-10-16 02:47:30,676 - DevAnaly - INFO - 🌐 [DEV] Filtered addresses: 0 -> 0 rows\n",
      "2025-10-16 02:47:30,678 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.addresses data: 4 -> 0 rows\n",
      "2025-10-16 02:47:30,679 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.edge_meta': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,681 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Edge.edge_meta', skipping time filtering.\n",
      "2025-10-16 02:47:30,682 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.edge_meta\n",
      "2025-10-16 02:47:30,684 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.edge_meta\n",
      "2025-10-16 02:47:30,686 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.edge_meta data: 1 -> 0 rows\n",
      "2025-10-16 02:47:30,687 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.token_service': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,689 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Edge.token_service', skipping time filtering.\n",
      "2025-10-16 02:47:30,690 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.token_service\n",
      "2025-10-16 02:47:30,692 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.token_service\n",
      "2025-10-16 02:47:30,694 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.token_service data: 1 -> 0 rows\n",
      "2025-10-16 02:47:30,695 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.address_type_tokens': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,697 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'Edge.address_type_tokens', skipping time filtering.\n",
      "2025-10-16 02:47:30,698 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.address_type_tokens\n",
      "2025-10-16 02:47:30,699 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.address_type_tokens\n",
      "2025-10-16 02:47:30,701 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.address_type_tokens data: 142 -> 0 rows\n",
      "2025-10-16 02:47:30,703 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.autofill_edge_extended': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,705 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for date_created\n",
      "2025-10-16 02:47:30,709 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for date_created\n",
      "2025-10-16 02:47:30,713 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for date_last_used\n",
      "2025-10-16 02:47:30,718 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for date_last_used\n",
      "2025-10-16 02:47:30,722 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.autofill_edge_extended: 15 -> 0 rows\n",
      "2025-10-16 02:47:30,724 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.autofill_edge_extended\n",
      "2025-10-16 02:47:30,726 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.autofill_edge_extended\n",
      "2025-10-16 02:47:30,729 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.autofill_edge_extended data: 15 -> 0 rows\n",
      "2025-10-16 02:47:30,730 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.safety_hub_navigations': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,735 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for navigation_date\n",
      "2025-10-16 02:47:30,738 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.safety_hub_navigations: 2 -> 0 rows\n",
      "2025-10-16 02:47:30,740 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.safety_hub_navigations\n",
      "2025-10-16 02:47:30,742 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.safety_hub_navigations\n",
      "2025-10-16 02:47:30,744 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.safety_hub_navigations data: 2 -> 0 rows\n",
      "2025-10-16 02:47:30,745 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.autofill_edge_field_values': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,747 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for date_created\n",
      "2025-10-16 02:47:30,751 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for date_created\n",
      "2025-10-16 02:47:30,756 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for date_last_used\n",
      "2025-10-16 02:47:30,761 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for date_last_used\n",
      "2025-10-16 02:47:30,767 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.autofill_edge_field_values: 90 -> 0 rows\n",
      "2025-10-16 02:47:30,769 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.autofill_edge_field_values\n",
      "2025-10-16 02:47:30,770 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.autofill_edge_field_values\n",
      "2025-10-16 02:47:30,772 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.autofill_edge_field_values data: 90 -> 0 rows\n",
      "2025-10-16 02:47:30,774 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.autofill_profile_edge_extended': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,776 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for date_of_birth_day\n",
      "2025-10-16 02:47:30,780 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for date_of_birth_day\n",
      "2025-10-16 02:47:30,782 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for date_of_birth_year\n",
      "2025-10-16 02:47:30,786 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for date_of_birth_year\n",
      "2025-10-16 02:47:30,792 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for date_of_birth_month\n",
      "2025-10-16 02:47:30,796 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.autofill_profile_edge_extended: 4 -> 0 rows\n",
      "2025-10-16 02:47:30,798 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.autofill_profile_edge_extended\n",
      "2025-10-16 02:47:30,800 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.autofill_profile_edge_extended\n",
      "2025-10-16 02:47:30,801 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.autofill_profile_edge_extended data: 4 -> 0 rows\n",
      "2025-10-16 02:47:30,803 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.autofill_edge_field_client_info': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,804 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for date_created\n",
      "2025-10-16 02:47:30,808 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for date_created\n",
      "2025-10-16 02:47:30,813 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.autofill_edge_field_client_info: 86 -> 0 rows\n",
      "2025-10-16 02:47:30,814 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.autofill_edge_field_client_info\n",
      "2025-10-16 02:47:30,816 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.autofill_edge_field_client_info\n",
      "2025-10-16 02:47:30,818 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.autofill_edge_field_client_info data: 86 -> 0 rows\n",
      "2025-10-16 02:47:30,819 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.autofill_edge_fieldid_cid_mapping': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,820 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for last_sync_time\n",
      "2025-10-16 02:47:30,824 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_sync_time\n",
      "2025-10-16 02:47:30,830 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.autofill_edge_fieldid_cid_mapping: 37 -> 0 rows\n",
      "2025-10-16 02:47:30,831 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.autofill_edge_fieldid_cid_mapping\n",
      "2025-10-16 02:47:30,833 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.autofill_edge_fieldid_cid_mapping\n",
      "2025-10-16 02:47:30,835 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.autofill_edge_fieldid_cid_mapping data: 37 -> 0 rows\n",
      "2025-10-16 02:47:30,836 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Edge.browser_essentials_safety_esm_protections': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,839 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for esm_protections_date\n",
      "2025-10-16 02:47:30,842 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Edge.browser_essentials_safety_esm_protections: 2 -> 0 rows\n",
      "2025-10-16 02:47:30,844 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to Edge.browser_essentials_safety_esm_protections\n",
      "2025-10-16 02:47:30,845 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.browser_essentials_safety_esm_protections\n",
      "2025-10-16 02:47:30,846 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Edge.browser_essentials_safety_esm_protections data: 2 -> 0 rows\n",
      "2025-10-16 02:47:30,848 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'binary': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,849 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'binary': 'timestamp'\n",
      "2025-10-16 02:47:30,851 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for timestamp\n",
      "2025-10-16 02:47:30,855 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for timestamp\n",
      "2025-10-16 02:47:30,860 - DevAnaly - DEBUG - ⏰ [DEV] Column 'timestamp' contributed 98 valid rows for filtering.\n",
      "2025-10-16 02:47:30,862 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to binary\n",
      "2025-10-16 02:47:30,864 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: binary\n",
      "2025-10-16 02:47:30,866 - DevAnaly - DEBUG - Dropped columns for binary: ['error', 'success', 'file_type']\n",
      "2025-10-16 02:47:30,868 - DevAnaly - INFO - 🌐 [DEV] Filtered binary: 98 -> 98 rows\n",
      "2025-10-16 02:47:30,869 - DevAnaly - DEBUG - 🔧 [DEV] Filtered binary data: 98 -> 98 rows\n",
      "2025-10-16 02:47:30,871 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'browser_collected_files': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,872 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'browser_collected_files': 'timestamp'\n",
      "2025-10-16 02:47:30,873 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for timestamp\n",
      "2025-10-16 02:47:30,878 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for timestamp\n",
      "2025-10-16 02:47:30,882 - DevAnaly - DEBUG - ⏰ [DEV] Column 'timestamp' contributed 20 valid rows for filtering.\n",
      "2025-10-16 02:47:30,884 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to browser_collected_files\n",
      "2025-10-16 02:47:30,886 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: browser_collected_files\n",
      "2025-10-16 02:47:30,889 - DevAnaly - DEBUG - Dropped columns for browser_collected_files: ['error', 'success', 'file_type']\n",
      "2025-10-16 02:47:30,891 - DevAnaly - INFO - 🌐 [DEV] Filtered browser_collected_files: 20 -> 20 rows\n",
      "2025-10-16 02:47:30,893 - DevAnaly - DEBUG - 🔧 [DEV] Filtered browser_collected_files data: 20 -> 20 rows\n",
      "2025-10-16 02:47:30,894 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'browser_discovered_profiles': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:30,896 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'browser_discovered_profiles', skipping time filtering.\n",
      "2025-10-16 02:47:30,897 - DevAnaly - INFO - 🔎 [DEV] Applying 'BROWSER' filter to browser_discovered_profiles\n",
      "2025-10-16 02:47:30,899 - DevAnaly - DEBUG - 📁 [DEV] Processing browser file: browser_discovered_profiles\n",
      "2025-10-16 02:47:30,901 - DevAnaly - DEBUG - Dropped columns for browser_discovered_profiles: ['exists']\n",
      "2025-10-16 02:47:30,902 - DevAnaly - INFO - 🌐 [DEV] Filtered browser_discovered_profiles: 5 -> 5 rows\n",
      "2025-10-16 02:47:30,904 - DevAnaly - DEBUG - 🔧 [DEV] Filtered browser_discovered_profiles data: 5 -> 5 rows\n",
      "2025-10-16 02:47:30,905 - DevAnaly - INFO - 📊 [DEV] BROWSER filtering summary: 154,587 -> 54,075 rows (reduction: 100,512 rows, 65.0%)\n",
      "2025-10-16 02:47:30,906 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): browser_statistics\n",
      "2025-10-16 02:47:30,908 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.segments\n",
      "2025-10-16 02:47:30,909 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.segment_usage\n",
      "2025-10-16 02:47:30,911 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.content_annotations\n",
      "2025-10-16 02:47:30,912 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.context_annotations\n",
      "2025-10-16 02:47:30,913 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.stats\n",
      "2025-10-16 02:47:30,914 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.sync_model_metadata\n",
      "2025-10-16 02:47:30,916 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.insecure_credentials\n",
      "2025-10-16 02:47:30,917 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.cookies\n",
      "2025-10-16 02:47:30,918 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.credit_cards\n",
      "2025-10-16 02:47:30,920 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.token_service\n",
      "2025-10-16 02:47:30,921 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.autofill_model_type_state\n",
      "2025-10-16 02:47:30,922 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.visit_source\n",
      "2025-10-16 02:47:30,923 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.history_sync_metadata\n",
      "2025-10-16 02:47:30,924 - DevAnaly - INFO - ⏭️ [DEV] Skipping empty file: Chrome.addresses (0 rows)\n",
      "2025-10-16 02:47:30,925 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.local_stored_cvc\n",
      "2025-10-16 02:47:30,927 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.address_type_tokens\n",
      "2025-10-16 02:47:30,928 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Chrome.payments_customer_data\n",
      "2025-10-16 02:47:30,929 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.segments\n",
      "2025-10-16 02:47:30,930 - DevAnaly - INFO - ⏭️ [DEV] Skipping empty file: Edge.downloads (0 rows)\n",
      "2025-10-16 02:47:30,932 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.segment_usage\n",
      "2025-10-16 02:47:30,933 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.content_annotations\n",
      "2025-10-16 02:47:30,934 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.context_annotations\n",
      "2025-10-16 02:47:30,935 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.stats\n",
      "2025-10-16 02:47:30,936 - DevAnaly - INFO - ⏭️ [DEV] Skipping empty file: Edge.logins (0 rows)\n",
      "2025-10-16 02:47:30,938 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.breached\n",
      "2025-10-16 02:47:30,939 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.logins_edge_extended\n",
      "2025-10-16 02:47:30,940 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.cookies\n",
      "2025-10-16 02:47:30,941 - DevAnaly - INFO - ⏭️ [DEV] Skipping empty file: Edge.keywords (0 rows)\n",
      "2025-10-16 02:47:30,943 - DevAnaly - INFO - ⏭️ [DEV] Skipping empty file: Edge.addresses (0 rows)\n",
      "2025-10-16 02:47:30,944 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.edge_meta\n",
      "2025-10-16 02:47:30,945 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.token_service\n",
      "2025-10-16 02:47:30,946 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.address_type_tokens\n",
      "2025-10-16 02:47:30,948 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.autofill_edge_extended\n",
      "2025-10-16 02:47:30,949 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.safety_hub_navigations\n",
      "2025-10-16 02:47:30,950 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.autofill_edge_field_values\n",
      "2025-10-16 02:47:30,951 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.autofill_profile_edge_extended\n",
      "2025-10-16 02:47:30,952 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.autofill_edge_field_client_info\n",
      "2025-10-16 02:47:30,953 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.autofill_edge_fieldid_cid_mapping\n",
      "2025-10-16 02:47:30,954 - DevAnaly - DEBUG - ⏭️ [DEV] Skipping browser file (not in whitelist): Edge.browser_essentials_safety_esm_protections\n",
      "2025-10-16 02:47:30,956 - DevAnaly - INFO - 📁 [DEV] Created category directory: filtered_data\\browser_20251016_024730\n",
      "2025-10-16 02:47:31,196 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Chrome.urls.csv (14384 rows)\n",
      "2025-10-16 02:47:31,591 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Chrome.visits.csv (29146 rows)\n",
      "2025-10-16 02:47:31,608 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Chrome.downloads.csv (165 rows)\n",
      "2025-10-16 02:47:31,635 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Chrome.visited_links.csv (3155 rows)\n",
      "2025-10-16 02:47:31,644 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Chrome.downloads_url_chains.csv (717 rows)\n",
      "2025-10-16 02:47:31,653 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Chrome.keyword_search_terms.csv (2357 rows)\n",
      "2025-10-16 02:47:31,662 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Chrome.logins.csv (84 rows)\n",
      "2025-10-16 02:47:31,674 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Chrome.sync_entities_metadata.csv (809 rows)\n",
      "2025-10-16 02:47:31,692 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Chrome.autofill.csv (817 rows)\n",
      "2025-10-16 02:47:31,698 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Chrome.keywords.csv (11 rows)\n",
      "2025-10-16 02:47:31,713 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Chrome.autofill_sync_metadata.csv (1809 rows)\n",
      "2025-10-16 02:47:31,719 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Edge.urls.csv (56 rows)\n",
      "2025-10-16 02:47:31,725 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Edge.visits.csv (84 rows)\n",
      "2025-10-16 02:47:31,729 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Edge.visited_links.csv (10 rows)\n",
      "2025-10-16 02:47:31,735 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Edge.downloads_url_chains.csv (338 rows)\n",
      "2025-10-16 02:47:31,740 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Edge.keyword_search_terms.csv (5 rows)\n",
      "2025-10-16 02:47:31,745 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\Edge.autofill.csv (5 rows)\n",
      "2025-10-16 02:47:31,751 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\binary.csv (98 rows)\n",
      "2025-10-16 02:47:31,756 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\browser_collected_files.csv (20 rows)\n",
      "2025-10-16 02:47:31,760 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\browser_20251016_024730\\browser_discovered_profiles.csv (5 rows)\n",
      "2025-10-16 02:47:31,761 - DevAnaly - INFO - ✅ [DEV] Saved 20 files, skipped 35 files, empty 5 files to: filtered_data\\browser_20251016_024730\n",
      "2025-10-16 02:47:31,762 - DevAnaly - INFO - ✅ [DEV] Completed _filter_data for category: BROWSER\n",
      "2025-10-16 02:47:31,763 - Analyzer - DEBUG - Filtering data success: BROWSER\n",
      "2025-10-16 02:47:32,509 - Analyzer - DEBUG - Sending data to backend for category: BROWSER\n",
      "2025-10-16 02:47:32,511 - Analyzer - DEBUG - 📦 Sending 54075 filtered artifacts for category: BROWSER\n",
      "2025-10-16 02:47:32,512 - BackendClient - INFO - 🧪 [TEST] Received 54075 artifacts for validation\n",
      "2025-10-16 02:47:32,886 - BackendClient - INFO - ✅ [TEST] Successfully validated and stored 54075 artifacts\n",
      "2025-10-16 02:47:32,887 - BackendClient - INFO - 🔢 [TEST] Total artifacts stored: 54075\n",
      "2025-10-16 02:47:32,889 - Analyzer - DEBUG - ✅ Successfully sent 54075 artifacts for category: BROWSER\n",
      "2025-10-16 02:47:32,890 - Analyzer - DEBUG - 📋 Stored 54075 complete artifact objects\n",
      "2025-10-16 02:47:32,891 - Analyzer - DEBUG - 🔢 Total artifacts stored so far: 54075\n",
      "2025-10-16 02:47:33,025 - Analyzer - DEBUG - 🔄 Processing category: DELETED\n",
      "2025-10-16 02:47:33,027 - DevAnaly - DEBUG - Starting data load for category: DELETED\n",
      "2025-10-16 02:47:33,052 - unknown_data.test - ERROR - Failed to load data for session-20251002-050523-6de09ba4/deleted: task_id 'session-20251002-050523-6de09ba4'와 module_type 'DELETED_FILES'에 해당하는 데이터를 찾을 수 없습니다.\n",
      "2025-10-16 02:47:33,054 - DevAnaly - WARNING - ⚠️ [DEV] Failed to load data for category DELETED: 데이터 로드 실패 [session-20251002-050523-6de09ba4/deleted]: task_id 'session-20251002-050523-6de09ba4'와 module_type 'DELETED_FILES'에 해당하는 데이터를 찾을 수 없습니다.\n",
      "2025-10-16 02:47:33,055 - DevAnaly - INFO - ⏭️ [DEV] Skipping category DELETED - no data available\n",
      "2025-10-16 02:47:33,057 - Analyzer - DEBUG - Filtering data for category: DELETED\n",
      "2025-10-16 02:47:33,058 - DevAnaly - INFO - 🔧 [DEV] Starting _filter_data for category: DELETED\n",
      "2025-10-16 02:47:33,059 - DevAnaly - INFO - ⏭️ [DEV] No data to filter for category: DELETED\n",
      "2025-10-16 02:47:33,059 - Analyzer - DEBUG - Filtering data success: DELETED\n",
      "2025-10-16 02:47:33,061 - Analyzer - DEBUG - Sending data to backend for category: DELETED\n",
      "2025-10-16 02:47:33,062 - Analyzer - DEBUG - 📦 Sending 0 filtered artifacts for category: DELETED\n",
      "2025-10-16 02:47:33,064 - BackendClient - INFO - 🧪 [TEST] Received 0 artifacts for validation\n",
      "2025-10-16 02:47:33,065 - BackendClient - INFO - ✅ [TEST] Successfully validated and stored 0 artifacts\n",
      "2025-10-16 02:47:33,067 - BackendClient - INFO - 🔢 [TEST] Total artifacts stored: 54075\n",
      "2025-10-16 02:47:33,069 - Analyzer - DEBUG - ✅ Successfully sent 0 artifacts for category: DELETED\n",
      "2025-10-16 02:47:33,070 - Analyzer - DEBUG - 📋 Stored 0 complete artifact objects\n",
      "2025-10-16 02:47:33,072 - Analyzer - DEBUG - 🔢 Total artifacts stored so far: 54075\n",
      "2025-10-16 02:47:33,185 - Analyzer - DEBUG - 🔄 Processing category: LNK\n",
      "2025-10-16 02:47:33,186 - DevAnaly - DEBUG - Starting data load for category: LNK\n",
      "2025-10-16 02:47:33,424 - DevAnaly - DEBUG - Successfully loaded 2 dataframes for category: LNK\n",
      "2025-10-16 02:47:33,425 - Analyzer - DEBUG - Filtering data for category: LNK\n",
      "2025-10-16 02:47:33,426 - DevAnaly - INFO - 🔧 [DEV] Starting _filter_data for category: LNK\n",
      "2025-10-16 02:47:33,427 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'lnk_files': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:33,428 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'lnk_files': 'target_info__target_times__access'\n",
      "2025-10-16 02:47:33,430 - DevAnaly - DEBUG - 🔍 [DEV] Detected ISO format for target_info__target_times__access\n",
      "2025-10-16 02:47:33,434 - DevAnaly - DEBUG - ⏰ [DEV] Column 'target_info__target_times__access' contributed 46 valid rows for filtering.\n",
      "2025-10-16 02:47:33,438 - DevAnaly - INFO - ⏰ [DEV] Time filtering for lnk_files: 197 -> 46 rows\n",
      "2025-10-16 02:47:33,440 - DevAnaly - INFO - 🔎 [DEV] Applying 'LNK' filter to lnk_files\n",
      "2025-10-16 02:47:33,446 - DevAnaly - DEBUG - 🔧 [DEV] Filtered lnk_files data: 197 -> 41 rows\n",
      "2025-10-16 02:47:33,447 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'search_directories': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:33,449 - DevAnaly - WARNING - ℹ️ [DEV] No time columns found for 'search_directories', skipping time filtering.\n",
      "2025-10-16 02:47:33,450 - DevAnaly - INFO - 🔎 [DEV] Applying 'LNK' filter to search_directories\n",
      "2025-10-16 02:47:33,451 - DevAnaly - DEBUG - 🔧 [DEV] Filtered search_directories data: 1 -> 1 rows\n",
      "2025-10-16 02:47:33,452 - DevAnaly - INFO - 📊 [DEV] LNK filtering summary: 198 -> 42 rows (reduction: 156 rows, 78.8%)\n",
      "2025-10-16 02:47:33,455 - DevAnaly - INFO - 📁 [DEV] Created category directory: filtered_data\\lnk_20251016_024733\n",
      "2025-10-16 02:47:33,462 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\lnk_20251016_024733\\lnk_files.csv (41 rows)\n",
      "2025-10-16 02:47:33,466 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\lnk_20251016_024733\\search_directories.csv (1 rows)\n",
      "2025-10-16 02:47:33,468 - DevAnaly - INFO - ✅ [DEV] Saved 2 files, skipped 0 files, empty 0 files to: filtered_data\\lnk_20251016_024733\n",
      "2025-10-16 02:47:33,470 - DevAnaly - INFO - ✅ [DEV] Completed _filter_data for category: LNK\n",
      "2025-10-16 02:47:33,471 - Analyzer - DEBUG - Filtering data success: LNK\n",
      "2025-10-16 02:47:33,478 - Analyzer - DEBUG - Sending data to backend for category: LNK\n",
      "2025-10-16 02:47:33,480 - Analyzer - DEBUG - 📦 Sending 42 filtered artifacts for category: LNK\n",
      "2025-10-16 02:47:33,482 - BackendClient - INFO - 🧪 [TEST] Received 42 artifacts for validation\n",
      "2025-10-16 02:47:33,483 - BackendClient - INFO - ✅ [TEST] Successfully validated and stored 42 artifacts\n",
      "2025-10-16 02:47:33,485 - BackendClient - INFO - 🔢 [TEST] Total artifacts stored: 54117\n",
      "2025-10-16 02:47:33,487 - Analyzer - DEBUG - ✅ Successfully sent 42 artifacts for category: LNK\n",
      "2025-10-16 02:47:33,488 - Analyzer - DEBUG - 📋 Stored 42 complete artifact objects\n",
      "2025-10-16 02:47:33,490 - Analyzer - DEBUG - 🔢 Total artifacts stored so far: 54117\n",
      "2025-10-16 02:47:33,601 - Analyzer - DEBUG - 🔄 Processing category: MESSENGER\n",
      "2025-10-16 02:47:33,602 - DevAnaly - DEBUG - Starting data load for category: MESSENGER\n",
      "2025-10-16 02:47:38,238 - DevAnaly - DEBUG - Successfully loaded 2 dataframes for category: MESSENGER\n",
      "2025-10-16 02:47:38,239 - Analyzer - DEBUG - Filtering data for category: MESSENGER\n",
      "2025-10-16 02:47:38,240 - DevAnaly - INFO - 🔧 [DEV] Starting _filter_data for category: MESSENGER\n",
      "2025-10-16 02:47:38,242 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'Discord.files': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:38,243 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'Discord.files': 'last_modified'\n",
      "2025-10-16 02:47:38,245 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for last_modified\n",
      "2025-10-16 02:47:38,251 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_modified\n",
      "2025-10-16 02:47:38,256 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_modified' contributed 2366 valid rows for filtering.\n",
      "2025-10-16 02:47:38,260 - DevAnaly - INFO - ⏰ [DEV] Time filtering for Discord.files: 4373 -> 2366 rows\n",
      "2025-10-16 02:47:38,261 - DevAnaly - INFO - 🔎 [DEV] Applying 'MESSENGER' filter to Discord.files\n",
      "2025-10-16 02:47:38,267 - DevAnaly - INFO - Filtered out 37 rows based on 13 excluded extensions.\n",
      "2025-10-16 02:47:38,268 - DevAnaly - INFO - 📱 [DEV] Processing Discord: 2329 rows\n",
      "2025-10-16 02:47:38,269 - DevAnaly - DEBUG - 🔧 [DEV] Filtered Discord.files data: 4373 -> 2329 rows\n",
      "2025-10-16 02:47:38,271 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'KakaoTalk.files': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:38,272 - DevAnaly - INFO - 🎯 [DEV] Found specific time column for 'KakaoTalk.files': 'last_modified'\n",
      "2025-10-16 02:47:38,281 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_modified\n",
      "2025-10-16 02:47:38,294 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_modified' contributed 3547 valid rows for filtering.\n",
      "2025-10-16 02:47:38,300 - DevAnaly - INFO - ⏰ [DEV] Time filtering for KakaoTalk.files: 17378 -> 3547 rows\n",
      "2025-10-16 02:47:38,301 - DevAnaly - INFO - 🔎 [DEV] Applying 'MESSENGER' filter to KakaoTalk.files\n",
      "2025-10-16 02:47:38,307 - DevAnaly - INFO - Filtered out 2 rows based on 13 excluded extensions.\n",
      "2025-10-16 02:47:38,309 - DevAnaly - INFO - 📱 [DEV] Processing KakaoTalk: 3545 rows\n",
      "2025-10-16 02:47:38,310 - DevAnaly - DEBUG - 🔧 [DEV] Filtered KakaoTalk.files data: 17378 -> 3545 rows\n",
      "2025-10-16 02:47:38,312 - DevAnaly - INFO - 📊 [DEV] MESSENGER filtering summary: 21,751 -> 5,874 rows (reduction: 15,877 rows, 73.0%)\n",
      "2025-10-16 02:47:38,314 - DevAnaly - INFO - 📁 [DEV] Created category directory: filtered_data\\messenger_20251016_024738\n",
      "2025-10-16 02:47:38,335 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\messenger_20251016_024738\\Discord.files.csv (2329 rows)\n",
      "2025-10-16 02:47:38,376 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\messenger_20251016_024738\\KakaoTalk.files.csv (3545 rows)\n",
      "2025-10-16 02:47:38,378 - DevAnaly - INFO - ✅ [DEV] Saved 2 files, skipped 0 files, empty 0 files to: filtered_data\\messenger_20251016_024738\n",
      "2025-10-16 02:47:38,379 - DevAnaly - INFO - ✅ [DEV] Completed _filter_data for category: MESSENGER\n",
      "2025-10-16 02:47:38,380 - Analyzer - DEBUG - Filtering data success: MESSENGER\n",
      "2025-10-16 02:47:38,424 - Analyzer - DEBUG - Sending data to backend for category: MESSENGER\n",
      "2025-10-16 02:47:38,426 - Analyzer - DEBUG - 📦 Sending 5874 filtered artifacts for category: MESSENGER\n",
      "2025-10-16 02:47:38,427 - BackendClient - INFO - 🧪 [TEST] Received 5874 artifacts for validation\n",
      "2025-10-16 02:47:38,456 - BackendClient - INFO - ✅ [TEST] Successfully validated and stored 5874 artifacts\n",
      "2025-10-16 02:47:38,457 - BackendClient - INFO - 🔢 [TEST] Total artifacts stored: 59991\n",
      "2025-10-16 02:47:38,459 - Analyzer - DEBUG - ✅ Successfully sent 5874 artifacts for category: MESSENGER\n",
      "2025-10-16 02:47:38,460 - Analyzer - DEBUG - 📋 Stored 5874 complete artifact objects\n",
      "2025-10-16 02:47:38,461 - Analyzer - DEBUG - 🔢 Total artifacts stored so far: 59991\n",
      "2025-10-16 02:47:38,575 - Analyzer - DEBUG - 🔄 Processing category: PREFETCH\n",
      "2025-10-16 02:47:38,577 - DevAnaly - DEBUG - Starting data load for category: PREFETCH\n",
      "2025-10-16 02:47:38,744 - DevAnaly - DEBUG - Successfully loaded 1 dataframes for category: PREFETCH\n",
      "2025-10-16 02:47:38,745 - Analyzer - DEBUG - Filtering data for category: PREFETCH\n",
      "2025-10-16 02:47:38,746 - DevAnaly - INFO - 🔧 [DEV] Starting _filter_data for category: PREFETCH\n",
      "2025-10-16 02:47:38,748 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'prefetch_files': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:38,749 - DevAnaly - WARNING - ⚠️ [DEV] Specific time column 'last_run_time_1' not found in 'prefetch_files'. Falling back to general search.\n",
      "2025-10-16 02:47:38,751 - DevAnaly - DEBUG - 🔍 [DEV] Detected ISO format for last_run_times_1\n",
      "2025-10-16 02:47:38,755 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_run_times_1' contributed 505 valid rows for filtering.\n",
      "2025-10-16 02:47:38,757 - DevAnaly - DEBUG - 🔍 [DEV] Detected ISO format for last_run_times_2\n",
      "2025-10-16 02:47:38,761 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_run_times_2' contributed 355 valid rows for filtering.\n",
      "2025-10-16 02:47:38,762 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for last_run_times_3\n",
      "2025-10-16 02:47:38,766 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_run_times_3\n",
      "2025-10-16 02:47:38,772 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_run_times_3' contributed 284 valid rows for filtering.\n",
      "2025-10-16 02:47:38,774 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for last_run_times_4\n",
      "2025-10-16 02:47:38,778 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_run_times_4\n",
      "2025-10-16 02:47:38,783 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_run_times_4' contributed 235 valid rows for filtering.\n",
      "2025-10-16 02:47:38,785 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for last_run_times_5\n",
      "2025-10-16 02:47:38,789 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_run_times_5\n",
      "2025-10-16 02:47:38,794 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_run_times_5' contributed 204 valid rows for filtering.\n",
      "2025-10-16 02:47:38,795 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for last_run_times_6\n",
      "2025-10-16 02:47:38,800 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_run_times_6\n",
      "2025-10-16 02:47:38,804 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_run_times_6' contributed 172 valid rows for filtering.\n",
      "2025-10-16 02:47:38,806 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for last_run_times_7\n",
      "2025-10-16 02:47:38,810 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_run_times_7\n",
      "2025-10-16 02:47:38,816 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_run_times_7' contributed 144 valid rows for filtering.\n",
      "2025-10-16 02:47:38,817 - DevAnaly - DEBUG - 🔍 [DEV] Converting categorical data to string for last_run_times_8\n",
      "2025-10-16 02:47:38,821 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for last_run_times_8\n",
      "2025-10-16 02:47:38,826 - DevAnaly - DEBUG - ⏰ [DEV] Column 'last_run_times_8' contributed 134 valid rows for filtering.\n",
      "2025-10-16 02:47:38,829 - DevAnaly - INFO - 🔎 [DEV] Applying 'PREFETCH' filter to prefetch_files\n",
      "2025-10-16 02:47:38,832 - DevAnaly - DEBUG - 🔧 [DEV] Filtered prefetch_files data: 505 -> 505 rows\n",
      "2025-10-16 02:47:38,833 - DevAnaly - INFO - 📊 [DEV] PREFETCH filtering summary: 505 -> 505 rows (reduction: 0 rows, 0.0%)\n",
      "2025-10-16 02:47:38,836 - DevAnaly - INFO - 📁 [DEV] Created category directory: filtered_data\\prefetch_20251016_024738\n",
      "2025-10-16 02:47:38,849 - DevAnaly - INFO - 💾 [DEV] Saved filtered data: filtered_data\\prefetch_20251016_024738\\prefetch_files.csv (505 rows)\n",
      "2025-10-16 02:47:38,851 - DevAnaly - INFO - ✅ [DEV] Saved 1 files, skipped 0 files, empty 0 files to: filtered_data\\prefetch_20251016_024738\n",
      "2025-10-16 02:47:38,852 - DevAnaly - INFO - ✅ [DEV] Completed _filter_data for category: PREFETCH\n",
      "2025-10-16 02:47:38,854 - Analyzer - DEBUG - Filtering data success: PREFETCH\n",
      "2025-10-16 02:47:38,866 - Analyzer - DEBUG - Sending data to backend for category: PREFETCH\n",
      "2025-10-16 02:47:38,867 - Analyzer - DEBUG - 📦 Sending 505 filtered artifacts for category: PREFETCH\n",
      "2025-10-16 02:47:38,869 - BackendClient - INFO - 🧪 [TEST] Received 505 artifacts for validation\n",
      "2025-10-16 02:47:38,872 - BackendClient - INFO - ✅ [TEST] Successfully validated and stored 505 artifacts\n",
      "2025-10-16 02:47:38,874 - BackendClient - INFO - 🔢 [TEST] Total artifacts stored: 60496\n",
      "2025-10-16 02:47:38,875 - Analyzer - DEBUG - ✅ Successfully sent 505 artifacts for category: PREFETCH\n",
      "2025-10-16 02:47:38,876 - Analyzer - DEBUG - 📋 Stored 505 complete artifact objects\n",
      "2025-10-16 02:47:38,877 - Analyzer - DEBUG - 🔢 Total artifacts stored so far: 60496\n",
      "2025-10-16 02:47:38,990 - Analyzer - DEBUG - 🔄 Processing category: USB\n",
      "2025-10-16 02:47:38,992 - DevAnaly - DEBUG - Starting data load for category: USB\n",
      "2025-10-16 02:47:39,017 - DevAnaly - DEBUG - Successfully loaded 1 dataframes for category: USB\n",
      "2025-10-16 02:47:39,018 - Analyzer - DEBUG - Filtering data for category: USB\n",
      "2025-10-16 02:47:39,020 - DevAnaly - INFO - 🔧 [DEV] Starting _filter_data for category: USB\n",
      "2025-10-16 02:47:39,021 - DevAnaly - DEBUG - ⏰ [DEV] Time filtering for 'usb_devices': Keeping data from 2025-03-16 to 2025-10-16\n",
      "2025-10-16 02:47:39,022 - DevAnaly - WARNING - ⚠️ [DEV] Specific time column 'setupapi_info__last_connection_time' not found in 'usb_devices'. Falling back to general search.\n",
      "2025-10-16 02:47:39,025 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for connection_times__first_install_time\n",
      "2025-10-16 02:47:39,030 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for connection_times__last_connection_time\n",
      "2025-10-16 02:47:39,034 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for connection_times__first_connection_time\n",
      "2025-10-16 02:47:39,040 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for connection_times__boot_after_connect_time\n",
      "2025-10-16 02:47:39,045 - DevAnaly - DEBUG - 🔍 [DEV] Using fallback datetime conversion for connection_times__last_disconnection_time\n",
      "2025-10-16 02:47:39,048 - DevAnaly - INFO - ⏰ [DEV] Time filtering for usb_devices: 1 -> 0 rows\n",
      "2025-10-16 02:47:39,050 - DevAnaly - INFO - 🔎 [DEV] Applying 'USB' filter to usb_devices\n",
      "2025-10-16 02:47:39,052 - DevAnaly - DEBUG - 🔧 [DEV] Filtered usb_devices data: 1 -> 0 rows\n",
      "2025-10-16 02:47:39,054 - DevAnaly - INFO - 📊 [DEV] USB filtering summary: 1 -> 0 rows (reduction: 1 rows, 100.0%)\n",
      "2025-10-16 02:47:39,055 - DevAnaly - INFO - ⏭️ [DEV] Skipping empty file: usb_devices (0 rows)\n",
      "2025-10-16 02:47:39,056 - DevAnaly - INFO - ⏭️ [DEV] No files to save for category: USB (all files empty or skipped)\n",
      "2025-10-16 02:47:39,058 - DevAnaly - INFO - ✅ [DEV] Completed _filter_data for category: USB\n",
      "2025-10-16 02:47:39,059 - Analyzer - DEBUG - Filtering data success: USB\n",
      "2025-10-16 02:47:39,062 - Analyzer - DEBUG - Sending data to backend for category: USB\n",
      "2025-10-16 02:47:39,063 - Analyzer - DEBUG - 📦 Sending 0 filtered artifacts for category: USB\n",
      "2025-10-16 02:47:39,064 - BackendClient - INFO - 🧪 [TEST] Received 0 artifacts for validation\n",
      "2025-10-16 02:47:39,066 - BackendClient - INFO - ✅ [TEST] Successfully validated and stored 0 artifacts\n",
      "2025-10-16 02:47:39,067 - BackendClient - INFO - 🔢 [TEST] Total artifacts stored: 60496\n",
      "2025-10-16 02:47:39,068 - Analyzer - DEBUG - ✅ Successfully sent 0 artifacts for category: USB\n",
      "2025-10-16 02:47:39,069 - Analyzer - DEBUG - 📋 Stored 0 complete artifact objects\n",
      "2025-10-16 02:47:39,070 - Analyzer - DEBUG - 🔢 Total artifacts stored so far: 60496\n",
      "2025-10-16 02:47:39,184 - Analyzer - INFO - Artifact processing completed successfully for task: session-20251002-050523-6de09ba4\n",
      "2025-10-16 02:47:39,186 - BackendClient - INFO - 🧪 [TEST] Received analysis result for behavior: acquisition\n",
      "2025-10-16 02:47:39,188 - BackendClient - INFO - 📋 [TEST] Analysis Result Details:\n",
      "2025-10-16 02:47:39,189 - BackendClient - INFO -   - Behavior: acquisition\n",
      "2025-10-16 02:47:39,190 - BackendClient - INFO -   - Risk Level: \n",
      "2025-10-16 02:47:39,191 - BackendClient - INFO -   - Summary: \n",
      "2025-10-16 02:47:39,193 - BackendClient - INFO -   - Artifact Count: 0\n",
      "2025-10-16 02:47:39,194 - BackendClient - INFO - ✅ [TEST] Successfully stored analysis result with ID: test_analysis_1\n",
      "2025-10-16 02:47:39,194 - Analyzer - DEBUG - Successfully saved analysis result for behavior: BehaviorType.acquisition (ID: test_analysis_1)\n",
      "2025-10-16 02:47:39,196 - BackendClient - INFO - 🧪 [TEST] Received analysis result for behavior: forgery\n",
      "2025-10-16 02:47:39,197 - BackendClient - INFO - 📋 [TEST] Analysis Result Details:\n",
      "2025-10-16 02:47:39,198 - BackendClient - INFO -   - Behavior: forgery\n",
      "2025-10-16 02:47:39,199 - BackendClient - INFO -   - Risk Level: \n",
      "2025-10-16 02:47:39,201 - BackendClient - INFO -   - Summary: \n",
      "2025-10-16 02:47:39,202 - BackendClient - INFO -   - Artifact Count: 0\n",
      "2025-10-16 02:47:39,203 - BackendClient - INFO - ✅ [TEST] Successfully stored analysis result with ID: test_analysis_2\n",
      "2025-10-16 02:47:39,205 - Analyzer - DEBUG - Successfully saved analysis result for behavior: BehaviorType.forgery (ID: test_analysis_2)\n",
      "2025-10-16 02:47:39,206 - BackendClient - INFO - 🧪 [TEST] Received analysis result for behavior: upload\n",
      "2025-10-16 02:47:39,207 - BackendClient - INFO - 📋 [TEST] Analysis Result Details:\n",
      "2025-10-16 02:47:39,208 - BackendClient - INFO -   - Behavior: upload\n",
      "2025-10-16 02:47:39,210 - BackendClient - INFO -   - Risk Level: \n",
      "2025-10-16 02:47:39,211 - BackendClient - INFO -   - Summary: \n",
      "2025-10-16 02:47:39,212 - BackendClient - INFO -   - Artifact Count: 0\n",
      "2025-10-16 02:47:39,213 - BackendClient - INFO - ✅ [TEST] Successfully stored analysis result with ID: test_analysis_3\n",
      "2025-10-16 02:47:39,215 - Analyzer - DEBUG - Successfully saved analysis result for behavior: BehaviorType.upload (ID: test_analysis_3)\n",
      "2025-10-16 02:47:39,216 - BackendClient - INFO - 🧪 [TEST] Received analysis result for behavior: deletion\n",
      "2025-10-16 02:47:39,217 - BackendClient - INFO - 📋 [TEST] Analysis Result Details:\n",
      "2025-10-16 02:47:39,219 - BackendClient - INFO -   - Behavior: deletion\n",
      "2025-10-16 02:47:39,220 - BackendClient - INFO -   - Risk Level: \n",
      "2025-10-16 02:47:39,221 - BackendClient - INFO -   - Summary: \n",
      "2025-10-16 02:47:39,222 - BackendClient - INFO -   - Artifact Count: 0\n",
      "2025-10-16 02:47:39,223 - BackendClient - INFO - ✅ [TEST] Successfully stored analysis result with ID: test_analysis_4\n",
      "2025-10-16 02:47:39,224 - Analyzer - DEBUG - Successfully saved analysis result for behavior: BehaviorType.deletion (ID: test_analysis_4)\n",
      "2025-10-16 02:47:39,225 - BackendClient - INFO - 🧪 [TEST] Received analysis result for behavior: etc\n",
      "2025-10-16 02:47:39,227 - BackendClient - INFO - 📋 [TEST] Analysis Result Details:\n",
      "2025-10-16 02:47:39,228 - BackendClient - INFO -   - Behavior: etc\n",
      "2025-10-16 02:47:39,229 - BackendClient - INFO -   - Risk Level: \n",
      "2025-10-16 02:47:39,230 - BackendClient - INFO -   - Summary: \n",
      "2025-10-16 02:47:39,231 - BackendClient - INFO -   - Artifact Count: 0\n",
      "2025-10-16 02:47:39,233 - BackendClient - INFO - ✅ [TEST] Successfully stored analysis result with ID: test_analysis_5\n",
      "2025-10-16 02:47:39,234 - Analyzer - DEBUG - Successfully saved analysis result for behavior: BehaviorType.etc (ID: test_analysis_5)\n",
      "2025-10-16 02:47:39,235 - Analyzer - INFO - Analysis process completed successfully for task: session-20251002-050523-6de09ba4\n",
      "2025-10-16 02:47:39,247 - testAnalyzer - INFO - 📊 [TEST] Summary: {\n",
      "    \"total_artifacts\": 60496,\n",
      "    \"total_analysis_results\": 5,\n",
      "    \"artifacts_by_type\": {\n",
      "        \"Chrome.urls_data\": 14384,\n",
      "        \"Chrome.visits_data\": 29146,\n",
      "        \"Chrome.downloads_data\": 165,\n",
      "        \"Chrome.visited_links_data\": 3155,\n",
      "        \"Chrome.downloads_url_chains_data\": 717,\n",
      "        \"Chrome.keyword_search_terms_data\": 2357,\n",
      "        \"Chrome.logins_data\": 84,\n",
      "        \"Chrome.sync_entities_metadata_data\": 809,\n",
      "        \"Chrome.autofill_data\": 817,\n",
      "        \"Chrome.keywords_data\": 11,\n",
      "        \"Chrome.autofill_sync_metadata_data\": 1809,\n",
      "        \"Edge.urls_data\": 56,\n",
      "        \"Edge.visits_data\": 84,\n",
      "        \"Edge.visited_links_data\": 10,\n",
      "        \"Edge.downloads_url_chains_data\": 338,\n",
      "        \"Edge.keyword_search_terms_data\": 5,\n",
      "        \"Edge.autofill_data\": 5,\n",
      "        \"binary_data\": 98,\n",
      "        \"browser_collected_files_data\": 20,\n",
      "        \"browser_discovered_profiles_data\": 5,\n",
      "        \"lnk_files_data\": 41,\n",
      "        \"search_directories_data\": 1,\n",
      "        \"Discord.files_data\": 2329,\n",
      "        \"KakaoTalk.files_data\": 3545,\n",
      "        \"prefetch_files_data\": 505\n",
      "    },\n",
      "    \"artifacts_by_behavior\": {\n",
      "        \"acquisition\": 0,\n",
      "        \"forgery\": 0,\n",
      "        \"upload\": 0,\n",
      "        \"deletion\": 0,\n",
      "        \"etc\": 0\n",
      "    },\n",
      "    \"artifacts_by_risk_level\": {\n",
      "        \"\": 5\n",
      "    }\n",
      "}\n",
      "2025-10-16 02:47:39,249 - testAnalyzer - INFO - 🎉 [TEST] Test analysis completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'task_id': 'session-20251002-050523-6de09ba4',\n",
       " 'job_id': 'test_job_id',\n",
       " 'summary': {'total_artifacts': 60496,\n",
       "  'total_analysis_results': 5,\n",
       "  'artifacts_by_type': {'Chrome.urls_data': 14384,\n",
       "   'Chrome.visits_data': 29146,\n",
       "   'Chrome.downloads_data': 165,\n",
       "   'Chrome.visited_links_data': 3155,\n",
       "   'Chrome.downloads_url_chains_data': 717,\n",
       "   'Chrome.keyword_search_terms_data': 2357,\n",
       "   'Chrome.logins_data': 84,\n",
       "   'Chrome.sync_entities_metadata_data': 809,\n",
       "   'Chrome.autofill_data': 817,\n",
       "   'Chrome.keywords_data': 11,\n",
       "   'Chrome.autofill_sync_metadata_data': 1809,\n",
       "   'Edge.urls_data': 56,\n",
       "   'Edge.visits_data': 84,\n",
       "   'Edge.visited_links_data': 10,\n",
       "   'Edge.downloads_url_chains_data': 338,\n",
       "   'Edge.keyword_search_terms_data': 5,\n",
       "   'Edge.autofill_data': 5,\n",
       "   'binary_data': 98,\n",
       "   'browser_collected_files_data': 20,\n",
       "   'browser_discovered_profiles_data': 5,\n",
       "   'lnk_files_data': 41,\n",
       "   'search_directories_data': 1,\n",
       "   'Discord.files_data': 2329,\n",
       "   'KakaoTalk.files_data': 3545,\n",
       "   'prefetch_files_data': 505},\n",
       "  'artifacts_by_behavior': {'acquisition': 0,\n",
       "   'forgery': 0,\n",
       "   'upload': 0,\n",
       "   'deletion': 0,\n",
       "   'etc': 0},\n",
       "  'artifacts_by_risk_level': {'': 5}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "테스트 코드\n",
    "\n",
    "테스트 방법:\n",
    "1) 위 class의 메서드를 작성한다.\n",
    "    (1) 꼭 위에 작성되어있는 메서드 하나로 완성할 필요는 없다. 하위 메서드 마음껏 만들어도 된다.\n",
    "2) task_id를 실제 DB에 있는 값으로 설정한다.\n",
    "3) 테스트하고싶은 항목의 주석을 해제하고 테스트를 진행한다.\n",
    "\"\"\"\n",
    "\n",
    "backend_client = BackendClient()\n",
    "analyzer = DevAnalyzer(backend_client)\n",
    "\n",
    "# task_id 직접 설정\n",
    "# task_id = \"session-20250930-060607-59984faf\"\n",
    "task_id = \"session-20251002-050523-6de09ba4\"\n",
    "\n",
    "# 1. filter 테스트(카테고리별로 각각)\n",
    "# analyzer.run_filter_test(task_id, category=Category.BROWSER)\n",
    "# analyzer.run_filter_test(task_id, category=Category.USB)\n",
    "# analyzer.run_filter_test(task_id, category=Category.LNK)\n",
    "# analyzer.run_filter_test(task_id, category=Category.DELETED)\n",
    "# analyzer.run_filter_test(task_id, category=Category.PREFETCH)\n",
    "# analyzer.run_filter_test(task_id, category=Category.MESSENGER)\n",
    "\n",
    "# 2. filter 테스트(모든 카테고리 통합)\n",
    "# analyzer.run_filter_test(task_id)\n",
    "\n",
    "# 3. generate까지 모든 통합 테스트\n",
    "analyzer.run_final_test(task_id)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
