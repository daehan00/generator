import logging
import os
import re
from typing import Dict, List, Set
import pandas as pd
from datetime import datetime
from dateutil.relativedelta import relativedelta

from testAnalyzer import TestAnalyzer, BackendClient, Category, ResultDataFrames, ResultDataFrame
from Analyzer import BehaviorType

class DevAnalyzer(TestAnalyzer):
    def __init__(self, backend_client: BackendClient, time_filter_months: int = 7) -> None:
        super().__init__(backend_client)
        # DevAnalyzer ì „ìš© ë¡œê±° ìƒì„±
        self.dev_logger = logging.getLogger("DevAnaly")
        # í•„í„°ë§ëœ ë°ì´í„° ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.output_dir = "filtered_data"
        # self._create_output_directory()
        
        # ë¸Œë¼ìš°ì € íŒŒì¼ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì„¤ì • (ì‹¤ì œ íŒŒì¼ëª… í˜•íƒœ)
        self.browser_whitelist: Set[str] = {
            "binary", "browser_collected_files", "browser_discovered_profiles",
            "Chrome.keyword_search_terms", "Edge.keyword_search_terms",
            "Chrome.keywords", "Edge.keywords",
            "Chrome.urls", "Edge.urls",
            "Chrome.visits", "Edge.visits", 
            "Chrome.visited_links", "Edge.visited_links",
            "Chrome.autofill", "Edge.autofill",
            # "Chrome.autofill_profiles", "Edge.autofill_profiles",
            "Chrome.addresses", "Edge.addresses",
            "Chrome.autofill_sync_metadata", "Edge.autofill_sync_metadata",
            # "Chrome.sync_entities_metadata", "Edge.sync_entities_metadata", 
            "Chrome.downloads", "Edge.downloads",
            "Chrome.downloads_url_chains", "Edge.downloads_url_chains",
            "Chrome.logins", "Edge.logins"
        }
        
        # ê´‘ê³ /ì¶”ì  ë„ë©”ì¸ ì„¤ì •
        self.ad_tracking_domains: List[str] = [
            'doubleclick.net', 'googlesyndication.com', 'adnxs.com', 
            'google-analytics.com', 'scorecardresearch.com', 'facebook.net', 
            'akamaihd.net', 'cloudfront.net', 'gstatic.com'
        ]
        
        # ì‹œê°„ í•„í„°ë§ ê¸°ê°„ ì„¤ì • (ê°œì›” ìˆ˜)
        self.time_filter_months: int = time_filter_months

        # íŒŒì¼ë³„ íŠ¹ì • ì‹œê°„ ì»¬ëŸ¼ í•„í„°ë§ ê·œì¹™
        self.time_filter_config: Dict[str, str] = {
            "binary": "timestamp",
            "browser_collected_files": "timestamp",
            "Chrome.keywords": "last_visited",
            "Edge.keywords": "last_visited",
            "Chrome.urls": "last_visit_time",
            "Edge.urls": "last_visit_time",
            "Chrome.visits": "visit_time",
            "Edge.visits": "visit_time",
            "Chrome.autofill": "date_last_used",
            "Edge.autofill": "date_last_used",
            "Chrome.downloads": "end_time",
            "Edge.downloads": "end_time",
            "Chrome.logins": "date_last_used",
            "Edge.logins": "date_last_used",
            "Chrome.addresses": "use_date",
            "Edge.addresses": "use_date",
            "mft_deleted_files": "deletion_time",
            "recycle_bin_files": "deleted_time",
            "lnk_files": "target_info__target_times__access",
            "prefetch_files": "last_run_time_1",
            # "usb_devices": "setupapi_info__last_connection_time",
            "KakaoTalk.files": "last_modified",
            "Discord.files": "last_modified"
        }
    
    def _create_output_directory(self) -> None:
        """í•„í„°ë§ëœ ë°ì´í„° ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
            #self.dev_logger.info(f"ğŸ“ [DEV] Created output directory: {self.output_dir}")
    
    def _save_filtered_data(self, category: Category, df_results: ResultDataFrames) -> str:
        """í•„í„°ë§ëœ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥ (ì´ë¯¸ í•„í„°ë§ëœ ë°ì´í„°ë¼ê³  ê°€ì •)"""
        if not df_results or not df_results.data:
            #self.dev_logger.info(f"â­ï¸ [DEV] No data to save for category: {category.name}")
            return ""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        category_dir = os.path.join(self.output_dir, f"{category.name.lower()}_{timestamp}")
        
        # ì €ì¥í•  ë°ì´í„°ê°€ ìˆìœ¼ë¯€ë¡œ ë°”ë¡œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(category_dir)
        #self.dev_logger.info(f"ğŸ“ [DEV] Created category directory: {category_dir}")
        
        saved_count = 0
        for result in df_results.data:
            # íŒŒì¼ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
            safe_filename = re.sub(r'[/\\:*?"<>|]', '_', result.name)
            if not safe_filename.endswith('.csv'):
                safe_filename += '.csv'
            
            file_path = os.path.join(category_dir, safe_filename)
            
            try:
                result.data.to_csv(file_path, index=False, encoding='utf-8-sig')
                #self.dev_logger.info(f"ğŸ’¾ [DEV] Saved filtered data: {file_path} ({len(result.data)} rows)")
                saved_count += 1
            except Exception as e:
                self.dev_logger.error(f"âŒ [DEV] Failed to save {file_path}: {str(e)}")
        
        #self.dev_logger.info(f"âœ… [DEV] Saved {saved_count} files to: {category_dir}")
        return category_dir
    
    def _filter_data(self, category: Category, df_results: ResultDataFrames) -> ResultDataFrames:
        """
        ë°ì´í„° í•„í„°ë§ ì²˜ë¦¬. íŒŒì¼ ì„ ë³„ì„ ë¨¼ì € ìˆ˜í–‰í•œ í›„ ë°ì´í„° í•„í„°ë§ì„ ì ìš©í•œë‹¤.
        ìˆœì„œ: íŒŒì¼ í•„í„°ë§ (í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸, ë¹ˆ íŒŒì¼) â†’ ë°ì´í„° í•„í„°ë§ (ì‹œê°„ â†’ ì—´/í–‰)
        """
        #self.dev_logger.info(f"ğŸ”§ [DEV] Starting _filter_data for category: {category.name}")
        
        if not df_results or not df_results.data:
            self.dev_logger.info(f"â­ï¸ [DEV] No data to filter for category: {category.name}")
            return ResultDataFrames(data=[])

        category_original_rows = 0
        category_filtered_rows = 0
        processed_data = []
        skipped_by_whitelist = 0

        for result in df_results.data:
            # --- 1. íŒŒì¼ ë ˆë²¨ í•„í„°ë§ (ì²˜ë¦¬ ëŒ€ìƒì„ ë¨¼ì € í™•ì •) ---
            if category == Category.BROWSER and result.name not in self.browser_whitelist:
                skipped_by_whitelist += 1
                continue
            
            if result.data.empty:
                continue
            
            original_count = len(result.data)
            category_original_rows += original_count

            # --- 2. ë°ì´í„° ë ˆë²¨ í•„í„°ë§ (í™•ì •ëœ íŒŒì¼ì— ëŒ€í•´ì„œë§Œ ìˆ˜í–‰) ---
            # 2-1. ì‹œê°„ í•„í„°ë§
            df = self._apply_time_filtering(result)
            
            # 2-2. ì¹´í…Œê³ ë¦¬ë³„ ì—´/í–‰ í•„í„°ë§ (ì„ì‹œ ë³€ìˆ˜ì— í•„í„°ë§ ê²°ê³¼ë¥¼ ë‹´ìŒ)
            temp_result = ResultDataFrame(name=result.name, data=df)
            match category:
                case Category.USB:
                    filtered_df = self._filter_usb_data(temp_result)
                case Category.LNK:
                    filtered_df = self._filter_lnk_data(temp_result)
                case Category.MESSENGER:
                    filtered_df = self._filter_messenger_data(temp_result)
                case Category.PREFETCH:
                    filtered_df = self._filter_prefetch_data(temp_result)
                case Category.DELETED:
                    filtered_df = self._filter_deleted_data(temp_result)
                case Category.BROWSER:
                    filtered_df = self._filter_browser_data(temp_result)
                case _:
                    self.dev_logger.warning(f"No specific filter for category {category.name}. Applying default limit.")
                    filtered_df = temp_result.data.head(10)
            
            final_count = len(filtered_df)

            # --- 3. ìµœì¢… ê²°ê³¼ ì¶”ê°€ ---
            # ëª¨ë“  í•„í„°ë§ í›„ ë°ì´í„°ê°€ ë‚¨ì•„ìˆëŠ” ê²½ìš°ì—ë§Œ ìµœì¢… ëª©ë¡ì— ì¶”ê°€
            if final_count > 0:
                result.data = filtered_df
                category_filtered_rows += final_count
                processed_data.append(result)
                self.dev_logger.debug(f"ğŸ”§ [DEV] Filtered {result.name}: {original_count} -> {final_count} rows. Keeping file.")
            else:
                self.dev_logger.info(f"â­ï¸ [DEV] Dropping file {result.name} after filtering (0 rows remaining).")

        # if skipped_by_whitelist > 0:
        #     self.dev_logger.info(f"â­ï¸ [DEV] Skipped {skipped_by_whitelist} browser files (not in whitelist).")
            
        # ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§ í†µê³„ ë¡œê¹…
        reduction = category_original_rows - category_filtered_rows
        reduction_percent = (reduction / category_original_rows * 100) if category_original_rows > 0 else 0
        self.dev_logger.info(f"ğŸ“Š [DEV] {category.name} filtering summary: {category_original_rows:,} -> {category_filtered_rows:,} rows (reduction: {reduction:,} rows, {reduction_percent:.1f}%)")

        # ì „ì²´ í†µê³„ ì—…ë°ì´íŠ¸
        if not hasattr(self, '_total_original_rows'):
            self._total_original_rows = 0
            self._total_filtered_rows = 0
        self._total_original_rows += category_original_rows
        self._total_filtered_rows += category_filtered_rows

        # ìµœì¢…ì ìœ¼ë¡œ í•„í„°ë§ëœ ë°ì´í„°ë¡œ ìƒˆ ê°ì²´ ìƒì„±
        filtered_df_results = ResultDataFrames(data=processed_data)

        # í•„í„°ë§ëœ ë°ì´í„° ì €ì¥
        # self._save_filtered_data(category, filtered_df_results)

        # self.dev_logger.info(f"âœ… [DEV] Completed _filter_data for category: {category.name}")
        return filtered_df_results

    def _apply_time_filtering(self, result: ResultDataFrame) -> pd.DataFrame:
        """
        ì§€ì •ëœ ê°œì›” ìˆ˜ ì´ì „ë¶€í„° í˜„ì¬ê¹Œì§€ì˜ ë°ì´í„°ë§Œ ìœ ì§€í•˜ëŠ” ì‹œê°„ í•„í„°ë§.
        íŒŒì¼ë³„ë¡œ ì§€ì •ëœ ì‹œê°„ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ í•´ë‹¹ ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ì‹œê°„ ì»¬ëŸ¼ì„ ì°¾ì•„ í•„í„°ë§.
        """
        df = result.data
        file_name = result.name

        if df.empty:
            return df

        current_time = datetime.now()
        cutoff_date = current_time - relativedelta(months=self.time_filter_months)
        
        # self.dev_logger.debug(
        #     f"â° [DEV] Time filtering for '{file_name}': Keeping data from "
        #     f"{cutoff_date.strftime('%Y-%m-%d')} to {current_time.strftime('%Y-%m-%d')}"
        # )

        target_columns = []
        
        # 1. íŒŒì¼ë³„ íŠ¹ì • ì‹œê°„ ì»¬ëŸ¼ ê·œì¹™ í™•ì¸
        specific_time_col = self.time_filter_config.get(file_name)
        if specific_time_col and specific_time_col in df.columns:
            # self.dev_logger.info(f"ğŸ¯ [DEV] Found specific time column for '{file_name}': '{specific_time_col}'")
            target_columns.append(specific_time_col)
        else:
            # 2. íŠ¹ì • ê·œì¹™ì´ ì—†ê±°ë‚˜ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´, ì¼ë°˜ì ì¸ ì‹œê°„ ì»¬ëŸ¼ íƒìƒ‰
            # if specific_time_col:
            #      self.dev_logger.warning(f"âš ï¸ [DEV] Specific time column '{specific_time_col}' not found in '{file_name}'. Falling back to general search.")
            
            time_keywords = ['time', 'date', 'created', 'modified', 'access', 'deletion', 'mtime', 'ctime', 'timestamp']
            target_columns = [
                col for col in df.columns 
                if isinstance(col, str) and any(keyword in col.lower() for keyword in time_keywords)
            ]

        if not target_columns:
            self.dev_logger.warning(f"â„¹ï¸ [DEV] No time columns found for '{file_name}', skipping time filtering.")
            return df

        # ì‹œê°„ í•„í„°ë§ ì ìš© (OR ì¡°ê±´)
        mask = pd.Series([False] * len(df), index=df.index)
        
        for col in target_columns:
            try:
                # addresses íŒŒì¼ì˜ use_date ì»¬ëŸ¼ íŠ¹ë³„ ì²˜ë¦¬ (Unix timestamp)
                if 'addresses' in file_name and col == 'use_date':
                    # self.dev_logger.debug(f"ğŸ” [DEV] Converting Unix timestamp in {file_name}")
                    # Unix timestampë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
                    temp_dates = self._smart_datetime_conversion(df[col], col)
                # cookies íŒŒì¼ì˜ WebKit timestamp íŠ¹ë³„ ì²˜ë¦¬
                elif 'cookies' in file_name and col.endswith('_utc'):
                    # self.dev_logger.debug(f"ğŸ” [DEV] Converting WebKit timestamp in {file_name}")
                    # WebKit timestamp (ë§ˆì´í¬ë¡œì´ˆ)ë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
                    temp_dates = self._smart_datetime_conversion(df[col], col)
                # ê¸°íƒ€ WebKit timestamp í˜•ì‹ ì²˜ë¦¬ (í° ìˆ«ì ê°’ë“¤)
                elif col.endswith('_utc') and df[col].dtype in ['int64', 'float64']:
                    # ê°’ì´ ë§¤ìš° í° ê²½ìš° WebKit timestampë¡œ ê°„ì£¼
                    sample_values = df[col].dropna().head(3)
                    if len(sample_values) > 0 and sample_values.iloc[0] > 1e15:  # WebKit timestamp ë²”ìœ„
                        # self.dev_logger.debug(f"ğŸ” [DEV] Converting WebKit timestamp for {col}")
                        temp_dates = self._smart_datetime_conversion(df[col], col)
                    else:
                        # ìˆ«ìí˜• ë°ì´í„°ì¸ ê²½ìš° Unix timestampë¡œ ì‹œë„
                        temp_dates = self._smart_datetime_conversion(df[col], col)
                else:
                    # ì¼ë°˜ì ì¸ datetime ë³€í™˜ - ë¨¼ì € ë°ì´í„° íƒ€ì…ê³¼ ìƒ˜í”Œ ê°’ í™•ì¸
                    temp_dates = self._smart_datetime_conversion(df[col], col)
                
                # íƒ€ì„ì¡´ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ tz_localize ì ìš©
                if hasattr(temp_dates.dt, 'tz') and temp_dates.dt.tz is not None:
                    temp_dates = temp_dates.dt.tz_localize(None)
                
                # ìœ íš¨í•œ ë‚ ì§œì´ê³ , cutoff_date ì´í›„ì¸ ë°ì´í„°ë§Œ ì„ íƒ
                valid_mask = temp_dates.notna() & (temp_dates >= cutoff_date)
                mask |= valid_mask
                
                if valid_mask.any():
                    self.dev_logger.debug(
                        f"â° [DEV] Column '{col}' contributed {valid_mask.sum()} valid rows for filtering."
                    )
            except Exception as e:
                self.dev_logger.warning(f"âš ï¸ [DEV] Could not filter by column '{col}' in '{file_name}': {str(e)}")
                continue
        
        # í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
        return df[mask]

    def _smart_datetime_conversion(self, series: pd.Series, column_name: str) -> pd.Series:
        """
        ìŠ¤ë§ˆíŠ¸í•œ datetime ë³€í™˜ - ë°ì´í„° íƒ€ì…ê³¼ ìƒ˜í”Œ ê°’ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë³€í™˜ ë°©ë²• ì„ íƒ
        """
        # Categorical ë°ì´í„° ì²˜ë¦¬ (ë¨¼ì € ì²˜ë¦¬)
        if isinstance(series.dtype, pd.CategoricalDtype):
            # self.dev_logger.debug(f"ğŸ” [DEV] Converting categorical data to string for {column_name}")
            series = pd.Series(series.astype(str), index=series.index if hasattr(series, 'index') else None)
        
        if series.empty:
            return pd.Series([], dtype='datetime64[ns]')
        
        # ìƒ˜í”Œ ê°’ë“¤ í™•ì¸ (NaN ì œì™¸)
        sample_values = series.dropna().head(5)
        if sample_values.empty:
            return pd.Series([pd.NaT] * len(series), index=series.index)
        
        # ìˆ«ìí˜• ë°ì´í„°ì¸ ê²½ìš° - ë” ì—„ê²©í•œ ì²´í¬
        if pd.api.types.is_numeric_dtype(series) and not pd.api.types.is_string_dtype(series):
            # Unix timestamp ë²”ìœ„ í™•ì¸ (1970-2038ë…„)
            min_val = sample_values.min()
            max_val = sample_values.max()
            
            if min_val > 1e15:  # WebKit timestamp (ë§ˆì´í¬ë¡œì´ˆ)
                # self.dev_logger.debug(f"ğŸ” [DEV] Detected WebKit timestamp for {column_name}")
                return pd.to_datetime(series, unit='us', errors='coerce')
            elif min_val > 1e9:  # Unix timestamp (ì´ˆ)
                # self.dev_logger.debug(f"ğŸ” [DEV] Detected Unix timestamp for {column_name}")
                return pd.to_datetime(series, unit='s', errors='coerce')
            elif min_val > 1e6:  # ë°€ë¦¬ì´ˆ timestamp
                # self.dev_logger.debug(f"ğŸ” [DEV] Detected millisecond timestamp for {column_name}")
                return pd.to_datetime(series, unit='ms', errors='coerce')
        
        # ë¬¸ìì—´ ë°ì´í„°ì´ê±°ë‚˜ ìˆ«ìí˜•ì´ì§€ë§Œ ë²”ìœ„ì— ë§ì§€ ì•ŠëŠ” ê²½ìš°
        sample_str = str(sample_values.iloc[0])
        
        # ISO í˜•ì‹ í™•ì¸
        if 'T' in sample_str and ('+' in sample_str or 'Z' in sample_str):
            # self.dev_logger.debug(f"ğŸ” [DEV] Detected ISO format for {column_name}")
            return pd.to_datetime(series, format='ISO8601', errors='coerce')
        
        # ì¼ë°˜ì ì¸ ë‚ ì§œ í˜•ì‹ë“¤ ì‹œë„
        common_formats = [
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%d',
            '%m/%d/%Y %H:%M:%S',
            '%m/%d/%Y',
            '%d/%m/%Y %H:%M:%S',
            '%d/%m/%Y',
            '%Y-%m-%d %H:%M:%S.%f',
            '%Y-%m-%d %H:%M:%S.%f%z'
        ]
        
        for fmt in common_formats:
            try:
                # ìƒ˜í”Œ ê°’ìœ¼ë¡œ í˜•ì‹ í…ŒìŠ¤íŠ¸
                test_val = pd.to_datetime(sample_str, format=fmt, errors='coerce')
                if not pd.isna(test_val):
                    # self.dev_logger.debug(f"ğŸ” [DEV] Detected format '{fmt}' for {column_name}")
                    return pd.to_datetime(series, format=fmt, errors='coerce')
            except:
                continue
        
        # ëª¨ë“  í˜•ì‹ì´ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ ë³€í™˜ (ê²½ê³  ì–µì œ)
        import warnings
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", UserWarning)
            warnings.simplefilter("ignore", FutureWarning)
            # self.dev_logger.debug(f"ğŸ” [DEV] Using fallback datetime conversion for {column_name}")
            return pd.to_datetime(series, errors='coerce')

    def _filter_browser_data(self, result: ResultDataFrame) -> pd.DataFrame:
        """ë¸Œë¼ìš°ì € ë°ì´í„°ì— ëŒ€í•œ í•„í„°ë§ ê·œì¹™"""
        # self.dev_logger.info(f"ğŸ” [DEV] Applying 'BROWSER' filter to {result.name}")
        df = result.data.copy()
        
        # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì— ì—†ëŠ” íŒŒì¼ì€ ê±´ë„ˆë›°ê¸°
        if result.name not in self.browser_whitelist:
            # self.dev_logger.debug(f"â­ï¸ [DEV] Skipping browser file (not in whitelist): {result.name}")
            return pd.DataFrame()
        
        # íŒŒì¼ëª…ì—ì„œ ì ‘ë‘ì‚¬ ì œê±°
        file_name = result.name.split('.', 1)[1] if result.name.startswith(('Chrome.', 'Edge.')) else result.name
        # self.dev_logger.debug(f"ğŸ“ [DEV] Processing browser file: {file_name}")
        
        # 1. ì—´ í•„í„°ë§
        columns_to_drop = self._get_browser_columns_to_drop(file_name)
        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]
        
        if existing_columns_to_drop:
            df.drop(columns=existing_columns_to_drop, inplace=True)
            # self.dev_logger.debug(f"Dropped columns for {file_name}: {existing_columns_to_drop}")
        
        # ë¡œê·¸ì¸ íŒŒì¼ì˜ ê²½ìš° ì²« ë²ˆì§¸ ì»¬ëŸ¼(ì¸ë±ìŠ¤)ë„ ì œê±°
        if file_name == "logins" and len(df.columns) > 0:
            df.drop(columns=[df.columns[0]], inplace=True)
        
        # 2. í–‰ í•„í„°ë§
        df = self._apply_browser_row_filtering(df, file_name)
        
        # self.dev_logger.info(f"ğŸŒ [DEV] Filtered {file_name}: {len(result.data)} -> {len(df)} rows")
        return df
    
    def _apply_browser_row_filtering(self, df: pd.DataFrame, file_name: str) -> pd.DataFrame:
        """ë¸Œë¼ìš°ì € ë°ì´í„°ì— ëŒ€í•œ í–‰ í•„í„°ë§ ê·œì¹™ ì ìš©"""
        original_count = len(df)
        
        if file_name == "visits":
            if 'visit_duration' in df.columns:
                df['visit_duration'] = pd.to_numeric(df['visit_duration'], errors='coerce')
                df = df[df['visit_duration'] != 0]
                # self.dev_logger.info(f"ğŸ—‘ï¸ [DEV] Removed rows with visit_duration=0")
        
        elif file_name == "urls":
            if 'visit_count' in df.columns:
                df['visit_count'] = pd.to_numeric(df['visit_count'], errors='coerce')
                df = df[df['visit_count'] > 0]
            
            if 'url' in df.columns:
                pattern = '|'.join(self.ad_tracking_domains)
                df = df[~df['url'].str.contains(pattern, case=False, na=False)]
                # self.dev_logger.info(f"ğŸ—‘ï¸ [DEV] Removed ad/tracking domains")
        
        elif file_name == "downloads":
            # ì™„ë£Œëœ ë‹¤ìš´ë¡œë“œë§Œ
            if 'state' in df.columns:
                df = df[df['state'].astype(str) == '1']
            
            # ì„ì‹œ íŒŒì¼ ì œê±°
            if 'target_path' in df.columns:
                temp_pattern = r'\.(?:tmp|crdownload)$'  # non-capturing group ì‚¬ìš©
                df = df[~df['target_path'].str.contains(temp_pattern, case=False, na=False)]
            
            # ë¹ˆ í–‰ ì œê±°
            df = df.dropna(how='all')
            if 'target_path' in df.columns:
                df = df[df['target_path'].notna() & (df['target_path'].astype(str).str.strip() != '')]
            if 'url' in df.columns:
                df = df[df['url'].notna() & (df['url'].astype(str).str.strip() != '')]
        
        elif file_name == "browser_collected_files":
            if 'data_type' in df.columns and 'download_state' in df.columns:
                df = df[(df['data_type'] == 'downloads') & (df['download_state'] == 'completed')]
        
        elif file_name == "sync_entities_metadata":
            # ì‚­ì œëœ í•­ëª© ì œê±°
            if 'is_deleted' in df.columns:
                df = df[df['is_deleted'] != True]
            
            # í´ë”ê°€ ì•„ë‹Œ ì‹¤ì œ ë°ì´í„°ë§Œ ìœ ì§€
            if 'is_folder' in df.columns:
                df = df[df['is_folder'] != True]
            
            # ë¹ˆ í–‰ ì œê±°
            df = df.dropna(how='all')
        
        if original_count != len(df):
            self.dev_logger.info(f"ğŸ—‘ï¸ [DEV] Row filtering: {original_count} -> {len(df)} rows")
        
        return df
    
    def _get_browser_columns_to_drop(self, file_name: str) -> List[str]:
        """ë¸Œë¼ìš°ì € íŒŒì¼ëª…ì— ë”°ë¥¸ ì œê±°í•  ì»¬ëŸ¼ ëª©ë¡ ë°˜í™˜"""
        column_map = {
            "binary": ['error', 'success', 'file_type'],
            "browser_collected_files": ['error', 'success', 'file_type'],
            "browser_discovered_profiles": ['exists'],
            "keyword_search_terms": ['normalized_term'],
            "keywords": [
                'url_hash', 'input_encodings', 'image_url_post_params', 
                'search_url_post_params', 'suggest_url_post_params',
                'alternate_urls', 'safe_for_autoreplace', 'created_from_play_api',
                'image_url', 'favicon_url'
            ],
            "urls": ['favicon_id'],
            "visits": ['incremented_omnibox_typed_score', 'app_id'],
            "autofill": ['value_lower'],
            "addresses": ['language_code'],
            "downloads": ['transient', 'http_method', 'etag', 'last_response_headers'],
            "logins": [
                'password_value', 'username_element', 'password_element', 
                'submit_element', 'display_name', 'icon_url', 'federation_url', 
                'skip_zero_click', 'generation_upload_status', 
                'possible_username_pairs', 'moving_blocked_for_list'
            ],
            "sync_entities_metadata": [
                'model_type', 'storage_key', 'client_tag_hash', 'server_id',
                'specifics_hash', 'base_specifics_hash', 'parent_id', 'version',
                'mtime', 'ctime', 'server_version', 'is_deleted', 'is_folder',
                'unique_position', 'is_bookmark', 'is_folder', 'is_unsynced'
            ]
        }
        return column_map.get(file_name, [])

    def _filter_deleted_data(self, result: ResultDataFrame) -> pd.DataFrame:
        """ì‚­ì œëœ íŒŒì¼ ë°ì´í„°ì— ëŒ€í•œ í•„í„°ë§"""
        # self.dev_logger.info(f"ğŸ” [DEV] Applying 'DELETED' filter to {result.name}")
        df = result.data.copy()
        
        # ì—´ í•„í„°ë§
        columns_to_drop = [
            'access_time_timestamp', 'creation_time_timestamp', 
            'deletion_time_timestamp', 'modification_time_timestamp', 
            'deleted_time_timestamp', 'is_directory', 'parse_status', 
            'recycle_bin', 'recycle_bin_version', 'Unnamed: 0', 'file_index'
        ]
        
        existing_columns = [col for col in columns_to_drop if col in df.columns]
        if existing_columns:
            df.drop(columns=existing_columns, inplace=True)
        
        # í–‰ í•„í„°ë§
        df = self._apply_deleted_row_filtering(df, result.name)
        return df
    
    def _apply_deleted_row_filtering(self, df: pd.DataFrame, file_name: str) -> pd.DataFrame:
        """ì‚­ì œëœ íŒŒì¼ ë°ì´í„°ì— ëŒ€í•œ í–‰ í•„í„°ë§ (ë‚´ë¶€ ìœ ì¶œ ì¦ê±° ë³´ì¡´ ë¡œì§ ê°•í™”)"""
        if 'mft_deleted' not in str(file_name).lower():
            return df
        
        original_count = len(df)
        
        # ì‹œìŠ¤í…œ íŒŒì¼ ì œê±° ($ ì ‘ë‘ì‚¬)
        file_col = 'file_name' if 'file_name' in df.columns else 'FileName'
        if file_col in df.columns:
            df = df[~df[file_col].str.startswith('$', na=False)]
        
        # ì‹œìŠ¤í…œ ê²½ë¡œ ì œê±° ë¡œì§
        path_col = 'full_path' if 'full_path' in df.columns else 'ParentPath'
        if path_col in df.columns:
            # 1. ê¸°ë³¸ì ìœ¼ë¡œ í•„í„°ë§í•  ì‹œìŠ¤í…œ ê²½ë¡œ ëª©ë¡
            system_paths = [
                r'C:\\Windows', r'C:\\Program Files', 
                r'C:\\ProgramData', r'C:\\\$Recycle\.Bin'
            ]
            system_path_pattern = '|'.join(system_paths)
            # ì‹œìŠ¤í…œ ê²½ë¡œì— í•´ë‹¹í•˜ëŠ” íŒŒì¼ë“¤ì„ ì‹ë³„ (True = ì‚­ì œ í›„ë³´)
            is_in_system_path = df[path_col].str.contains(system_path_pattern, case=False, na=False, regex=True)

            # --- âœ¨ ìƒˆë¡œìš´ ì˜ˆì™¸ ë¡œì§ ì‹œì‘ âœ¨ ---
            
            # 2. í•„í„°ë§ì—ì„œ ì œì™¸í•  ì˜ˆì™¸ ì¡°ê±´ ì •ì˜
            # 2-1. ì‹œìŠ¤í…œ ê²½ë¡œì— ìˆë”ë¼ë„ ë³´ì¡´í•´ì•¼ í•  íŒŒì¼ í™•ì¥ì ëª©ë¡
            suspicious_extensions = (
                '.zip', '.rar', '.7z', '.egg',  # ì••ì¶• íŒŒì¼
                '.xlsx', '.xls', '.docx', '.doc', '.pptx', '.ppt', # ì˜¤í”¼ìŠ¤ ë¬¸ì„œ
                '.pdf', '.hwp' # ë¬¸ì„œ íŒŒì¼
            )
            is_suspicious_extension = df[file_col].str.lower().str.endswith(suspicious_extensions, na=False)

            # 2-2. ê³µê²©ìê°€ ì€ë‹‰ì„ ìœ„í•´ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íŠ¹ì • ì„ì‹œ í´ë” ëª©ë¡
            allowed_temp_paths = [
                r'C:\\Windows\\Temp',  # Windows ê³µìš© ì„ì‹œ í´ë”
                r'\\AppData\\Local\\Temp' # ì‚¬ìš©ì í”„ë¡œí•„ ì„ì‹œ í´ë” (ê²½ë¡œì— í¬í•¨ë˜ëŠ”ì§€ ê²€ì‚¬)
            ]
            allowed_temp_pattern = '|'.join(allowed_temp_paths)
            is_in_allowed_temp_path = df[path_col].str.contains(allowed_temp_pattern, case=False, na=False, regex=True)

            # 3. ì˜ˆì™¸ ì¡°ê±´ í†µí•©: í™•ì¥ìê°€ ì˜ì‹¬ìŠ¤ëŸ½ê±°ë‚˜, í—ˆìš©ëœ ì„ì‹œ í´ë”ì— ìˆìœ¼ë©´ ë³´ì¡´ (True = ë³´ì¡´)
            should_be_preserved = is_suspicious_extension | is_in_allowed_temp_path
            
            # --- âœ¨ ì˜ˆì™¸ ë¡œì§ ì¢…ë£Œ âœ¨ ---

            # 4. ìµœì¢… í•„í„°ë§ ì ìš©: 'ì‚­ì œ í›„ë³´'ì´ë©´ì„œ 'ë³´ì¡´' ëŒ€ìƒì´ ì•„ë‹Œ íŒŒì¼ë§Œ ì‹¤ì œ ì‚­ì œ
            rows_to_drop = is_in_system_path & ~should_be_preserved
            df = df[~rows_to_drop]

            # ì‹œìŠ¤í…œ ë¡œê·¸ íŒŒì¼ ì œê±° (ì´ì „ê³¼ ë™ì¼)
            if file_col in df.columns:
                system_files = ['bootex.log', 'LOG', 'setup.log', 'install.log']
                system_pattern = '|'.join(system_files)
                df = df[~df[file_col].str.contains(system_pattern, case=False, na=False)]
        
        if original_count != len(df):
            self.dev_logger.info(f"ğŸ—‘ï¸ [DEV] Filtered: {original_count} -> {len(df)} rows")
        
        return df

    def _filter_lnk_data(self, result: ResultDataFrame) -> pd.DataFrame:
        """LNK ë°ì´í„° í•„í„°ë§"""
        # self.dev_logger.info(f"ğŸ” [DEV] Applying 'LNK' filter to {result.name}")
        df = result.data.copy()
        
        # ì—´ í•„í„°ë§
        static_columns = [
            'header__header_size', 'header__header_size_hex', 
            'link_info__link_info_header_size', 'header__hot_key', 
            'header__icon_index', 'link_info__volume_info__drive_type',
            'link_info__volume_info__drive_type_formatted'
        ]
        
        pattern_columns = [
            col for col in df.columns 
            if isinstance(col, str) and (
                col.endswith(('_time_raw', '_hex', '_timestamp')) or 
                col.startswith('link_info__offsets__')
            )
        ]
        
        columns_to_drop = static_columns + pattern_columns
        existing_columns = [col for col in columns_to_drop if col in df.columns]
        
        if existing_columns:
            df.drop(columns=existing_columns, inplace=True)
        
        # í–‰ í•„í„°ë§
        df = self._apply_lnk_row_filtering(df, result.name)
        return df
    
    def _apply_lnk_row_filtering(self, df: pd.DataFrame, file_name: str) -> pd.DataFrame:
        """LNK í–‰ í•„í„°ë§"""
        if 'lnk_files' not in str(file_name).lower():
            return df
        
        # ì‹œìŠ¤í…œ íŒŒì¼ ì œê±°
        target_col = next((col for col in ['Target_Path', 'target_info__target_path'] 
                          if col in df.columns), None)
        
        if target_col:
            system_keywords = [
                'Windows', 'System32', 'ProgramData', 'AppData', 
                'Update', 'Setup', 'Installer', 'MicrosoftEdge', 
                'Chrome', 'Temp', 'Cache'
            ]
            pattern = '|'.join(system_keywords)
            df = df[~df[target_col].str.contains(pattern, case=False, na=False)]
        
        return df

    def _filter_messenger_data(self, result: ResultDataFrame) -> pd.DataFrame:
        """ë©”ì‹ ì € ë°ì´í„° í•„í„°ë§"""
        # self.dev_logger.info(f"ğŸ” [DEV] Applying 'MESSENGER' filter to {result.name}")
        df = result.data.copy()
        
        # ì—´ í•„í„°ë§
        columns_to_drop = [
            'created_timestamp', 'last_modified_timestamp', 'relative_path',
            'is_valid_file', 'messenger_type', 'file_index'
        ]
        
        existing_columns = [col for col in columns_to_drop if col in df.columns]
        if existing_columns:
            df.drop(columns=existing_columns, inplace=True)
        
        # í–‰ í•„í„°ë§
        df = self._apply_messenger_row_filtering(df, result.name)
        
        messenger_type = "Discord" if "Discord" in result.name else "KakaoTalk" if "KakaoTalk" in result.name else "Unknown"
        self.dev_logger.info(f"ğŸ“± [DEV] Processing {messenger_type}: {len(df)} rows")
        
        return df
    
    def _apply_messenger_row_filtering(self, df: pd.DataFrame, result_name: str) -> pd.DataFrame:
        """ë©”ì‹ ì € ë°ì´í„° í–‰ í•„í„°ë§"""
        if 'file_name' not in df.columns:
            # self.dev_logger.warning("âš ï¸ 'file_name' column not found, skipping extension filtering.")
            return df
        
        # ì œì™¸í•  í™•ì¥ì
        exclude_extensions = (
            ".sys", ".ico", ".cur", ".msi", ".bat", ".sh",
            ".ttf", ".otf", ".editorconfig", ".eslintrc", ".npmignore"
        )
        
        initial_count = len(df)
        mask = ~df['file_name'].str.lower().str.endswith(exclude_extensions, na=False)
        filtered_df = df[mask]
        
        self.dev_logger.info(
            f"Filtered out {initial_count - len(filtered_df)} rows "
            f"based on {len(exclude_extensions)} excluded extensions."
        )
        
        return filtered_df

    def _filter_prefetch_data(self, result: ResultDataFrame) -> pd.DataFrame:
        """Prefetch ë°ì´í„° í•„í„°ë§"""
        self.dev_logger.info(f"ğŸ” [DEV] Applying 'PREFETCH' filter to {result.name}")
        df = result.data.copy()
        
        columns_to_drop = [
            'structure__executable_file_name', 'file_index', 
            'structure__signature', 'structure__format_version', 
            'is_compressed', 'parse_success', 'error_message',
            'structure__trace_chains_offset', 'structure__volumes_info_offset',
            'structure__filename_strings_size', 'structure__filename_strings_offset'
        ]
        
        existing_columns = [col for col in columns_to_drop if col in df.columns]
        if existing_columns:
            df.drop(columns=existing_columns, inplace=True)
        
        return df

    def _filter_usb_data(self, result: ResultDataFrame) -> pd.DataFrame:
        """USB ë°ì´í„° í•„í„°ë§"""
        # self.dev_logger.info(f"ğŸ” [DEV] Applying 'USB' filter to {result.name}")
        df = result.data.copy()
        
        static_columns = [
            'volume_info__file_system', 'volume_info__mount_point', 
            'volume_info__volume_guid', 'volume_info__drive_letter', 
            'volume_info__volume_label', 'setupapi_info__serial_number', 
            'device_metadata__serial_number', 'setupapi_info__product_id', 
            'setupapi_info__volume_name', 'data_sources__has_usb_data', 
            'data_sources__has_usbstor_data', 'data_sources__has_setupapi_data', 
            'primary_source', 'portable_device_info__friendly_name'
        ]
        
        pattern_columns = [col for col in df.columns if col.startswith('connection_times__')]
        columns_to_drop = static_columns + pattern_columns
        
        existing_columns = [col for col in columns_to_drop if col in df.columns]
        if existing_columns:
            df.drop(columns=existing_columns, inplace=True)
        
        return df

    
    
    def _load_data(self, category: Category) -> ResultDataFrames:
        """ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ë¡œë“œ - íŒŒì¼ì´ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°"""
        try:
            # self.dev_logger.debug(f"Starting data load for category: {category.name}")
            
            # helperë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë¡œë“œ ë° ì¸ì½”ë”© ì²˜ë¦¬
            df_results = self.helper.get_encoded_results(self.task_id, category)
            
            if not df_results:
                self.dev_logger.warning(f"No dataframes found for category {category.name} and task {self.task_id}")
                # ë¹ˆ ResultDataFrames ë°˜í™˜
                return ResultDataFrames(data=[])
            
            # self.dev_logger.debug(f"Successfully loaded {len(df_results.data)} dataframes for category: {category.name}")
            return df_results
            
        except Exception as e:
            # self.dev_logger.warning(f"âš ï¸ [DEV] Failed to load data for category {category.name}: {e}")
            # self.dev_logger.info(f"â­ï¸ [DEV] Skipping category {category.name} - no data available")
            # ë¹ˆ ResultDataFrames ë°˜í™˜í•˜ì—¬ ê±´ë„ˆë›°ê¸°
            return ResultDataFrames(data=[])

    def _generate_analysis_result(self):
        """
        self.created_artifactsë¥¼ í™œìš©í•˜ì—¬ í–‰ìœ„ë³„ ë¶„ë¥˜ê²°ê³¼ë¥¼ ìƒì„±í•¨.
        ê²°ê³¼ëŠ” ìƒì†ë°›ì€ í´ë˜ìŠ¤ì—ì„œ ì •ì˜ëœ ì•„ë˜ self.analyze_resultsì— ì—…ë°ì´íŠ¸í•  ê²ƒ.

        self.analyze_results = {
            behavior: {
                "job_id": self.job_id,
                "task_id": self.task_id,
                "behavior": behavior.name,
                "analysis_summary": "",
                "risk_level": "",
                "artifact_ids": []
            } for behavior in BehaviorType
        }
        í–‰ìœ„ë³„ë¡œ ê´€ë ¨ëœ ì•„í‹°íŒ©íŠ¸ë§Œ ë¶„ë¥˜í•˜ì—¬ ë¶„ì„ì„ ìˆ˜í–‰.
        """
        self.dev_logger.info("ğŸ”§ [DEV] Starting _generate_analysis_result (Behavior-specific Classification)")
        
        # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ë©”ì„œë“œ ë¨¼ì € ì‹¤í–‰
        super()._generate_analysis_result()
        
        # created_artifactsê°€ ë¹„ì–´ìˆìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
        if not self.created_artifacts:
            self.dev_logger.warning("âš ï¸ [DEV] No artifacts created, skipping analysis result generation")
            return
        
        # í–‰ìœ„ë³„ ì•„í‹°íŒ©íŠ¸ ë¶„ë¥˜
        behavior_artifacts = self._classify_artifacts_by_behavior(self.created_artifacts)
        
        # ê° í–‰ìœ„ë³„ë¡œ ë¶„ì„ ê²°ê³¼ ìƒì„±
        for behavior_type in BehaviorType:
            if behavior_type in self.analyze_results:
                artifacts_for_behavior = behavior_artifacts.get(behavior_type, [])
                artifact_ids = [artifact.get('id') or artifact.get('artifact_id') 
                              for artifact in artifacts_for_behavior 
                              if artifact.get('id') or artifact.get('artifact_id')]
                
                # í–‰ìœ„ë³„ ë¶„ì„ ìš”ì•½ ìƒì„±
                analysis_summary = self._create_behavior_analysis_summary(behavior_type, artifacts_for_behavior)
                
                # í–‰ìœ„ë³„ ìœ„í—˜ë„ í‰ê°€
                risk_level = self._evaluate_behavior_risk_level(behavior_type, artifacts_for_behavior)
                
                self.analyze_results[behavior_type].update({
                    "artifact_ids": artifact_ids,
                    "analysis_summary": analysis_summary,
                    "risk_level": risk_level,
                    "artifact_count": len(artifact_ids)
                })
                
                self.dev_logger.info(
                    f"âœ… [DEV] {behavior_type.name}: {len(artifact_ids)} artifacts, risk={risk_level}"
                )
        
        # ì „ì²´ ë¶„ì„ í†µê³„ ë¡œê¹…
        self._log_behavior_analysis_statistics(behavior_artifacts)
        
        self.dev_logger.info("âœ… [DEV] Completed _generate_analysis_result (Behavior-specific Classification)")


    def _classify_artifacts_by_behavior(self, artifacts: List[dict]) -> Dict[BehaviorType, List[dict]]:
        """ì•„í‹°íŒ©íŠ¸ë¥¼ í–‰ìœ„ë³„ë¡œ ë¶„ë¥˜"""
        behavior_artifacts = {behavior: [] for behavior in BehaviorType}
        
        for artifact in artifacts:
            artifact_type = artifact.get('artifact_type', '')
            
            # Acquisition (íšë“) - ë‹¤ìš´ë¡œë“œ, ë¸Œë¼ìš°ì €, ë©”ì‹ ì € ê´€ë ¨
            if any(keyword in artifact_type.lower() for keyword in [
                'downloads', 'browser', 'messenger', 'discord', 'kakaotalk', 
                'urls', 'visits', 'autofill', 'logins'
            ]):
                behavior_artifacts[BehaviorType.acquisition].append(artifact)
            
            # Deletion (ì‚­ì œ) - íœ´ì§€í†µ, ì‚­ì œëœ íŒŒì¼ ê´€ë ¨
            elif any(keyword in artifact_type.lower() for keyword in [
                'deleted', 'recycle', 'mft_deleted'
            ]):
                behavior_artifacts[BehaviorType.deletion].append(artifact)
            
            # Forgery (ìœ„ì¡°) - LNK íŒŒì¼, Prefetch ê´€ë ¨
            elif any(keyword in artifact_type.lower() for keyword in [
                'lnk', 'prefetch'
            ]):
                behavior_artifacts[BehaviorType.forgery].append(artifact)
            
            # Upload (ì—…ë¡œë“œ) - USB ì¥ì¹˜ ê´€ë ¨
            elif any(keyword in artifact_type.lower() for keyword in [
                'usb'
            ]):
                behavior_artifacts[BehaviorType.upload].append(artifact)
            
            # Etc (ê¸°íƒ€) - ë‚˜ë¨¸ì§€ ëª¨ë“  ê²ƒ
            else:
                behavior_artifacts[BehaviorType.etc].append(artifact)
        
        return behavior_artifacts

    def _create_behavior_analysis_summary(self, behavior_type: BehaviorType, artifacts: List[dict]) -> str:
        """í–‰ìœ„ë³„ ë¶„ì„ ìš”ì•½ ìƒì„±"""
        artifact_count = len(artifacts)
        
        if artifact_count == 0:
            return f"No artifacts found for {behavior_type.name} behavior."
        
        # ì•„í‹°íŒ©íŠ¸ íƒ€ì…ë³„ í†µê³„
        type_counts = {}
        for artifact in artifacts:
            artifact_type = artifact.get('artifact_type', 'unknown')
            type_counts[artifact_type] = type_counts.get(artifact_type, 0) + 1
        
        # ìƒìœ„ íƒ€ì…ë“¤
        top_types = sorted(type_counts.items(), key=lambda x: x[1], reverse=True)[:3]
        type_summary = ", ".join([f"{atype}: {count}" for atype, count in top_types])
        
        return (f"Analysis of {artifact_count} artifacts for {behavior_type.name} behavior. "
                f"Top artifact types: {type_summary}")

    def _evaluate_behavior_risk_level(self, behavior_type: BehaviorType, artifacts: List[dict]) -> str:
        """í–‰ìœ„ë³„ ìœ„í—˜ë„ í‰ê°€ - ëª¨ë“  í–‰ìœ„ë¥¼ normalë¡œ ì„¤ì •"""
        # ìœ„í—˜ë„ í‰ê°€ ì—†ì´ ëª¨ë“  í–‰ìœ„ë¥¼ normalë¡œ ì„¤ì •
        return 'normal'

    def _log_behavior_analysis_statistics(self, behavior_artifacts: Dict[BehaviorType, List[dict]]):
        """í–‰ìœ„ë³„ ë¶„ì„ í†µê³„ ë¡œê¹…"""
        total_artifacts = sum(len(artifacts) for artifacts in behavior_artifacts.values())
        
        self.dev_logger.info("=" * 80)
        self.dev_logger.info("ğŸ“Š [DEV] Behavior-specific Analysis Statistics")
        self.dev_logger.info("=" * 80)
        
        # í–‰ìœ„ë³„ í†µê³„
        self.dev_logger.info("ğŸ” Behavior-wise Statistics:")
        for behavior_type, artifacts in behavior_artifacts.items():
            count = len(artifacts)
            percentage = (count / total_artifacts * 100) if total_artifacts > 0 else 0
            risk_level = self.analyze_results[behavior_type].get('risk_level', 'unknown')
            
            self.dev_logger.info(
                f"  â€¢ {behavior_type.name}: {count:,} artifacts ({percentage:.1f}%) - Risk: {risk_level}"
            )
        
        # í•„í„°ë§ íš¨ê³¼ í†µê³„
        total_original_rows = getattr(self, '_total_original_rows', 0)
        total_filtered_rows = getattr(self, '_total_filtered_rows', 0)
        filtering_reduction = total_original_rows - total_filtered_rows
        filtering_percentage = (filtering_reduction / total_original_rows * 100) if total_original_rows > 0 else 0
        
        if total_original_rows > 0:
            self.dev_logger.info("\nğŸ“‰ Filtering Effectiveness:")
            self.dev_logger.info(f"  â€¢ Original Data: {total_original_rows:,} rows")
            self.dev_logger.info(f"  â€¢ Filtered Data: {total_filtered_rows:,} rows")
            self.dev_logger.info(f"  â€¢ Reduction: {filtering_reduction:,} rows ({filtering_percentage:.1f}%)")
        
        self.dev_logger.info("=" * 80)