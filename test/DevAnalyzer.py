import logging
import os
import re
from typing import Dict, List, Set
import pandas as pd
from datetime import datetime
from dateutil.relativedelta import relativedelta

from test.testAnalyzer import TestAnalyzer, BackendClient, Category, ResultDataFrames, ResultDataFrame
from test.Analyzer import BehaviorType

class DevAnalyzer(TestAnalyzer):
    def __init__(self, backend_client: BackendClient, time_filter_months: int = 7) -> None:
        super().__init__(backend_client)
        # DevAnalyzer ì „ìš© ë¡œê±° ìƒì„±
        self.dev_logger = logging.getLogger("DevAnaly")
        # í•„í„°ë§ëœ ë°ì´í„° ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.output_dir = "filtered_data"
        # self._create_output_directory()
        
        # ë¸Œë¼ìš°ì € íŒŒì¼ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì„¤ì • (ì‹¤ì œ íŒŒì¼ëª… í˜•íƒœ)
        self.browser_whitelist: Set[str] = {
            "binary", "browser_collected_files", "browser_discovered_profiles",
            "Chrome.keyword_search_terms", "Edge.keyword_search_terms",
            "Chrome.keywords", "Edge.keywords",
            "Chrome.urls", "Edge.urls",
            "Chrome.visits", "Edge.visits", 
            "Chrome.visited_links", "Edge.visited_links",
            "Chrome.autofill", "Edge.autofill",
            # "Chrome.autofill_profiles", "Edge.autofill_profiles",
            "Chrome.addresses", "Edge.addresses",
            "Chrome.autofill_sync_metadata", "Edge.autofill_sync_metadata",
            "Chrome.sync_entities_metadata", "Edge.sync_entities_metadata", 
            "Chrome.downloads", "Edge.downloads",
            "Chrome.downloads_url_chains", "Edge.downloads_url_chains",
            "Chrome.logins", "Edge.logins"
        }
        
        # ê´‘ê³ /ì¶”ì  ë„ë©”ì¸ ì„¤ì •
        self.ad_tracking_domains: List[str] = [
            'doubleclick.net', 'googlesyndication.com', 'adnxs.com', 
            'google-analytics.com', 'scorecardresearch.com', 'facebook.net', 
            'akamaihd.net', 'cloudfront.net', 'gstatic.com'
        ]
        
        # ì‹œê°„ í•„í„°ë§ ê¸°ê°„ ì„¤ì • (ê°œì›” ìˆ˜)
        self.time_filter_months: int = time_filter_months

        # íŒŒì¼ë³„ íŠ¹ì • ì‹œê°„ ì»¬ëŸ¼ í•„í„°ë§ ê·œì¹™
        self.time_filter_config: Dict[str, str] = {
            "binary": "timestamp",
            "browser_collected_files": "timestamp",
            "Chrome.keywords": "last_visited",
            "Edge.keywords": "last_visited",
            "Chrome.urls": "last_visit_time",
            "Edge.urls": "last_visit_time",
            "Chrome.visits": "visit_time",
            "Edge.visits": "visit_time",
            "Chrome.autofill": "date_last_used",
            "Edge.autofill": "date_last_used",
            "Chrome.downloads": "end_time",
            "Edge.downloads": "end_time",
            "Chrome.logins": "date_last_used",
            "Edge.logins": "date_last_used",
            "Chrome.addresses": "use_date",
            "Edge.addresses": "use_date",
            "mft_deleted_files": "deletion_time",
            "recycle_bin_files": "deleted_time",
            "lnk_files": "target_info__target_times__access",
            "prefetch_files": "last_run_time_1",
            "usb_devices": "setupapi_info__last_connection_time",
            "KakaoTalk.files": "last_modified",
            "Discord.files": "last_modified"
        }
    
    def _create_output_directory(self) -> None:
        """í•„í„°ë§ëœ ë°ì´í„° ì €ì¥ì„ ìœ„í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
            self.dev_logger.info(f"ğŸ“ [DEV] Created output directory: {self.output_dir}")
    
    def _save_filtered_data(self, category: Category, df_results: ResultDataFrames) -> str:
        """í•„í„°ë§ëœ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        self._create_output_directory()
        # ë¹ˆ ë°ì´í„°ì¸ ê²½ìš° ì €ì¥í•˜ì§€ ì•ŠìŒ
        if not df_results or not df_results.data:
            self.dev_logger.info(f"â­ï¸ [DEV] No data to save for category: {category.name}")
            return ""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        category_dir = os.path.join(self.output_dir, f"{category.name.lower()}_{timestamp}")
        
        saved_count = 0
        skipped_count = 0
        empty_count = 0
        
        # ë¨¼ì € ì €ì¥í•  íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        files_to_save = []
        for result in df_results.data:
            # ë¸Œë¼ìš°ì € ì¹´í…Œê³ ë¦¬ì˜ ê²½ìš° í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í™•ì¸
            if category == Category.BROWSER and result.name not in self.browser_whitelist:
                self.dev_logger.debug(f"â­ï¸ [DEV] Skipping browser file (not in whitelist): {result.name}")
                skipped_count += 1
                continue
            
            # ë¹ˆ ë°ì´í„°ì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
            if result.data.empty:
                self.dev_logger.info(f"â­ï¸ [DEV] Skipping empty file: {result.name} (0 rows)")
                empty_count += 1
                continue
            
            files_to_save.append(result)
        
        # ì €ì¥í•  íŒŒì¼ì´ ì—†ìœ¼ë©´ ë””ë ‰í† ë¦¬ ìƒì„±í•˜ì§€ ì•ŠìŒ
        if not files_to_save:
            self.dev_logger.info(f"â­ï¸ [DEV] No files to save for category: {category.name} (all files empty or skipped)")
            return ""
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        if not os.path.exists(category_dir):
            os.makedirs(category_dir)
            self.dev_logger.info(f"ğŸ“ [DEV] Created category directory: {category_dir}")
        
        # íŒŒì¼ ì €ì¥
        for result in files_to_save:
            # íŒŒì¼ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
            safe_filename = re.sub(r'[/\\:*?"<>|]', '_', result.name)
            if not safe_filename.endswith('.csv'):
                safe_filename += '.csv'
            
            file_path = os.path.join(category_dir, safe_filename)
            
            try:
                result.data.to_csv(file_path, index=False, encoding='utf-8-sig')
                self.dev_logger.info(f"ğŸ’¾ [DEV] Saved filtered data: {file_path} ({len(result.data)} rows)")
                saved_count += 1
            except Exception as e:
                self.dev_logger.error(f"âŒ [DEV] Failed to save {file_path}: {str(e)}")
        
        self.dev_logger.info(f"âœ… [DEV] Saved {saved_count} files, skipped {skipped_count} files, empty {empty_count} files to: {category_dir}")
        return category_dir
    
    def _filter_data(self, category: Category, df_results: ResultDataFrames) -> ResultDataFrames:
        """
        ë°ì´í„° í•„í„°ë§ ì²˜ë¦¬. ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë°›ì•„ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•œë‹¤.
        """
        self.dev_logger.info(f"ğŸ”§ [DEV] Starting _filter_data for category: {category.name}")
        
        # ë¹ˆ ë°ì´í„°ì¸ ê²½ìš° ì¡°ê¸° ë°˜í™˜
        if not df_results or not df_results.data:
            self.dev_logger.info(f"â­ï¸ [DEV] No data to filter for category: {category.name}")
            return df_results
        
        # í•„í„°ë§ í†µê³„ ì´ˆê¸°í™”
        category_original_rows = 0
        category_filtered_rows = 0
        
        for result in df_results.data:
            original_count = len(result.data)
            category_original_rows += original_count
            
            # 1. ì‹œê°„ í•„í„°ë§
            result.data = self._apply_time_filtering(result)
            after_time_filter = len(result.data)
            if original_count != after_time_filter:
                self.dev_logger.info(f"â° [DEV] Time filtering for {result.name}: {original_count} -> {after_time_filter} rows")
            
            # 2. ì¹´í…Œê³ ë¦¬ë³„ ì—´/í–‰ í•„í„°ë§
            match category:
                case Category.USB:
                    result.data = self._filter_usb_data(result)
                case Category.LNK:
                    result.data = self._filter_lnk_data(result)
                case Category.MESSENGER:
                    result.data = self._filter_messenger_data(result)
                case Category.PREFETCH:
                    result.data = self._filter_prefetch_data(result)
                case Category.DELETED:
                    result.data = self._filter_deleted_data(result)
                case Category.BROWSER:
                    result.data = self._filter_browser_data(result)
                case _:
                    self.dev_logger.warning(f"No specific filter for category {category.name}. Applying default limit.")
                    result.data = result.data.head(10)
            
            final_count = len(result.data)
            category_filtered_rows += final_count
            
            self.dev_logger.debug(f"ğŸ”§ [DEV] Filtered {result.name} data: {original_count} -> {final_count} rows")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§ í†µê³„ ë¡œê¹…
        reduction = category_original_rows - category_filtered_rows
        reduction_percent = (reduction / category_original_rows * 100) if category_original_rows > 0 else 0
        
        self.dev_logger.info(f"ğŸ“Š [DEV] {category.name} filtering summary: {category_original_rows:,} -> {category_filtered_rows:,} rows (reduction: {reduction:,} rows, {reduction_percent:.1f}%)")
        
        # ì „ì²´ í†µê³„ ì—…ë°ì´íŠ¸
        if not hasattr(self, '_total_original_rows'):
            self._total_original_rows = 0
            self._total_filtered_rows = 0
        
        self._total_original_rows += category_original_rows
        self._total_filtered_rows += category_filtered_rows
        
        # í•„í„°ë§ëœ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥
        # self._save_filtered_data(category, df_results)
        
        self.dev_logger.info(f"âœ… [DEV] Completed _filter_data for category: {category.name}")
        return df_results

    def _apply_time_filtering(self, result: ResultDataFrame) -> pd.DataFrame:
        """
        ì§€ì •ëœ ê°œì›” ìˆ˜ ì´ì „ë¶€í„° í˜„ì¬ê¹Œì§€ì˜ ë°ì´í„°ë§Œ ìœ ì§€í•˜ëŠ” ì‹œê°„ í•„í„°ë§.
        íŒŒì¼ë³„ë¡œ ì§€ì •ëœ ì‹œê°„ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ í•´ë‹¹ ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ì‹œê°„ ì»¬ëŸ¼ì„ ì°¾ì•„ í•„í„°ë§.
        """
        df = result.data
        file_name = result.name

        if df.empty:
            return df

        current_time = datetime.now()
        cutoff_date = current_time - relativedelta(months=self.time_filter_months)
        
        self.dev_logger.debug(
            f"â° [DEV] Time filtering for '{file_name}': Keeping data from "
            f"{cutoff_date.strftime('%Y-%m-%d')} to {current_time.strftime('%Y-%m-%d')}"
        )

        target_columns = []
        
        # 1. íŒŒì¼ë³„ íŠ¹ì • ì‹œê°„ ì»¬ëŸ¼ ê·œì¹™ í™•ì¸
        specific_time_col = self.time_filter_config.get(file_name)
        if specific_time_col and specific_time_col in df.columns:
            self.dev_logger.info(f"ğŸ¯ [DEV] Found specific time column for '{file_name}': '{specific_time_col}'")
            target_columns.append(specific_time_col)
        else:
            # 2. íŠ¹ì • ê·œì¹™ì´ ì—†ê±°ë‚˜ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´, ì¼ë°˜ì ì¸ ì‹œê°„ ì»¬ëŸ¼ íƒìƒ‰
            if specific_time_col:
                 self.dev_logger.warning(f"âš ï¸ [DEV] Specific time column '{specific_time_col}' not found in '{file_name}'. Falling back to general search.")
            
            time_keywords = ['time', 'date', 'created', 'modified', 'access', 'deletion', 'mtime', 'ctime', 'timestamp']
            target_columns = [
                col for col in df.columns 
                if isinstance(col, str) and any(keyword in col.lower() for keyword in time_keywords)
            ]

        if not target_columns:
            self.dev_logger.warning(f"â„¹ï¸ [DEV] No time columns found for '{file_name}', skipping time filtering.")
            return df

        # ì‹œê°„ í•„í„°ë§ ì ìš© (OR ì¡°ê±´)
        mask = pd.Series([False] * len(df), index=df.index)
        
        for col in target_columns:
            try:
                # addresses íŒŒì¼ì˜ use_date ì»¬ëŸ¼ íŠ¹ë³„ ì²˜ë¦¬ (Unix timestamp)
                if 'addresses' in file_name and col == 'use_date':
                    self.dev_logger.debug(f"ğŸ” [DEV] Converting Unix timestamp in {file_name}")
                    # Unix timestampë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
                    temp_dates = self._smart_datetime_conversion(df[col], col)
                # cookies íŒŒì¼ì˜ WebKit timestamp íŠ¹ë³„ ì²˜ë¦¬
                elif 'cookies' in file_name and col.endswith('_utc'):
                    self.dev_logger.debug(f"ğŸ” [DEV] Converting WebKit timestamp in {file_name}")
                    # WebKit timestamp (ë§ˆì´í¬ë¡œì´ˆ)ë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
                    temp_dates = self._smart_datetime_conversion(df[col], col)
                # ê¸°íƒ€ WebKit timestamp í˜•ì‹ ì²˜ë¦¬ (í° ìˆ«ì ê°’ë“¤)
                elif col.endswith('_utc') and df[col].dtype in ['int64', 'float64']:
                    # ê°’ì´ ë§¤ìš° í° ê²½ìš° WebKit timestampë¡œ ê°„ì£¼
                    sample_values = df[col].dropna().head(3)
                    if len(sample_values) > 0 and sample_values.iloc[0] > 1e15:  # WebKit timestamp ë²”ìœ„
                        self.dev_logger.debug(f"ğŸ” [DEV] Converting WebKit timestamp for {col}")
                        temp_dates = self._smart_datetime_conversion(df[col], col)
                    else:
                        # ìˆ«ìí˜• ë°ì´í„°ì¸ ê²½ìš° Unix timestampë¡œ ì‹œë„
                        temp_dates = self._smart_datetime_conversion(df[col], col)
                else:
                    # ì¼ë°˜ì ì¸ datetime ë³€í™˜ - ë¨¼ì € ë°ì´í„° íƒ€ì…ê³¼ ìƒ˜í”Œ ê°’ í™•ì¸
                    temp_dates = self._smart_datetime_conversion(df[col], col)
                
                # íƒ€ì„ì¡´ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ tz_localize ì ìš©
                if hasattr(temp_dates.dt, 'tz') and temp_dates.dt.tz is not None:
                    temp_dates = temp_dates.dt.tz_localize(None)
                
                # ìœ íš¨í•œ ë‚ ì§œì´ê³ , cutoff_date ì´í›„ì¸ ë°ì´í„°ë§Œ ì„ íƒ
                valid_mask = temp_dates.notna() & (temp_dates >= cutoff_date)
                mask |= valid_mask
                
                if valid_mask.any():
                    self.dev_logger.debug(
                        f"â° [DEV] Column '{col}' contributed {valid_mask.sum()} valid rows for filtering."
                    )
            except Exception as e:
                self.dev_logger.warning(f"âš ï¸ [DEV] Could not filter by column '{col}' in '{file_name}': {str(e)}")
                continue
        
        # í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
        return df[mask]

    def _smart_datetime_conversion(self, series: pd.Series, column_name: str) -> pd.Series:
        """
        ìŠ¤ë§ˆíŠ¸í•œ datetime ë³€í™˜ - ë°ì´í„° íƒ€ì…ê³¼ ìƒ˜í”Œ ê°’ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë³€í™˜ ë°©ë²• ì„ íƒ
        """
        # Categorical ë°ì´í„° ì²˜ë¦¬ (ë¨¼ì € ì²˜ë¦¬)
        if isinstance(series.dtype, pd.CategoricalDtype):
            self.dev_logger.debug(f"ğŸ” [DEV] Converting categorical data to string for {column_name}")
            series = pd.Series(series.astype(str), index=series.index if hasattr(series, 'index') else None)
        
        if series.empty:
            return pd.Series([], dtype='datetime64[ns]')
        
        # ìƒ˜í”Œ ê°’ë“¤ í™•ì¸ (NaN ì œì™¸)
        sample_values = series.dropna().head(5)
        if sample_values.empty:
            return pd.Series([pd.NaT] * len(series), index=series.index)
        
        # ìˆ«ìí˜• ë°ì´í„°ì¸ ê²½ìš° - ë” ì—„ê²©í•œ ì²´í¬
        if pd.api.types.is_numeric_dtype(series) and not pd.api.types.is_string_dtype(series):
            # Unix timestamp ë²”ìœ„ í™•ì¸ (1970-2038ë…„)
            min_val = sample_values.min()
            max_val = sample_values.max()
            
            if min_val > 1e15:  # WebKit timestamp (ë§ˆì´í¬ë¡œì´ˆ)
                self.dev_logger.debug(f"ğŸ” [DEV] Detected WebKit timestamp for {column_name}")
                return pd.to_datetime(series, unit='us', errors='coerce')
            elif min_val > 1e9:  # Unix timestamp (ì´ˆ)
                self.dev_logger.debug(f"ğŸ” [DEV] Detected Unix timestamp for {column_name}")
                return pd.to_datetime(series, unit='s', errors='coerce')
            elif min_val > 1e6:  # ë°€ë¦¬ì´ˆ timestamp
                self.dev_logger.debug(f"ğŸ” [DEV] Detected millisecond timestamp for {column_name}")
                return pd.to_datetime(series, unit='ms', errors='coerce')
        
        # ë¬¸ìì—´ ë°ì´í„°ì´ê±°ë‚˜ ìˆ«ìí˜•ì´ì§€ë§Œ ë²”ìœ„ì— ë§ì§€ ì•ŠëŠ” ê²½ìš°
        sample_str = str(sample_values.iloc[0])
        
        # ISO í˜•ì‹ í™•ì¸
        if 'T' in sample_str and ('+' in sample_str or 'Z' in sample_str):
            self.dev_logger.debug(f"ğŸ” [DEV] Detected ISO format for {column_name}")
            return pd.to_datetime(series, format='ISO8601', errors='coerce')
        
        # ì¼ë°˜ì ì¸ ë‚ ì§œ í˜•ì‹ë“¤ ì‹œë„
        common_formats = [
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%d',
            '%m/%d/%Y %H:%M:%S',
            '%m/%d/%Y',
            '%d/%m/%Y %H:%M:%S',
            '%d/%m/%Y',
            '%Y-%m-%d %H:%M:%S.%f',
            '%Y-%m-%d %H:%M:%S.%f%z'
        ]
        
        for fmt in common_formats:
            try:
                # ìƒ˜í”Œ ê°’ìœ¼ë¡œ í˜•ì‹ í…ŒìŠ¤íŠ¸
                test_val = pd.to_datetime(sample_str, format=fmt, errors='coerce')
                if not pd.isna(test_val):
                    self.dev_logger.debug(f"ğŸ” [DEV] Detected format '{fmt}' for {column_name}")
                    return pd.to_datetime(series, format=fmt, errors='coerce')
            except:
                continue
        
        # ëª¨ë“  í˜•ì‹ì´ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ ë³€í™˜ (ê²½ê³  ì–µì œ)
        import warnings
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", UserWarning)
            warnings.simplefilter("ignore", FutureWarning)
            self.dev_logger.debug(f"ğŸ” [DEV] Using fallback datetime conversion for {column_name}")
            return pd.to_datetime(series, errors='coerce')

    def _filter_browser_data(self, result: ResultDataFrame) -> pd.DataFrame:
        """ë¸Œë¼ìš°ì € ë°ì´í„°ì— ëŒ€í•œ í•„í„°ë§ ê·œì¹™"""
        self.dev_logger.info(f"ğŸ” [DEV] Applying 'BROWSER' filter to {result.name}")
        df = result.data.copy()
        
        # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ì— ì—†ëŠ” íŒŒì¼ì€ ê±´ë„ˆë›°ê¸°
        if result.name not in self.browser_whitelist:
            self.dev_logger.debug(f"â­ï¸ [DEV] Skipping browser file (not in whitelist): {result.name}")
            return pd.DataFrame()
        
        # íŒŒì¼ëª…ì—ì„œ ì ‘ë‘ì‚¬ ì œê±°
        file_name = result.name.split('.', 1)[1] if result.name.startswith(('Chrome.', 'Edge.')) else result.name
        self.dev_logger.debug(f"ğŸ“ [DEV] Processing browser file: {file_name}")
        
        # 1. ì—´ í•„í„°ë§
        columns_to_drop = self._get_browser_columns_to_drop(file_name)
        existing_columns_to_drop = [col for col in columns_to_drop if col in df.columns]
        
        if existing_columns_to_drop:
            df.drop(columns=existing_columns_to_drop, inplace=True)
            self.dev_logger.debug(f"Dropped columns for {file_name}: {existing_columns_to_drop}")
        
        # ë¡œê·¸ì¸ íŒŒì¼ì˜ ê²½ìš° ì²« ë²ˆì§¸ ì»¬ëŸ¼(ì¸ë±ìŠ¤)ë„ ì œê±°
        if file_name == "logins" and len(df.columns) > 0:
            df.drop(columns=[df.columns[0]], inplace=True)
        
        # 2. í–‰ í•„í„°ë§
        df = self._apply_browser_row_filtering(df, file_name)
        
        self.dev_logger.info(f"ğŸŒ [DEV] Filtered {file_name}: {len(result.data)} -> {len(df)} rows")
        return df
    
    def _apply_browser_row_filtering(self, df: pd.DataFrame, file_name: str) -> pd.DataFrame:
        """ë¸Œë¼ìš°ì € ë°ì´í„°ì— ëŒ€í•œ í–‰ í•„í„°ë§ ê·œì¹™ ì ìš©"""
        original_count = len(df)
        
        if file_name == "visits":
            if 'visit_duration' in df.columns:
                df['visit_duration'] = pd.to_numeric(df['visit_duration'], errors='coerce')
                df = df[df['visit_duration'] != 0]
                self.dev_logger.info(f"ğŸ—‘ï¸ [DEV] Removed rows with visit_duration=0")
        
        elif file_name == "urls":
            if 'visit_count' in df.columns:
                df['visit_count'] = pd.to_numeric(df['visit_count'], errors='coerce')
                df = df[df['visit_count'] > 0]
            
            if 'url' in df.columns:
                pattern = '|'.join(self.ad_tracking_domains)
                df = df[~df['url'].str.contains(pattern, case=False, na=False)]
                self.dev_logger.info(f"ğŸ—‘ï¸ [DEV] Removed ad/tracking domains")
        
        elif file_name == "downloads":
            # ì™„ë£Œëœ ë‹¤ìš´ë¡œë“œë§Œ
            if 'state' in df.columns:
                df = df[df['state'].astype(str) == '1']
            
            # ì„ì‹œ íŒŒì¼ ì œê±°
            if 'target_path' in df.columns:
                temp_pattern = r'\.(?:tmp|crdownload)$'  # non-capturing group ì‚¬ìš©
                df = df[~df['target_path'].str.contains(temp_pattern, case=False, na=False)]
            
            # ë¹ˆ í–‰ ì œê±°
            df = df.dropna(how='all')
            if 'target_path' in df.columns:
                df = df[df['target_path'].notna() & (df['target_path'].astype(str).str.strip() != '')]
            if 'url' in df.columns:
                df = df[df['url'].notna() & (df['url'].astype(str).str.strip() != '')]
        
        elif file_name == "browser_collected_files":
            if 'data_type' in df.columns and 'download_state' in df.columns:
                df = df[(df['data_type'] == 'downloads') & (df['download_state'] == 'completed')]
        
        elif file_name == "sync_entities_metadata":
            # ì‚­ì œëœ í•­ëª© ì œê±°
            if 'is_deleted' in df.columns:
                df = df[df['is_deleted'] != True]
            
            # í´ë”ê°€ ì•„ë‹Œ ì‹¤ì œ ë°ì´í„°ë§Œ ìœ ì§€
            if 'is_folder' in df.columns:
                df = df[df['is_folder'] != True]
            
            # ë¹ˆ í–‰ ì œê±°
            df = df.dropna(how='all')
        
        if original_count != len(df):
            self.dev_logger.info(f"ğŸ—‘ï¸ [DEV] Row filtering: {original_count} -> {len(df)} rows")
        
        return df
    
    def _get_browser_columns_to_drop(self, file_name: str) -> List[str]:
        """ë¸Œë¼ìš°ì € íŒŒì¼ëª…ì— ë”°ë¥¸ ì œê±°í•  ì»¬ëŸ¼ ëª©ë¡ ë°˜í™˜"""
        column_map = {
            "binary": ['error', 'success', 'file_type'],
            "browser_collected_files": ['error', 'success', 'file_type'],
            "browser_discovered_profiles": ['exists'],
            "keyword_search_terms": ['normalized_term'],
            "keywords": [
                'url_hash', 'input_encodings', 'image_url_post_params', 
                'search_url_post_params', 'suggest_url_post_params',
                'alternate_urls', 'safe_for_autoreplace', 'created_from_play_api',
                'image_url', 'favicon_url'
            ],
            "urls": ['favicon_id'],
            "visits": ['incremented_omnibox_typed_score', 'app_id'],
            "autofill": ['value_lower'],
            "addresses": ['language_code'],
            "downloads": ['transient', 'http_method', 'etag', 'last_response_headers'],
            "logins": [
                'password_value', 'username_element', 'password_element', 
                'submit_element', 'display_name', 'icon_url', 'federation_url', 
                'skip_zero_click', 'generation_upload_status', 
                'possible_username_pairs', 'moving_blocked_for_list'
            ],
            "sync_entities_metadata": [
                'model_type', 'storage_key', 'client_tag_hash', 'server_id',
                'specifics_hash', 'base_specifics_hash', 'parent_id', 'version',
                'mtime', 'ctime', 'server_version', 'is_deleted', 'is_folder',
                'unique_position', 'is_bookmark', 'is_folder', 'is_unsynced'
            ]
        }
        return column_map.get(file_name, [])

    def _filter_deleted_data(self, result: ResultDataFrame) -> pd.DataFrame:
        """ì‚­ì œëœ íŒŒì¼ ë°ì´í„°ì— ëŒ€í•œ í•„í„°ë§"""
        self.dev_logger.info(f"ğŸ” [DEV] Applying 'DELETED' filter to {result.name}")
        df = result.data.copy()
        
        # ì—´ í•„í„°ë§
        columns_to_drop = [
            'access_time_timestamp', 'creation_time_timestamp', 
            'deletion_time_timestamp', 'modification_time_timestamp', 
            'deleted_time_timestamp', 'is_directory', 'parse_status', 
            'recycle_bin', 'recycle_bin_version', 'Unnamed: 0', 'file_index'
        ]
        
        existing_columns = [col for col in columns_to_drop if col in df.columns]
        if existing_columns:
            df.drop(columns=existing_columns, inplace=True)
        
        # í–‰ í•„í„°ë§
        df = self._apply_deleted_row_filtering(df, result.name)
        return df
    
    def _apply_deleted_row_filtering(self, df: pd.DataFrame, file_name: str) -> pd.DataFrame:
        """ì‚­ì œëœ íŒŒì¼ ë°ì´í„°ì— ëŒ€í•œ í–‰ í•„í„°ë§"""
        if 'mft_deleted' not in str(file_name).lower():
            return df
        
        original_count = len(df)
        
        # ì‹œìŠ¤í…œ íŒŒì¼ ì œê±° ($ ì ‘ë‘ì‚¬)
        file_col = 'file_name' if 'file_name' in df.columns else 'FileName'
        if file_col in df.columns:
            df = df[~df[file_col].str.startswith('$', na=False)]
        
        # ì‹œìŠ¤í…œ ê²½ë¡œ ì œê±°
        path_col = 'full_path' if 'full_path' in df.columns else 'ParentPath'
        if path_col in df.columns:
            system_paths = [
                r'C:\\Windows', r'C:\\Program Files', 
                r'C:\\ProgramData', r'C:\\\$Recycle\.Bin'
            ]
            pattern = '|'.join(system_paths)
            df = df[~df[path_col].str.contains(pattern, case=False, na=False, regex=True)]
            
            # ì‹œìŠ¤í…œ ë¡œê·¸ íŒŒì¼ ì œê±°
            if file_col in df.columns:
                system_files = ['bootex.log', 'LOG', 'setup.log', 'install.log']
                system_pattern = '|'.join(system_files)
                df = df[~df[file_col].str.contains(system_pattern, case=False, na=False)]
        
        if original_count != len(df):
            self.dev_logger.info(f"ğŸ—‘ï¸ [DEV] Filtered: {original_count} -> {len(df)} rows")
        
        return df

    def _filter_lnk_data(self, result: ResultDataFrame) -> pd.DataFrame:
        """LNK ë°ì´í„° í•„í„°ë§"""
        self.dev_logger.info(f"ğŸ” [DEV] Applying 'LNK' filter to {result.name}")
        df = result.data.copy()
        
        # ì—´ í•„í„°ë§
        static_columns = [
            'header__header_size', 'header__header_size_hex', 
            'link_info__link_info_header_size', 'header__hot_key', 
            'header__icon_index', 'link_info__volume_info__drive_type',
            'link_info__volume_info__drive_type_formatted'
        ]
        
        pattern_columns = [
            col for col in df.columns 
            if isinstance(col, str) and (
                col.endswith(('_time_raw', '_hex', '_timestamp')) or 
                col.startswith('link_info__offsets__')
            )
        ]
        
        columns_to_drop = static_columns + pattern_columns
        existing_columns = [col for col in columns_to_drop if col in df.columns]
        
        if existing_columns:
            df.drop(columns=existing_columns, inplace=True)
        
        # í–‰ í•„í„°ë§
        df = self._apply_lnk_row_filtering(df, result.name)
        return df
    
    def _apply_lnk_row_filtering(self, df: pd.DataFrame, file_name: str) -> pd.DataFrame:
        """LNK í–‰ í•„í„°ë§"""
        if 'lnk_files' not in str(file_name).lower():
            return df
        
        # ì‹œìŠ¤í…œ íŒŒì¼ ì œê±°
        target_col = next((col for col in ['Target_Path', 'target_info__target_path'] 
                          if col in df.columns), None)
        
        if target_col:
            system_keywords = [
                'Windows', 'System32', 'ProgramData', 'AppData', 
                'Update', 'Setup', 'Installer', 'MicrosoftEdge', 
                'Chrome', 'Temp', 'Cache'
            ]
            pattern = '|'.join(system_keywords)
            df = df[~df[target_col].str.contains(pattern, case=False, na=False)]
        
        return df

    def _filter_messenger_data(self, result: ResultDataFrame) -> pd.DataFrame:
        """ë©”ì‹ ì € ë°ì´í„° í•„í„°ë§"""
        self.dev_logger.info(f"ğŸ” [DEV] Applying 'MESSENGER' filter to {result.name}")
        df = result.data.copy()
        
        # ì—´ í•„í„°ë§
        columns_to_drop = [
            'created_timestamp', 'last_modified_timestamp', 'relative_path',
            'is_valid_file', 'messenger_type', 'file_index'
        ]
        
        existing_columns = [col for col in columns_to_drop if col in df.columns]
        if existing_columns:
            df.drop(columns=existing_columns, inplace=True)
        
        # í–‰ í•„í„°ë§
        df = self._apply_messenger_row_filtering(df, result.name)
        
        messenger_type = "Discord" if "Discord" in result.name else "KakaoTalk" if "KakaoTalk" in result.name else "Unknown"
        self.dev_logger.info(f"ğŸ“± [DEV] Processing {messenger_type}: {len(df)} rows")
        
        return df
    
    def _apply_messenger_row_filtering(self, df: pd.DataFrame, result_name: str) -> pd.DataFrame:
        """ë©”ì‹ ì € ë°ì´í„° í–‰ í•„í„°ë§"""
        if 'file_name' not in df.columns:
            self.dev_logger.warning("âš ï¸ 'file_name' column not found, skipping extension filtering.")
            return df
        
        # ì œì™¸í•  í™•ì¥ì
        exclude_extensions = (
            ".sys", ".ico", ".cur", ".msi", ".bat", ".sh",
            ".ttf", ".otf", ".editorconfig", ".eslintrc", ".npmignore"
        )
        
        initial_count = len(df)
        mask = ~df['file_name'].str.lower().str.endswith(exclude_extensions, na=False)
        filtered_df = df[mask]
        
        self.dev_logger.info(
            f"Filtered out {initial_count - len(filtered_df)} rows "
            f"based on {len(exclude_extensions)} excluded extensions."
        )
        
        return filtered_df

    def _filter_prefetch_data(self, result: ResultDataFrame) -> pd.DataFrame:
        """Prefetch ë°ì´í„° í•„í„°ë§"""
        self.dev_logger.info(f"ğŸ” [DEV] Applying 'PREFETCH' filter to {result.name}")
        df = result.data.copy()
        
        columns_to_drop = [
            'structure__executable_file_name', 'file_index', 
            'structure__signature', 'structure__format_version', 
            'is_compressed', 'parse_success', 'error_message',
            'structure__trace_chains_offset', 'structure__volumes_info_offset',
            'structure__filename_strings_size', 'structure__filename_strings_offset'
        ]
        
        existing_columns = [col for col in columns_to_drop if col in df.columns]
        if existing_columns:
            df.drop(columns=existing_columns, inplace=True)
        
        return df

    def _filter_usb_data(self, result: ResultDataFrame) -> pd.DataFrame:
        """USB ë°ì´í„° í•„í„°ë§"""
        self.dev_logger.info(f"ğŸ” [DEV] Applying 'USB' filter to {result.name}")
        df = result.data.copy()
        
        static_columns = [
            'volume_info__file_system', 'volume_info__mount_point', 
            'volume_info__volume_guid', 'volume_info__drive_letter', 
            'volume_info__volume_label', 'setupapi_info__serial_number', 
            'device_metadata__serial_number', 'setupapi_info__product_id', 
            'setupapi_info__volume_name', 'data_sources__has_usb_data', 
            'data_sources__has_usbstor_data', 'data_sources__has_setupapi_data', 
            'primary_source', 'portable_device_info__friendly_name'
        ]
        
        pattern_columns = [col for col in df.columns if col.startswith('connection_times__')]
        columns_to_drop = static_columns + pattern_columns
        
        existing_columns = [col for col in columns_to_drop if col in df.columns]
        if existing_columns:
            df.drop(columns=existing_columns, inplace=True)
        
        return df

    
        
    def _load_data(self, category: Category) -> ResultDataFrames:
        """ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ë¡œë“œ - íŒŒì¼ì´ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°"""
        try:
            self.dev_logger.debug(f"Starting data load for category: {category.name}")
            
            # helperë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë¡œë“œ ë° ì¸ì½”ë”© ì²˜ë¦¬
            df_results = self.helper.get_encoded_results(self.task_id, category)
            
            if not df_results:
                self.dev_logger.warning(f"No dataframes found for category {category.name} and task {self.task_id}")
                # ë¹ˆ ResultDataFrames ë°˜í™˜
                return ResultDataFrames(data=[])
            
            self.dev_logger.debug(f"Successfully loaded {len(df_results.data)} dataframes for category: {category.name}")
            return df_results
            
        except Exception as e:
            self.dev_logger.warning(f"âš ï¸ [DEV] Failed to load data for category {category.name}: {e}")
            self.dev_logger.info(f"â­ï¸ [DEV] Skipping category {category.name} - no data available")
            # ë¹ˆ ResultDataFrames ë°˜í™˜í•˜ì—¬ ê±´ë„ˆë›°ê¸°
            return ResultDataFrames(data=[])

    # def _generate_analysis_result(self):
    #     """
    #     self.created_artifactsë¥¼ í™œìš©í•˜ì—¬ ë¶„ë¥˜ê²°ê³¼ë¥¼ ìƒì„±í•¨.
    #     ê²°ê³¼ëŠ” ìƒì†ë°›ì€ í´ë˜ìŠ¤ì—ì„œ ì •ì˜ëœ ì•„ë˜ self.analyze_resultsì— ì—…ë°ì´íŠ¸í•  ê²ƒ.

    #     self.analyze_results = {
    #         behavior: {
    #             "job_id": self.job_id,
    #             "task_id": self.task_id,
    #             "behavior": behavior.name,
    #             "analysis_summary": "",
    #             "risk_level": "",
    #             "artifact_ids": []
    #         } for behavior in BehaviorType
    #     }
    #     """
    #     self.dev_logger.info("ğŸ”§ [DEV] Starting _generate_analysis_result")
        
    #     # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ ë©”ì„œë“œ ë¨¼ì € ì‹¤í–‰
    #     super()._generate_analysis_result()
        
    #     # created_artifactsê°€ ë¹„ì–´ìˆìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
    #     if not self.created_artifacts:
    #         self.dev_logger.warning("âš ï¸ [DEV] No artifacts created, skipping analysis result generation")
    #         return
        
    #     # ê° í–‰ìœ„ ìœ í˜•ë³„ë¡œ ì•„í‹°íŒ©íŠ¸ ë¶„ì„
    #     for behavior_type in BehaviorType:
    #         self.dev_logger.debug(f"ğŸ” [DEV] Analyzing behavior type: {behavior_type.name}")
            
    #         # í•´ë‹¹ í–‰ìœ„ì™€ ê´€ë ¨ëœ ì•„í‹°íŒ©íŠ¸ ìˆ˜ì§‘
    #         related_artifacts = self._get_artifacts_by_behavior(behavior_type)
            
    #         if not related_artifacts:
    #             self.dev_logger.debug(f"â„¹ï¸ [DEV] No artifacts found for {behavior_type.name}")
    #             continue
            
    #         # ì•„í‹°íŒ©íŠ¸ ID ëª©ë¡ ì¶”ì¶œ
    #         artifact_ids = [artifact.get('id') or artifact.get('artifact_id') 
    #                     for artifact in related_artifacts 
    #                     if artifact.get('id') or artifact.get('artifact_id')]
            
    #         # ë¶„ì„ ìš”ì•½ ìƒì„±
    #         analysis_summary = self._create_analysis_summary(behavior_type, related_artifacts)
            
    #         # ìœ„í—˜ë„ í‰ê°€
    #         risk_level = self._evaluate_risk_level(behavior_type, related_artifacts)
            
    #         # ê²°ê³¼ ì—…ë°ì´íŠ¸
    #         if behavior_type in self.analyze_results:
    #             self.analyze_results[behavior_type].update({
    #                 "artifact_ids": artifact_ids,
    #                 "analysis_summary": analysis_summary,
    #                 "risk_level": risk_level,
    #                 "artifact_count": len(artifact_ids)
    #             })
                
    #             self.dev_logger.info(
    #                 f"âœ… [DEV] Updated {behavior_type.name}: "
    #                 f"{len(artifact_ids)} artifacts, risk={risk_level}"
    #             )
        
    #     # ì „ì²´ ë¶„ì„ í†µê³„ ë¡œê¹…
    #     self._log_analysis_statistics()
        
    #     self.dev_logger.info("âœ… [DEV] Completed _generate_analysis_result")


    # def _get_artifacts_by_behavior(self, behavior_type: BehaviorType) -> List[dict]:
    #     """íŠ¹ì • í–‰ìœ„ ìœ í˜•ê³¼ ê´€ë ¨ëœ ì•„í‹°íŒ©íŠ¸ í•„í„°ë§"""
    #     related_artifacts = []
        
    #     for artifact in self.created_artifacts:
    #         # ì•„í‹°íŒ©íŠ¸ì˜ ì¹´í…Œê³ ë¦¬ë‚˜ ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í–‰ìœ„ ìœ í˜• ë§¤ì¹­
    #         artifact_behavior = artifact.get('behavior_type') or artifact.get('category')
            
    #         # ì§ì ‘ ë§¤ì¹­
    #         if artifact_behavior == behavior_type:
    #             related_artifacts.append(artifact)
    #             continue
            
    #         # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ë§¤í•‘
    #         if self._is_artifact_related_to_behavior(artifact, behavior_type):
    #             related_artifacts.append(artifact)
        
    #     return related_artifacts


    # def _is_artifact_related_to_behavior(self, artifact: dict, behavior_type: BehaviorType) -> bool:
    #     """ì•„í‹°íŒ©íŠ¸ê°€ íŠ¹ì • í–‰ìœ„ ìœ í˜•ê³¼ ê´€ë ¨ì´ ìˆëŠ”ì§€ íŒë‹¨"""
    #     category = artifact.get('category', '').lower()
    #     artifact_type = artifact.get('type', '').lower()
    #     file_name = artifact.get('file_name', '').lower()
        
    #     # í–‰ìœ„ ìœ í˜•ë³„ ë§¤í•‘ ê·œì¹™
    #     behavior_mappings = {
    #         BehaviorType.USB_USAGE: ['usb', 'external_device', 'removable'],
    #         BehaviorType.FILE_ACCESS: ['lnk', 'shortcut', 'recent', 'jump_list'],
    #         BehaviorType.WEB_BROWSING: ['browser', 'chrome', 'edge', 'firefox', 'url', 'download'],
    #         BehaviorType.MESSENGER_USAGE: ['messenger', 'kakao', 'discord', 'telegram', 'chat'],
    #         BehaviorType.PROGRAM_EXECUTION: ['prefetch', 'execution', 'process', 'application'],
    #         BehaviorType.FILE_DELETION: ['deleted', 'recycle', 'mft_deleted', 'removed'],
    #         BehaviorType.DATA_EXFILTRATION: ['download', 'transfer', 'upload', 'export'],
    #     }
        
    #     # í•´ë‹¹ í–‰ìœ„ ìœ í˜•ì˜ í‚¤ì›Œë“œ í™•ì¸
    #     keywords = behavior_mappings.get(behavior_type, [])
        
    #     return any(keyword in category or keyword in artifact_type or keyword in file_name 
    #             for keyword in keywords)


    # def _create_analysis_summary(self, behavior_type: BehaviorType, artifacts: List[dict]) -> str:
    #     """í–‰ìœ„ ìœ í˜•ë³„ ë¶„ì„ ìš”ì•½ ìƒì„±"""
    #     artifact_count = len(artifacts)
        
    #     # í–‰ìœ„ ìœ í˜•ë³„ ìš”ì•½ í…œí”Œë¦¿
    #     summary_templates = {
    #         BehaviorType.USB_USAGE: self._summarize_usb_usage,
    #         BehaviorType.FILE_ACCESS: self._summarize_file_access,
    #         BehaviorType.WEB_BROWSING: self._summarize_web_browsing,
    #         BehaviorType.MESSENGER_USAGE: self._summarize_messenger_usage,
    #         BehaviorType.PROGRAM_EXECUTION: self._summarize_program_execution,
    #         BehaviorType.FILE_DELETION: self._summarize_file_deletion,
    #         BehaviorType.DATA_EXFILTRATION: self._summarize_data_exfiltration,
    #     }
        
    #     # í•´ë‹¹ í–‰ìœ„ì— ë§ëŠ” ìš”ì•½ í•¨ìˆ˜ ì‹¤í–‰
    #     summarize_func = summary_templates.get(behavior_type)
    #     if summarize_func:
    #         return summarize_func(artifacts)
        
    #     # ê¸°ë³¸ ìš”ì•½
    #     return f"Found {artifact_count} artifact(s) related to {behavior_type.name}"


    # def _summarize_usb_usage(self, artifacts: List[dict]) -> str:
    #     """USB ì‚¬ìš© ë¶„ì„ ìš”ì•½"""
    #     device_count = len(set(a.get('device_id') for a in artifacts if a.get('device_id')))
    #     connection_count = sum(a.get('connection_count', 1) for a in artifacts)
        
    #     return (f"Detected {device_count} unique USB device(s) with "
    #             f"{connection_count} total connection(s). "
    #             f"Analysis based on {len(artifacts)} artifact(s).")


    # def _summarize_file_access(self, artifacts: List[dict]) -> str:
    #     """íŒŒì¼ ì ‘ê·¼ ë¶„ì„ ìš”ì•½"""
    #     file_types = set()
    #     for artifact in artifacts:
    #         file_name = artifact.get('file_name', '')
    #         if '.' in file_name:
    #             ext = file_name.rsplit('.', 1)[-1].lower()
    #             file_types.add(ext)
        
    #     accessed_files = len(artifacts)
    #     return (f"Analyzed {accessed_files} file access record(s) "
    #             f"across {len(file_types)} file type(s): {', '.join(sorted(file_types)[:5])}")


    # def _summarize_web_browsing(self, artifacts: List[dict]) -> str:
    #     """ì›¹ ë¸Œë¼ìš°ì§• ë¶„ì„ ìš”ì•½"""
    #     url_count = sum(1 for a in artifacts if 'url' in str(a.get('type', '')).lower())
    #     download_count = sum(1 for a in artifacts if 'download' in str(a.get('type', '')).lower())
        
    #     return (f"Analyzed {len(artifacts)} web browsing artifact(s): "
    #             f"{url_count} URL visit(s), {download_count} download(s)")


    # def _summarize_messenger_usage(self, artifacts: List[dict]) -> str:
    #     """ë©”ì‹ ì € ì‚¬ìš© ë¶„ì„ ìš”ì•½"""
    #     messenger_types = set(a.get('messenger_type') for a in artifacts if a.get('messenger_type'))
    #     file_count = len(artifacts)
        
    #     if messenger_types:
    #         messengers = ', '.join(sorted(messenger_types))
    #         return f"Detected {file_count} messenger file(s) from: {messengers}"
        
    #     return f"Detected {file_count} messenger-related file(s)"


    # def _summarize_program_execution(self, artifacts: List[dict]) -> str:
    #     """í”„ë¡œê·¸ë¨ ì‹¤í–‰ ë¶„ì„ ìš”ì•½"""
    #     programs = set(a.get('program_name') or a.get('executable_name') 
    #                 for a in artifacts 
    #                 if a.get('program_name') or a.get('executable_name'))
        
    #     return (f"Identified {len(programs)} unique program(s) executed. "
    #             f"Total {len(artifacts)} execution record(s).")


    # def _summarize_file_deletion(self, artifacts: List[dict]) -> str:
    #     """íŒŒì¼ ì‚­ì œ ë¶„ì„ ìš”ì•½"""
    #     deleted_count = len(artifacts)
    #     total_size = sum(a.get('file_size', 0) for a in artifacts)
        
    #     size_mb = total_size / (1024 * 1024) if total_size > 0 else 0
    #     return (f"Found {deleted_count} deleted file(s). "
    #             f"Total size: {size_mb:.2f} MB")


    # def _summarize_data_exfiltration(self, artifacts: List[dict]) -> str:
    #     """ë°ì´í„° ìœ ì¶œ ë¶„ì„ ìš”ì•½"""
    #     transfer_count = len(artifacts)
    #     suspicious_count = sum(1 for a in artifacts if a.get('is_suspicious', False))
        
    #     return (f"Detected {transfer_count} data transfer event(s). "
    #             f"{suspicious_count} flagged as potentially suspicious.")


    # def _evaluate_risk_level(self, behavior_type: BehaviorType, artifacts: List[dict]) -> str:
    #     """ìœ„í—˜ë„ í‰ê°€"""
    #     artifact_count = len(artifacts)
        
    #     # í–‰ìœ„ ìœ í˜•ë³„ ê¸°ë³¸ ìœ„í—˜ë„
    #     base_risk = {
    #         BehaviorType.DATA_EXFILTRATION: 'HIGH',
    #         BehaviorType.FILE_DELETION: 'MEDIUM',
    #         BehaviorType.USB_USAGE: 'MEDIUM',
    #         BehaviorType.WEB_BROWSING: 'LOW',
    #         BehaviorType.MESSENGER_USAGE: 'LOW',
    #         BehaviorType.FILE_ACCESS: 'LOW',
    #         BehaviorType.PROGRAM_EXECUTION: 'LOW',
    #     }
        
    #     risk = base_risk.get(behavior_type, 'LOW')
        
    #     # ì•„í‹°íŒ©íŠ¸ ìˆ˜ì— ë”°ë¥¸ ìœ„í—˜ë„ ì¡°ì •
    #     if artifact_count > 100:
    #         if risk == 'LOW':
    #             risk = 'MEDIUM'
    #         elif risk == 'MEDIUM':
    #             risk = 'HIGH'
    #     elif artifact_count > 50:
    #         if risk == 'LOW':
    #             risk = 'MEDIUM'
        
    #     # ì˜ì‹¬ìŠ¤ëŸ¬ìš´ íŒ¨í„´ ê°ì§€
    #     suspicious_count = sum(1 for a in artifacts if a.get('is_suspicious', False))
    #     if suspicious_count > artifact_count * 0.3:  # 30% ì´ìƒì´ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê²½ìš°
    #         if risk == 'LOW':
    #             risk = 'MEDIUM'
    #         elif risk == 'MEDIUM':
    #             risk = 'HIGH'
        
    #     return risk


    # def _log_analysis_statistics(self):
    #     """ì „ì²´ ë¶„ì„ í†µê³„ ë¡œê¹… - ì—…ê·¸ë ˆì´ë“œëœ ë²„ì „"""
    #     total_artifacts = len(self.created_artifacts)
    #     behaviors_with_data = sum(1 for result in self.analyze_results.values() 
    #                             if result.get('artifact_count', 0) > 0)
        
    #     high_risk_count = sum(1 for result in self.analyze_results.values() 
    #                         if result.get('risk_level') == 'HIGH')
    #     medium_risk_count = sum(1 for result in self.analyze_results.values() 
    #                         if result.get('risk_level') == 'MEDIUM')
    #     low_risk_count = sum(1 for result in self.analyze_results.values() 
    #                         if result.get('risk_level') == 'LOW')
        
    #     # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ê³„ì‚°
    #     category_stats = {}
    #     for artifact in self.created_artifacts:
    #         category = artifact.get('category', 'Unknown')
    #         if category not in category_stats:
    #             category_stats[category] = {'count': 0, 'risk_levels': []}
    #         category_stats[category]['count'] += 1
    #         category_stats[category]['risk_levels'].append(artifact.get('risk_level', 'UNKNOWN'))
        
    #     # í•„í„°ë§ íš¨ê³¼ í†µê³„ (ì›ë³¸ ë°ì´í„°ì™€ ë¹„êµ)
    #     total_original_rows = getattr(self, '_total_original_rows', 0)
    #     total_filtered_rows = getattr(self, '_total_filtered_rows', 0)
    #     filtering_reduction = total_original_rows - total_filtered_rows
    #     filtering_percentage = (filtering_reduction / total_original_rows * 100) if total_original_rows > 0 else 0
        
    #     # ìƒì„¸ í†µê³„ ë¡œê¹…
    #     self.dev_logger.info("=" * 80)
    #     self.dev_logger.info("ğŸ“Š [DEV] Enhanced Analysis Statistics Summary")
    #     self.dev_logger.info("=" * 80)
        
    #     # ê¸°ë³¸ í†µê³„
    #     self.dev_logger.info("ğŸ” Basic Statistics:")
    #     self.dev_logger.info(f"  â€¢ Total Artifacts: {total_artifacts:,}")
    #     self.dev_logger.info(f"  â€¢ Behaviors with Data: {behaviors_with_data}/{len(BehaviorType)}")
    #     self.dev_logger.info(f"  â€¢ Data Coverage: {(behaviors_with_data/len(BehaviorType)*100):.1f}%")
        
    #     # ìœ„í—˜ë„ ë¶„í¬
    #     self.dev_logger.info("\nâš ï¸ Risk Level Distribution:")
    #     self.dev_logger.info(f"  â€¢ High Risk: {high_risk_count} behaviors")
    #     self.dev_logger.info(f"  â€¢ Medium Risk: {medium_risk_count} behaviors")
    #     self.dev_logger.info(f"  â€¢ Low Risk: {low_risk_count} behaviors")
        
    #     # í•„í„°ë§ íš¨ê³¼
    #     if total_original_rows > 0:
    #         self.dev_logger.info("\nğŸ“‰ Filtering Effectiveness:")
    #         self.dev_logger.info(f"  â€¢ Original Data: {total_original_rows:,} rows")
    #         self.dev_logger.info(f"  â€¢ Filtered Data: {total_filtered_rows:,} rows")
    #         self.dev_logger.info(f"  â€¢ Reduction: {filtering_reduction:,} rows ({filtering_percentage:.1f}%)")
        
    #     # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    #     if category_stats:
    #         self.dev_logger.info("\nğŸ“ Category-wise Statistics:")
    #         for category, stats in sorted(category_stats.items()):
    #             risk_distribution = {}
    #             for risk in stats['risk_levels']:
    #                 risk_distribution[risk] = risk_distribution.get(risk, 0) + 1
                
    #             risk_str = ", ".join([f"{risk}: {count}" for risk, count in risk_distribution.items()])
    #             self.dev_logger.info(f"  â€¢ {category}: {stats['count']} artifacts ({risk_str})")
        
    #     # í–‰ìœ„ë³„ ìƒì„¸ í†µê³„
    #     self.dev_logger.info("\nğŸ¯ Behavior-wise Details:")
    #     for behavior_type, result in self.analyze_results.items():
    #         artifact_count = result.get('artifact_count', 0)
    #         if artifact_count > 0:
    #             risk_level = result.get('risk_level', 'UNKNOWN')
    #             analysis_summary = result.get('analysis_summary', 'No summary available')
                
    #             # ìœ„í—˜ë„ì— ë”°ë¥¸ ì´ëª¨ì§€
    #             risk_emoji = {
    #                 'HIGH': 'ğŸ”´',
    #                 'MEDIUM': 'ğŸŸ¡', 
    #                 'LOW': 'ğŸŸ¢',
    #                 'UNKNOWN': 'âšª'
    #             }.get(risk_level, 'âšª')
                
    #             self.dev_logger.info(f"  {risk_emoji} {behavior_type.name}:")
    #             self.dev_logger.info(f"    â€¢ Artifacts: {artifact_count}")
    #             self.dev_logger.info(f"    â€¢ Risk Level: {risk_level}")
    #             self.dev_logger.info(f"    â€¢ Summary: {analysis_summary[:100]}{'...' if len(analysis_summary) > 100 else ''}")
        
    #     # ì„±ëŠ¥ í†µê³„
    #     processing_time = getattr(self, '_processing_time', 0)
    #     if processing_time > 0:
    #         self.dev_logger.info(f"\nâ±ï¸ Performance:")
    #         self.dev_logger.info(f"  â€¢ Processing Time: {processing_time:.2f} seconds")
    #         if total_artifacts > 0:
    #             self.dev_logger.info(f"  â€¢ Artifacts per Second: {total_artifacts/processing_time:.2f}")
        
    #     self.dev_logger.info("=" * 80)

