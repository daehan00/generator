from dataclasses import dataclass
import logging
from unknown_data import Category, ResultDataFrames
from unknown_data.test import TestHelper
from unknown_data.loader.base import Config_db
import gc  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ìœ„í•œ ëª¨ë“ˆ
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
import logging
import enum
import uuid

logger = logging.getLogger(__name__)



class BackendClient:
    """í…ŒìŠ¤íŠ¸ìš© BackendClient - ì‹¤ì œ ì „ì†¡ ëŒ€ì‹  ë°ì´í„° ê²€ì¦ ë° ì¶œë ¥"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"BackendClient")
        self.sent_artifacts: List[Dict[str, Any]] = []
        self.sent_analysis_results: List[Dict[str, Any]] = []
        self.artifact_id_counter = 1
        self.analysis_result_id_counter = 1
    
    def send_artifact_datas(self, artifacts: List[Dict[str, Any]]) -> Optional[List[Dict[str, Any]]]:
        """ì•„í‹°íŒ©íŠ¸ ë°ì´í„° ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°˜í™˜"""
        self.logger.info(f"ğŸ§ª [TEST] Received {len(artifacts)} artifacts for validation")
        
        # ë°ì´í„° ê²€ì¦
        created_artifacts = []
        for i, artifact in enumerate(artifacts):
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            required_fields = ['data', 'collected_at', 'artifact_type']
            missing_fields = [field for field in required_fields if field not in artifact]
            
            if missing_fields:
                self.logger.error(f"âŒ Artifact {i}: Missing required fields: {missing_fields}")
                return None
            
            # ë°ì´í„° íƒ€ì… ê²€ì¦
            if not isinstance(artifact['data'], dict):
                self.logger.error(f"âŒ Artifact {i}: 'data' field must be a dictionary")
                return None
            
            # í…ŒìŠ¤íŠ¸ìš© ID ìƒì„± ë° ì™„ì „í•œ ì•„í‹°íŒ©íŠ¸ ê°ì²´ ìƒì„±
            created_artifact = {
                "id": str(uuid.uuid4()),
                **artifact,
                # "created_at": datetime.now(timezone.utc).isoformat(),
                "status": ""
            }
            created_artifacts.append(created_artifact)
            self.artifact_id_counter += 1
            
        self.sent_artifacts.extend(created_artifacts)
        self.logger.info(f"âœ… [TEST] Successfully validated and stored {len(created_artifacts)} artifacts")
        self.logger.info(f"ğŸ”¢ [TEST] Total artifacts stored: {len(self.sent_artifacts)}")
        
        return created_artifacts
    
    def send_analysis_result_with_artifacts(self, analysis_result_data: Dict[str, Any]) -> Optional[str]:
        """ë¶„ì„ ê²°ê³¼ ë°ì´í„° ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°˜í™˜"""
        self.logger.info(f"ğŸ§ª [TEST] Received analysis result for behavior: {analysis_result_data.get('behavior')}")
        
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        required_fields = ['job_id', 'task_id', 'behavior', 'analysis_summary', 'risk_level', 'artifact_ids']
        missing_fields = [field for field in required_fields if field not in analysis_result_data]
        
        if missing_fields:
            self.logger.error(f"âŒ Analysis result missing required fields: {missing_fields}")
            return None
        
        # ìœ„í—˜ë„ ê²€ì¦
        valid_risk_levels = [level.value for level in RiskLevel]
        if analysis_result_data['risk_level'] not in valid_risk_levels and analysis_result_data['risk_level']:
            self.logger.error(f"âŒ Invalid risk level: {analysis_result_data['risk_level']}")
            return None
        
        # í…ŒìŠ¤íŠ¸ìš© ID ìƒì„±
        result_id = f"test_analysis_{self.analysis_result_id_counter}"
        self.analysis_result_id_counter += 1
        
        # ê²°ê³¼ ì €ì¥
        stored_result = {
            "id": result_id,
            **analysis_result_data,
            "created_at": datetime.now(timezone.utc).isoformat()
        }
        self.sent_analysis_results.append(stored_result)
        
        # ê²°ê³¼ ì¶œë ¥
        self.logger.info(f"ğŸ“‹ [TEST] Analysis Result Details:")
        self.logger.info(f"  - Behavior: {analysis_result_data['behavior']}")
        self.logger.info(f"  - Risk Level: {analysis_result_data['risk_level']}")
        self.logger.info(f"  - Summary: {analysis_result_data['analysis_summary']}")
        self.logger.info(f"  - Artifact Count: {len(analysis_result_data['artifact_ids'])}")
        
        self.logger.info(f"âœ… [TEST] Successfully stored analysis result with ID: {result_id}")
        return result_id
    
    def get_test_summary(self) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê²°ê³¼ ìš”ì•½"""
        return {
            "total_artifacts": len(self.sent_artifacts),
            "total_analysis_results": len(self.sent_analysis_results),
            "artifacts_by_type": self._count_by_field(self.sent_artifacts, 'artifact_type'),
            "artifacts_by_behavior": self._count_by_field_behavior(self.sent_analysis_results, 'behavior'),
            "artifacts_by_risk_level": self._count_by_field(self.sent_analysis_results, 'risk_level')
        }
    
    def _count_by_field(self, items: List[Dict], field: str) -> Dict[str, int]:
        """í•„ë“œë³„ ì¹´ìš´íŠ¸"""
        counts = {}
        for item in items:
            value = item.get(field, 'unknown')
            counts[value] = counts.get(value, 0) + 1
        return counts

    def _count_by_field_behavior(self, items: List[Dict], field: str) -> Dict[str, int]:
        """í•„ë“œë³„ ì¹´ìš´íŠ¸"""
        counts = {}
        for item in items:
            value = item.get(field, 'unknown')
            counts[value] = len(item["artifact_ids"])
        return counts



@dataclass
class Artifact:
    data: Dict[str, Any]
    collected_at: datetime
    artifact_type: Optional[str] = None
    source: Optional[str] = None
    summary: Optional[str] = None
    hash_value: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Artifact ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'data': self.data,
            'collected_at': self.collected_at.isoformat() if isinstance(self.collected_at, datetime) else self.collected_at,
            'artifact_type': self.artifact_type,
            'source': self.source,
            'summary': self.summary,
            'hash_value': self.hash_value
        }


class RiskLevel(str, enum.Enum):
    normal = "normal"
    suspicious = "suspicious"
    dangerous = "dangerous"


class BehaviorType(str, enum.Enum):
    acquisition = "acquisition"
    forgery = "forgery"
    upload = "upload"
    deletion = "deletion"
    etc = "etc"

FILE_COLUMN_MAP = {
    # ë¸Œë¼ìš°ì €
    "binary": {"keyword": "file_neme", "time": "timestamp"},
    "browser_collected_files": {"keyword": "file_neme", "time": "timestamp"},
    "browser_discovered_profiles": {"keyword": "base_path", "time": None},
    "Chrome.keyword_search_terms": {"keyword": "term", "time": None},
    "Edge.keyword_search_terms": {"keyword": "term", "time": None},
    "Chrome_keyword": {"keyword": "keyword", "time": "last_visited"},
    "Edge_keyword": {"keyword": "keyword", "time": "last_visited"},
    "Chrome.urls": {"keyword": "title", "time": "last_visit_time"},
    "Edge.urls": {"keyword": "title", "time": "last_visit_time"},
    "Chrome.visits": {"keyword": "url", "time": "visit_time"},
    "Edge.visits": {"keyword": "url", "time": "visit_time"},
    "Chrome.visited_links": {"keyword": "link_url_id", "time": None},
    "Edge.visited_links": {"keyword": "link_url_id", "time": None},
    "Chrome.autofill": {"keyword": "value", "time": "date_created"},
    "Edge.autofill": {"keyword": "value", "time": "date_created"},
    "Chrome.addresses": {"keyword": "guid", "time": "use_date"},
    "Edge.addresses": {"keyword": "guid", "time": "use_date"},
    "Chrome.autofill_sync_metadata": {"keyword": "storage_key", "time": None},
    "Edge.autofill_sync_metadata": {"keyword": "storage_key", "time": None},
    "Chrome.downloads": {"keyword": "target_path", "time": "end_time"},
    "Edge.downloads": {"keyword": "target_path", "time": "end_time"},
    "Chrome.downloads_url_chains": {"keyword": "url", "time": None},
    "Edge.downloads_url_chains": {"keyword": "url", "time": None},
    "Chrome.logins": {"keyword": "signon_realm", "time": "date_last_used"},
    "Edge.logins": {"keyword": "signon_realm", "time": "date_last_used"},
    
    # íœ´ì§€í†µ
    "data_sources": {"keyword": "has_mft_data", "time": None},
    "mft_deleted_files": {"keyword": "file_name", "time": "deleted_time"},
    "recycle_bin_files": {"keyword": "original_file_name", "time": "deleted_time"},
    "statistics": {"keyword": "total_deleted_files", "time": None},

    # LNK
    "lnk_files": {"keyword": "file_name", "time": "target_info__target_times__access"},
    
    # ë©”ì‹ ì €
    "Discord.files": {"keyword": "file_name", "time": "last_modified"},
    "KakaoTalk.files": {"keyword": "file_name", "time": "last_modified"},
    
    # í”„ë¦¬íŒ¨ì¹˜
    "prefetch_files": {"keyword": "file_name", "time": "last_run_times_1"},
    
    # USB
    "usb_devices": {"keyword": "setupapi_info__user_account", "time": "setupapi_info__last_connection_time"}
}

class Analyzer:
    def __init__(self, backend_client: BackendClient) -> None:
        self.task_id: str = ''
        self.backend_client = backend_client    
        self.created_artifacts: List[Dict[str, Any]] = []
        self.set_database_settings()

    def set_database_settings(self) -> None:
        """ í—¬í¼ëŠ” í˜„ì¬ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì ìš©ë¨. ë°°í¬ìš©ìœ¼ë¡œ ë³€ê²½í•´ì•¼ í•¨. """
        db_config = Config_db(
            dbms="postgresql",
            username="forensic_agent",
            password="0814",
            ip="13.124.25.47",
            port=5432,
            database_name="forensic_agent"
        )
        
        self.helper = TestHelper(db_config)

    def analyze(self, task_id: str, job_id:str):
        self.task_id = task_id
        self.job_id = job_id

        try:
            for category in Category:
                logger.debug(f"ğŸ”„ Processing category: {category.name}")
                
                self._filter_and_save_artifacts(category)
            
            logger.info(f"Artifact processing completed successfully for task: {task_id}")

            self._make_and_save_analyze_results()

            logger.info(f"Analysis process completed successfully for task: {task_id}")

        except Exception as e:
            logger.error(f"âŒ Analysis failed for task {task_id}: {e}", exc_info=True)
            raise

    def _filter_and_save_artifacts(self, category: Category):
        """ ì›ë³¸ ì•„í‹°íŒ©íŠ¸ë“¤ì— ëŒ€í•œ ì²˜ë¦¬ ë©”ì„œë“œ"""
        try:
            # 1. ì›ë³¸ ì•„í‹°íŒ©íŠ¸ ë°ì´í„° ë¡œë“œ(ë°ì´í„° íƒ€ì…: ResultDataFrames)
            df_results = self._load_data(category)
            
            # 2. ì•„í‹°íŒ©íŠ¸ ë°ì´í„° í•„í„°ë§ ì²˜ë¦¬(ë°ì´í„° íƒ€ì…: ResultDataFrames)
            # dataframes_dictê°€ Noneì¸ ê²½ìš° ë¡œê·¸ë§Œ ì¶œë ¥í•˜ê³  ë¹ˆ ë°ì´í„°ë¡œ ì²˜ë¦¬
            if df_results is None:
                logger.error(f"No data found for category: {category.name}")
                return

            logger.debug(f"Filtering data for category: {category.name}")
            filtered_data = self._filter_data(category, df_results)
            logger.debug(f"Filtering data success: {category.name}")

            if filtered_data.data is None:
                logger.info(f"No data is found to analyze for category: {category.name}")
                return
            
            # 3. ResultDataFramesì—ì„œ ë°±ì—”ë“œë¡œ ì „ì†¡í•˜ê¸° ìœ„í•œ í˜•íƒœë¡œ ê°€ê³µ
            artifacts_data = self._filtered_data_to_artifacts_data(filtered_data)
            
            # 4. ê°€ê³µëœ ë°ì´í„° ë°±ì—”ë“œë¡œ ì „ì†¡
            logger.debug(f"Sending data to backend for category: {category.name}")
            success = self._send_data_to_backend(category, artifacts_data)
            
            # ë©”ëª¨ë¦¬ í•´ì œ: ë¶„ì„ ê²°ê³¼ ì‚­ì œ
            del filtered_data
            del artifacts_data
            # df_results í•´ì œ
            if df_results is not None:
                del df_results
            gc.collect()

            if not success:
                raise
        except Exception as e:
            logger.error(f"âŒ {category.name} Analysis failed for task {self.task_id}: {e}", exc_info=True)
            raise
    
    def _load_data(self, category: Category) -> ResultDataFrames:
        """ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ë¡œë“œ - helperë¥¼ ì´ìš©í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            logger.debug(f"Starting data load for category: {category.name}")
            
            # helperë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë¡œë“œ ë° ì¸ì½”ë”© ì²˜ë¦¬
            df_results = self.helper.get_encoded_results(self.task_id, category)
            
            if not df_results:
                logger.error(f"No dataframes found for category {category.name} and task {self.task_id}")
                raise ValueError(f"No data available for category {category.name} and task {self.task_id}")
            
            logger.debug(f"Successfully loaded {len(df_results.data)} dataframes for category: {category.name}")
            return df_results
            
        except Exception as e:
            logger.error(f"Failed to load data for category {category.name}: {e}", exc_info=True)
            raise
    
    def _filter_data(self, category: Category, df_results: ResultDataFrames) -> ResultDataFrames:
        """
        <ê°œë°œì´ í•„ìš”í•œ ë©”ì„œë“œ>
        ìˆ˜ë§ì€ ë°ì´í„° ì¤‘ í•„ìš”ì—†ëŠ” ë°ì´í„°ë¥¼ ë²„ë¦¬ê³  í•„í„°ë§í•˜ëŠ” ê³¼ì •
        ì„ì‹œë¡œ ì‘ì„±í•´ë†“ì€ ë©”ì„œë“œì´ë‹¤.
        """
        # for result in df_results.data:
        #     result.data = result.data.head(5) # í˜„ì¬ëŠ” ë°ì´í„°í”„ë ˆì„ë³„ë¡œ 5ê°œì”©ë§Œ ê°€ì ¸ì™€ì„œ ì €ì¥í•˜ëŠ” í…ŒìŠ¤íŠ¸ ì½”ë“œ.
        
        return ResultDataFrames()
    
    def _minha_hamsu(self, artifact_type: str):
        data = FILE_COLUMN_MAP.get(artifact_type, {})
        return data.get("keyword", ""), data.get("time", "")


    def _filtered_data_to_artifacts_data(self, filtered_data: ResultDataFrames) -> List[Dict[str, Any]]:
        """ ì²˜ë¦¬í•œ ë°ì´í„°ë¥¼ ë°±ì—”ë“œë¡œ ì „ì†¡í•˜ê¸° ìœ„í•œ í˜•íƒœë¡œ ë³€í™˜ """
        artifacts_dict_list = []
        
        for data in filtered_data.data:
            artifacts_json = data.data.to_dict('records')
            data_name = data.name if hasattr(data, 'name') else 'unknown'
            
            # ê³µí†µ ë©”íƒ€ë°ì´í„° (ë°˜ë³µë˜ëŠ” ë¶€ë¶„ì„ ë¯¸ë¦¬ ê³„ì‚°)
            artifact_type = f"{data_name}_data"

            source_key, time_key = self._minha_hamsu(data_name)
            
            for artifact_data in artifacts_json:
                # íƒ€ì… ì•ˆì „ì„±ì„ ìœ„í•œ ìºìŠ¤íŒ…
                artifact_data_dict: Dict[str, Any] = {str(k): v for k, v in artifact_data.items()}
                
                # ì§ì ‘ ë”•ì…”ë„ˆë¦¬ ìƒì„± (ê°ì²´ ìƒì„± ì˜¤ë²„í—¤ë“œ ì—†ìŒ)
                artifact_dict = {
                    'data': artifact_data_dict,
                    # 'collected_at': current_time_iso,
                    'artifact_type': artifact_type,
                    'source': artifact_data_dict.get(source_key, None),
                    'collected_at': artifact_data_dict.get(time_key, None),
                }
                
                artifacts_dict_list.append(artifact_dict)

        return artifacts_dict_list
    
    def _send_data_to_backend(self, category: Category, artifacts_data: List[Dict[str, Any]]) -> bool:
        """ ë³€í™˜ëœ ë°ì´í„°ë¥¼ ë°›ì•„ ë°±ì—”ë“œë¡œ ì „ì†¡ """

        logger.debug(f"ğŸ“¦ Sending {len(artifacts_data)} filtered artifacts for category: {category.name}")
        
        # ì´ë¯¸ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì´ë¯€ë¡œ ë°”ë¡œ ì „ì†¡
        created_artifacts = self.backend_client.send_artifact_datas(artifacts_data)
        
        if created_artifacts is not None:
            # ìƒì„±ëœ ì™„ì „í•œ ì•„í‹°íŒ©íŠ¸ ê°ì²´ë“¤ì„ ì €ì¥
            self.created_artifacts.extend(created_artifacts)
            
            logger.debug(f"âœ… Successfully sent {len(artifacts_data)} artifacts for category: {category.name}")
            logger.debug(f"ğŸ“‹ Stored {len(created_artifacts)} complete artifact objects")
            logger.debug(f"ğŸ”¢ Total artifacts stored so far: {len(self.created_artifacts)}")
            return True
        else:
            logger.error(f"âŒ Failed to send artifacts for category: {category.name}")
            return False

    def _make_and_save_analyze_results(self):
        """ í–‰ìœ„ì— ë”°ë¥¸ ë¶„ë¥˜ê²°ê³¼ë¥¼ ì €ì¥ """
        self.analyze_results = {
            behavior: {
                "job_id": self.job_id,
                "task_id": self.task_id,
                "behavior": behavior.name,
                "analysis_summary": "",
                "risk_level": "",
                "artifact_ids": []
            } for behavior in BehaviorType
        }

        # ë¶„ì„ ê²°ê³¼ ìƒì„±
        self._generate_analysis_result()

        # analyze_results ìˆœíšŒí•˜ë©´ì„œ ì €ì¥
        failed_behaviors = []
        for behavior in self.analyze_results.keys():
            # ëª¨ë“  behaviorì— ëŒ€í•´ ë°±ì—”ë“œë¡œ ì œì¶œ
            result = self.analyze_results[behavior]
            analysis_result_id = self.backend_client.send_analysis_result_with_artifacts(result)
            
            if analysis_result_id is None:
                failed_behaviors.append(behavior)
                logger.error(f"Failed to save analysis result for behavior: {behavior}")
            else:
                logger.debug(f"Successfully saved analysis result for behavior: {behavior} (ID: {analysis_result_id})")
        
        if failed_behaviors:
            raise RuntimeError(f"Failed to save analysis results for behaviors: {[b.name for b in failed_behaviors]}")

    def _generate_analysis_result(self):
        """
        <ê°œë°œì´ í•„ìš”í•œ ë©”ì„œë“œ>
        self.created_artifactsë¥¼ í™œìš©í•˜ì—¬ ë¶„ë¥˜ê²°ê³¼ë¥¼ ìƒì„±í•¨.
        ê²°ê³¼ëŠ” self.analyze_resultsì— ì—…ë°ì´íŠ¸í•  ê²ƒ.
        """
        pass