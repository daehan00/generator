"""
ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ëª¨ë“ˆ
- ì•„í‹°íŒ©íŠ¸ ë¡œë”© ë° ì²­í¬ ë¶„í• 
- ë³´ê³ ì„œ ì¶œë ¥
- ì²­í¬ ì‚¬ì´ì¦ˆ ìµœì í™”
"""

from typing import List
from common.test_backendclient import TestBackendClient
from common.models import ScenarioCreate
from langchain.chat_models import init_chat_model


from dotenv import load_dotenv
load_dotenv("../.env")

# LLM ì´ˆê¸°í™”
llm_small = init_chat_model("google_genai:gemini-2.5-flash-lite", temperature=0)
llm_medium = init_chat_model("google_genai:gemini-2.5-flash", temperature=0)
llm_large = init_chat_model("google_genai:gemini-2.5-pro", temperature=0)

def load_artifacts(task_id: str) -> List[dict]:
    """ì•„í‹°íŒ©íŠ¸ë¥¼ ë°±ì—”ë“œì—ì„œ ë¡œë“œ"""
    backend_client = TestBackendClient()
    job_id = "test+job_id"
    return backend_client.load_artifacts(task_id, job_id)


def chunk_artifacts(artifacts: List[dict], chunk_size: int = 50) -> List[List[dict]]:
    """ì•„í‹°íŒ©íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    return [artifacts[i:i+chunk_size] for i in range(0, len(artifacts), chunk_size)]


def pretty_print_scenario(scenario: ScenarioCreate):
    """ì‹œë‚˜ë¦¬ì˜¤ ê°ì²´ë¥¼ ë°›ì•„ ê°€ë…ì„± ì¢‹ì€ ë³´ê³ ì„œ í˜•íƒœë¡œ ì¶œë ¥í•©ë‹ˆë‹¤."""
    
    print("="*80)
    print(f"ğŸ“œ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ ë³´ê³ ì„œ: {scenario.name}")
    print("="*80)
    
    print("\n[ ë³´ê³ ì„œ ê°œìš” ]")
    print(f"  - {scenario.description}")
    
    print("\n[ ì‹ë³„ ì •ë³´ ]")
    print(f"  - Job ID: {scenario.job_id}")
    print(f"  - Task ID: {scenario.task_id}")
    
    print("\n[ ì¬êµ¬ì„±ëœ ê³µê²© ë‹¨ê³„ (Timeline) ]")
    print("-" * 80)
    
    if not scenario.steps:
        print("  (ë¶„ì„ëœ ë‹¨ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.)")
    else:
        # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬ (ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆì§€ë§Œ ì•ˆì „ì¥ì¹˜)
        sorted_steps = sorted(scenario.steps, key=lambda s: s.order_no)
        
        for step in sorted_steps:
            # datetime ê°ì²´ë¥¼ ë³´ê¸° ì¢‹ì€ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…
            timestamp_str = step.timestamp.strftime('%Y-%m-%d %H:%M:%S') if step.timestamp else ""
            
            # ì•„í‹°íŒ©íŠ¸ ID ë¦¬ìŠ¤íŠ¸ë¥¼ ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë³€í™˜
            artifacts_str = ", ".join(step.artifact_ids)
            
            print(f"\nâ–¶ Step {step.order_no}: [{timestamp_str}]")
            print(f"  - ë‚´ìš©: {step.description}")
            print(f"  - ì—°ê´€ ì•„í‹°íŒ©íŠ¸: [{artifacts_str}]")
    
    print("\n" + "="*80)


def calculate_optimal_chunk_size(total_artifacts: int, target_chunks: int = 100) -> int:
    """
    ì•„í‹°íŒ©íŠ¸ ì´ ê°œìˆ˜ì— ë”°ë¼ ìµœì ì˜ ì²­í¬ ì‚¬ì´ì¦ˆ ê³„ì‚°
    
    Args:
        total_artifacts: ì „ì²´ ì•„í‹°íŒ©íŠ¸ ê°œìˆ˜
        target_chunks: ëª©í‘œ ì²­í¬ ê°œìˆ˜ (ê¸°ë³¸ 100ê°œ)
        
    Returns:
        ìµœì í™”ëœ ì²­í¬ ì‚¬ì´ì¦ˆ
    """
    # ëª©í‘œ ì²­í¬ ê°œìˆ˜ë¡œ ë‚˜ëˆˆ ê°’
    calculated_size = total_artifacts // target_chunks
    
    # ìµœì†Œ/ìµœëŒ€ ì œí•œ ì„¤ì •
    min_chunk_size = 500   # ë„ˆë¬´ ì‘ìœ¼ë©´ ë¹„íš¨ìœ¨
    max_chunk_size = 2000  # ë„ˆë¬´ í¬ë©´ í† í° ì´ˆê³¼ ìœ„í—˜
    
    # ë²”ìœ„ ë‚´ë¡œ ì¡°ì •
    optimal_size = max(min_chunk_size, min(calculated_size, max_chunk_size))
    
    # ì‹¤ì œ ì²­í¬ ê°œìˆ˜ ê³„ì‚°
    actual_chunks = (total_artifacts + optimal_size - 1) // optimal_size
    
    print(f"ğŸ“Š ì²­í¬ ì‚¬ì´ì¦ˆ ìµœì í™” ê²°ê³¼:")
    print(f"  - ì „ì²´ ì•„í‹°íŒ©íŠ¸: {total_artifacts:,}ê°œ")
    print(f"  - ì²­í¬ ì‚¬ì´ì¦ˆ: {optimal_size}ê°œ")
    print(f"  - ì˜ˆìƒ ì²­í¬ ê°œìˆ˜: {actual_chunks}ê°œ")
    print(f"  - Map ë‹¨ê³„ LLM í˜¸ì¶œ: {actual_chunks}ë²ˆ")
    print(f"  - ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„: ~{actual_chunks * 2.5:.0f}ì´ˆ (ì•½ {actual_chunks * 2.5 / 60:.1f}ë¶„)")
    
    return optimal_size
