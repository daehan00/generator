{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf107739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Generator import Generator\n",
    "from test_backendclient import TestBackendClient\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "# ê¸°ì¡´ í•¸ë“¤ëŸ¬ë“¤ ì œê±° (ì¤‘ë³µ ë°©ì§€)\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "# ë¡œê¹… ì„¤ì • ì¶”ê°€ (ì½˜ì†” + íŒŒì¼)\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),  # ë…¸íŠ¸ë¶ ì¶œë ¥ìœ¼ë¡œ ë¡œê·¸ í‘œì‹œ\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ë£¨íŠ¸ ë¡œê±° ë ˆë²¨ ì„¤ì •\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "generator = Generator()\n",
    "backend_client = TestBackendClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495095cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "from models import ScenarioCreate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from test_backendclient import TestBackendClient\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# -------------------- 1. AgentState ë° ì¤‘ê°„ ê²°ê³¼ ì •ì˜ --------------------\n",
    "class ChunkAnalysisResult(BaseModel):\n",
    "    \"\"\"Map ë‹¨ê³„ì—ì„œ í•„í„°ë§ëœ ì¤‘ìš” ì•„í‹°íŒ©íŠ¸ ë¦¬ìŠ¤íŠ¸\"\"\"\n",
    "    important_artifacts: List[dict] = Field(\n",
    "        description=\"ì •ë³´ìœ ì¶œê³¼ ê´€ë ¨ëœ ì¤‘ìš”í•œ ì•„í‹°íŒ©íŠ¸ë§Œ ì„ ë³„í•œ ë¦¬ìŠ¤íŠ¸. ê° ì•„í‹°íŒ©íŠ¸ëŠ” ì›ë³¸ ë°ì´í„° êµ¬ì¡° ê·¸ëŒ€ë¡œ ìœ ì§€.\"\n",
    "    )\n",
    "    chunk_summary: str = Field(\n",
    "        description=\"ì´ ì²­í¬ì—ì„œ ë°œê²¬ëœ ì˜ì‹¬ í™œë™ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨íˆ ìš”ì•½ (ì˜ˆ: 'ì•…ì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰')\"\n",
    "    )\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    artifact_chunks: List[List[dict]]\n",
    "    current_chunk_index: int\n",
    "    intermediate_results: List[ChunkAnalysisResult]\n",
    "    final_report: ScenarioCreate | None\n",
    "    job_id: str\n",
    "    task_id: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed364da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- ì•„í‹°íŒ©íŠ¸ ë¡œë”© ë° ì²­í¬ ë¶„í•  --------------------\n",
    "def load_artifacts(task_id: str) -> List[dict]:\n",
    "    backend_client = TestBackendClient()\n",
    "    job_id = \"test+job_id\"\n",
    "    return backend_client.load_artifacts(task_id, job_id)\n",
    "\n",
    "def chunk_artifacts(artifacts: List[dict], chunk_size: int = 50) -> List[List[dict]]:\n",
    "    return [artifacts[i:i+chunk_size] for i in range(0, len(artifacts), chunk_size)]\n",
    "\n",
    "def pretty_print_scenario(scenario: ScenarioCreate):\n",
    "    \"\"\"ì‹œë‚˜ë¦¬ì˜¤ ê°ì²´ë¥¼ ë°›ì•„ ê°€ë…ì„± ì¢‹ì€ ë³´ê³ ì„œ í˜•íƒœë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"ğŸ“œ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ ë³´ê³ ì„œ: {scenario.name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n[ ë³´ê³ ì„œ ê°œìš” ]\")\n",
    "    print(f\"  - {scenario.description}\")\n",
    "    \n",
    "    print(\"\\n[ ì‹ë³„ ì •ë³´ ]\")\n",
    "    print(f\"  - Job ID: {scenario.job_id}\")\n",
    "    print(f\"  - Task ID: {scenario.task_id}\")\n",
    "    \n",
    "    print(\"\\n[ ì¬êµ¬ì„±ëœ ê³µê²© ë‹¨ê³„ (Timeline) ]\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if not scenario.steps:\n",
    "        print(\"  (ë¶„ì„ëœ ë‹¨ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.)\")\n",
    "    else:\n",
    "        # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬ (ì´ë¯¸ ì •ë ¬ë˜ì–´ ìˆì§€ë§Œ ì•ˆì „ì¥ì¹˜)\n",
    "        sorted_steps = sorted(scenario.steps, key=lambda s: s.order_no)\n",
    "        \n",
    "        for step in sorted_steps:\n",
    "            # datetime ê°ì²´ë¥¼ ë³´ê¸° ì¢‹ì€ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…\n",
    "            timestamp_str = step.timestamp.strftime('%Y-%m-%d %H:%M:%S') if step.timestamp else \"\"\n",
    "            \n",
    "            # ì•„í‹°íŒ©íŠ¸ ID ë¦¬ìŠ¤íŠ¸ë¥¼ ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "            artifacts_str = \", \".join(step.artifact_ids)\n",
    "            \n",
    "            print(f\"\\nâ–¶ Step {step.order_no}: [{timestamp_str}]\")\n",
    "            print(f\"  - ë‚´ìš©: {step.description}\")\n",
    "            print(f\"  - ì—°ê´€ ì•„í‹°íŒ©íŠ¸: [{artifacts_str}]\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb1f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- 2. Map ë‹¨ê³„: analyze_chunk --------------------\n",
    "llm_small = init_chat_model(\"google_genai:gemini-2.5-flash-lite\", temperature=0)\n",
    "llm_large = init_chat_model(\"google_genai:gemini-2.5-pro\", temperature=0)\n",
    "\n",
    "def analyze_chunk(state: AgentState):\n",
    "    \"\"\"\n",
    "    Small modelì„ ì‚¬ìš©í•˜ì—¬ ì²­í¬ì—ì„œ ì¤‘ìš”í•œ ì•„í‹°íŒ©íŠ¸ë§Œ ë¹ ë¥´ê²Œ í•„í„°ë§.\n",
    "    ì›ë³¸ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì—¬ Reduce ë‹¨ê³„ë¡œ ì „ë‹¬.\n",
    "    \"\"\"\n",
    "    chunk_idx = state[\"current_chunk_index\"]\n",
    "    artifact_chunks = state[\"artifact_chunks\"]\n",
    "    chunk = artifact_chunks[chunk_idx]\n",
    "\n",
    "    # ì•„í‹°íŒ©íŠ¸ë¥¼ ê°„ëµí•˜ê²Œ í¬ë§· (small modelìš© - í† í° ì ˆì•½)\n",
    "    artifacts_summary = []\n",
    "    for idx, artifact in enumerate(chunk):\n",
    "        # í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ (summaryê°€ ì—†ìœ¼ë¯€ë¡œ dataì—ì„œ ì£¼ìš” ì •ë³´ ì¶”ì¶œ)\n",
    "        artifact_type = artifact.get('artifact_type', 'N/A')\n",
    "        collected_at = artifact.get('collected_at', 'N/A')\n",
    "        artifact_id = artifact.get('id', 'N/A')\n",
    "        \n",
    "        # data í•„ë“œì—ì„œ ì¤‘ìš”í•œ ì •ë³´ë§Œ ì„ ë³„\n",
    "        data = artifact.get('data', {})\n",
    "        # dataë¥¼ ê°„ëµí•˜ê²Œ ìš”ì•½ (ë„ˆë¬´ ê¸¸ë©´ í† í° ë‚­ë¹„)\n",
    "        data_summary = {}\n",
    "        for key in ['file_name', 'file_path', 'process_name', 'command_line', \n",
    "                    'url', 'device_name', 'sender', 'receiver', 'subject']:\n",
    "            if key in data and data[key]:\n",
    "                data_summary[key] = str(data[key])[:100]  # ìµœëŒ€ 100ìë¡œ ì œí•œ\n",
    "        \n",
    "        artifacts_summary.append({\n",
    "            \"index\": idx,\n",
    "            \"type\": artifact_type,\n",
    "            \"collected_at\": collected_at,\n",
    "            \"key_data\": data_summary,  # summary ëŒ€ì‹  dataì˜ í•µì‹¬ ì •ë³´ ì‚¬ìš©\n",
    "            \"id\": artifact_id\n",
    "        })\n",
    "    \n",
    "    artifacts_text = json.dumps(artifacts_summary, ensure_ascii=False, indent=2)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"ë‹¹ì‹ ì€ ì—„ê²©í•œ ë³´ì•ˆ ë¶„ì„ê°€ì…ë‹ˆë‹¤. \n",
    "**ì •ë³´ìœ ì¶œ ì‹œë‚˜ë¦¬ì˜¤ì— ì§ì ‘ì ìœ¼ë¡œ ì—°ê´€ëœ ì•„í‹°íŒ©íŠ¸ë§Œ** ì„ íƒí•˜ì„¸ìš”.\n",
    "\n",
    "**í•„í„°ë§ ìš°ì„ ìˆœìœ„ (ì ìˆ˜ì œ):**\n",
    "\n",
    "ğŸ”´ HIGH (ë°˜ë“œì‹œ í¬í•¨):\n",
    "- ì™¸ë¶€ë¡œ íŒŒì¼ ì „ì†¡: messenger_file, email_attachment, usb_device\n",
    "- ë¯¼ê° íŒŒì¼ ì ‘ê·¼: íšŒì‚¬ ê¸°ë°€, ê°œì¸ì •ë³´ íŒŒì¼\n",
    "- ì•…ì„± ë„êµ¬ ì‹¤í–‰: ì•”í˜¸í™”, ì••ì¶•, ì‚­ì œ ë„êµ¬\n",
    "\n",
    "ğŸŸ¡ MEDIUM (ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê²½ìš°ë§Œ):\n",
    "- ë¹„ì •ìƒ ì‹œê°„ëŒ€ ì ‘ê·¼ (ìƒˆë²½ 2-5ì‹œ)\n",
    "- ëŒ€ëŸ‰ íŒŒì¼ ì²˜ë¦¬\n",
    "- ì‹œìŠ¤í…œ ë¡œê·¸ ì‚­ì œ ì‹œë„\n",
    "\n",
    "âšª LOW (ì œì™¸):\n",
    "- ì¼ë°˜ ë¸Œë¼ìš°ì € ë°©ë¬¸\n",
    "- ì •ìƒ ì—…ë¬´ íŒŒì¼ ì ‘ê·¼\n",
    "- ì‹œìŠ¤í…œ ìë™ ìƒì„± ë¡œê·¸\n",
    "- í˜„ì¬ë¡œë¶€í„° 6ê°œì›” ì´ì „ ë°ì´í„°\n",
    "\n",
    "**ì„ íƒ ê¸°ì¤€:**\n",
    "1. HIGH ìš°ì„ ìˆœìœ„ì— í•´ë‹¹í•˜ëŠ”ê°€?\n",
    "2. MEDIUM + ì¶”ê°€ ì˜ì‹¬ ìš”ì†Œ 2ê°œ ì´ìƒ?\n",
    "3. ì•„ë‹ˆë©´ ì œì™¸\n",
    "\n",
    "**ëª©í‘œ: ì²­í¬ë‹¹ ìµœëŒ€ 5-10ê°œë§Œ ì„ íƒ** (ì „ì²´ì˜ 10% ì´í•˜)\n",
    "\n",
    "**ì¶œë ¥:**\n",
    "- important_indices: HIGH + ëª…í™•í•œ MEDIUMë§Œ í¬í•¨\n",
    "- chunk_summary: ì„ íƒí•œ ì´ìœ ë¥¼ ëª…í™•íˆ\"\"\"),\n",
    "        (\"human\", \"ì•„í‹°íŒ©íŠ¸ ëª©ë¡:\\n{artifacts_text}\\n\\nì²­í¬ í¬ê¸°: {chunk_size}ê°œ\\nëª©í‘œ ì„ íƒ ê°œìˆ˜: ìµœëŒ€ {max_select}ê°œ\")\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # í•„í„°ë§ ê²°ê³¼ (index ë¦¬ìŠ¤íŠ¸) ë°›ê¸°\n",
    "    class FilterResult(BaseModel):\n",
    "        important_indices: List[int] = Field(description=\"ì¤‘ìš”í•œ ì•„í‹°íŒ©íŠ¸ì˜ index ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸\")\n",
    "        chunk_summary: str = Field(description=\"ì²­í¬ì˜ ê°„ë‹¨í•œ ìš”ì•½ (í•œ ë¬¸ì¥)\")\n",
    "    \n",
    "    structured_llm = llm_small.with_structured_output(FilterResult)\n",
    "    filter_chain = prompt | structured_llm\n",
    "    \n",
    "    try:\n",
    "        # ì²­í¬ í¬ê¸°ì— ë”°ë¼ ìµœëŒ€ ì„ íƒ ê°œìˆ˜ ë™ì  ê³„ì‚°\n",
    "        chunk_size = len(chunk)\n",
    "        max_select = max(5, chunk_size // 50)  # ìµœì†Œ 5ê°œ, ë˜ëŠ” 2%\n",
    "        \n",
    "        filter_result = filter_chain.invoke({\n",
    "            \"artifacts_text\": artifacts_text,\n",
    "            \"chunk_size\": chunk_size,\n",
    "            \"max_select\": max_select\n",
    "        })\n",
    "        \n",
    "        # LLMì´ Noneì„ ë°˜í™˜í•˜ê±°ë‚˜ í•„ìˆ˜ í•„ë“œê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "        if filter_result is None or not hasattr(filter_result, 'important_indices'):\n",
    "            print(f\"âš ï¸  ì²­í¬ {chunk_idx + 1}: LLM ì‘ë‹µ ì˜¤ë¥˜ - ë¹ˆ ê²°ê³¼ë¡œ ì²˜ë¦¬\")\n",
    "            important_artifacts = []\n",
    "            chunk_summary = \"ë¶„ì„ ì‹¤íŒ¨\"\n",
    "        else:\n",
    "            # ì„ íƒëœ ì•„í‹°íŒ©íŠ¸ì˜ ì›ë³¸ ë°ì´í„° ì¶”ì¶œ\n",
    "            important_artifacts = [\n",
    "                chunk[idx] for idx in filter_result.important_indices \n",
    "                if 0 <= idx < len(chunk)\n",
    "            ]\n",
    "            \n",
    "            # ìœ ì˜ë¯¸í•œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "            if not important_artifacts:\n",
    "                print(f\"âš ï¸  ì²­í¬ {chunk_idx + 1}: ìœ ì˜ë¯¸í•œ ë°ì´í„° ì—†ìŒ (ìŠ¤í‚µ)\")\n",
    "                chunk_summary = \"ê´€ë ¨ì„± ì—†ëŠ” ë°ì´í„°\"\n",
    "            else:\n",
    "                chunk_summary = filter_result.chunk_summary\n",
    "                print(f\"âœ… ì²­í¬ {chunk_idx + 1}: {len(important_artifacts)}ê°œ ì¤‘ìš” ì•„í‹°íŒ©íŠ¸ ë°œê²¬\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì²­í¬ {chunk_idx + 1}: ë¶„ì„ ì¤‘ ì—ëŸ¬ ë°œìƒ - {str(e)}\")\n",
    "        important_artifacts = []\n",
    "        chunk_summary = f\"ë¶„ì„ ì˜¤ë¥˜: {type(e).__name__}\"\n",
    "    \n",
    "    chunk_result = ChunkAnalysisResult(\n",
    "        important_artifacts=important_artifacts,\n",
    "        chunk_summary=chunk_summary\n",
    "    )\n",
    "\n",
    "    # ì¤‘ê°„ ê²°ê³¼ ëˆ„ì \n",
    "    new_results = state.get(\"intermediate_results\", []) + [chunk_result]\n",
    "    return {\n",
    "        \"current_chunk_index\": chunk_idx + 1,\n",
    "        \"intermediate_results\": new_results\n",
    "    }\n",
    "\n",
    "# -------------------- 3. Reduce ë‹¨ê³„: synthesize_report --------------------\n",
    "def synthesize_report(state: AgentState):\n",
    "    \"\"\"\n",
    "    í•„í„°ë§ëœ ì¤‘ìš” ì•„í‹°íŒ©íŠ¸ë“¤ì„ Large modelë¡œ ì •ë°€ ë¶„ì„í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œ ìƒì„±.\n",
    "    \"\"\"\n",
    "    intermediate_results = state[\"intermediate_results\"]\n",
    "    job_id = state[\"job_id\"]\n",
    "    task_id = state[\"task_id\"]\n",
    "\n",
    "    # ëª¨ë“  ì¤‘ìš” ì•„í‹°íŒ©íŠ¸ë¥¼ í•˜ë‚˜ë¡œ ëª¨ìœ¼ê¸°\n",
    "    all_important_artifacts = []\n",
    "    chunk_summaries = []\n",
    "    \n",
    "    for idx, result in enumerate(intermediate_results, 1):\n",
    "        # ìœ ì˜ë¯¸í•œ ë°ì´í„°ê°€ ìˆëŠ” ì²­í¬ë§Œ í¬í•¨\n",
    "        if result.important_artifacts:\n",
    "            chunk_summaries.append(f\"ì²­í¬ {idx}: {result.chunk_summary}\")\n",
    "            all_important_artifacts.extend(result.important_artifacts)\n",
    "    \n",
    "    # ì „ì²´ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "    if not all_important_artifacts:\n",
    "        print(\"\\nâŒ ì „ì²´ ì²­í¬ì—ì„œ ìœ ì˜ë¯¸í•œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        # ë¹ˆ ë³´ê³ ì„œ ë°˜í™˜\n",
    "        return {\n",
    "            \"final_report\": ScenarioCreate(\n",
    "                name=\"ë¶„ì„ ê²°ê³¼ ì—†ìŒ\",\n",
    "                description=\"ì œê³µëœ ì•„í‹°íŒ©íŠ¸ì—ì„œ ì •ë³´ìœ ì¶œê³¼ ê´€ë ¨ëœ ìœ ì˜ë¯¸í•œ ë°ì´í„°ë¥¼ ë°œê²¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\",\n",
    "                job_id=job_id,\n",
    "                task_id=task_id,\n",
    "                steps=[],\n",
    "                report_detail_id=None\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    print(f\"\\nğŸ“Š ì´ {len(all_important_artifacts)}ê°œì˜ ì¤‘ìš” ì•„í‹°íŒ©íŠ¸ ë°œê²¬\")\n",
    "    \n",
    "    # ì „ì²´ ìš”ì•½\n",
    "    chunks_overview = \"\\n\".join(chunk_summaries)\n",
    "    \n",
    "    # ì•„í‹°íŒ©íŠ¸ë¥¼ ìƒì„¸í•˜ê²Œ í¬ë§· (Large modelì€ í† í° ì—¬ìœ  ìˆìŒ)\n",
    "    formatted_artifacts = []\n",
    "    for artifact in all_important_artifacts:\n",
    "        artifact_type = artifact.get('artifact_type', 'N/A')\n",
    "        artifact_id = artifact.get('id', 'N/A')\n",
    "        collected_at = artifact.get('collected_at', 'N/A')\n",
    "        \n",
    "        # data í•„ë“œ ì „ì²´ í¬í•¨ (Large modelì€ ìƒì„¸ ë¶„ì„ ê°€ëŠ¥)\n",
    "        data_details = json.dumps(artifact.get('data', {}), ensure_ascii=False, indent=2)\n",
    "        \n",
    "        formatted_string = (\n",
    "            f\"[ID: {artifact_id}]\\n\"\n",
    "            f\"  ìœ í˜•: {artifact_type}\\n\"\n",
    "            f\"  ë°ì´í„°:\\n{data_details}\"\n",
    "        )\n",
    "        formatted_artifacts.append(formatted_string)\n",
    "    \n",
    "    artifacts_detail = \"\\n\\n\".join(formatted_artifacts)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"\"\"ë‹¹ì‹ ì€ ë‚´ë¶€ì •ë³´ ìœ ì¶œ ì‚¬ê³  ë¶„ì„ ë° ë³´ê³ ì„œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "\n",
    "**í˜„ì¬ ìƒí™©:**\n",
    "ì—¬ëŸ¬ ì²­í¬ì—ì„œ í•„í„°ë§ëœ ì•„í‹°íŒ©íŠ¸ë“¤ì´ ì œê³µë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "ê° ì²­í¬ì˜ ê°„ë‹¨í•œ ìš”ì•½:\n",
    "{chunks_overview}\n",
    "\n",
    "**ë‹¹ì‹ ì˜ ì„ë¬´:**\n",
    "ì œê³µëœ ëª¨ë“  ì•„í‹°íŒ©íŠ¸ì˜ ì›ë³¸ ë°ì´í„°ë¥¼ ë©´ë°€íˆ ë¶„ì„í•˜ì—¬,\n",
    "**ë…¼ë¦¬ì ì¸ í–‰ìœ„ ë‹¨ìœ„**ë¡œ ì¬êµ¬ì„±ëœ ì •ë³´ìœ ì¶œ ì‹œë‚˜ë¦¬ì˜¤ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "**ë³´ê³ ì„œ ì‘ì„± ê·œì¹™:**\n",
    "\n",
    "1. **Step ìƒì„± - í–‰ìœ„ ì¤‘ì‹¬:**\n",
    "   - ê° Step = í•˜ë‚˜ì˜ ì˜ë¯¸ ìˆëŠ” ê³µê²© í–‰ìœ„\n",
    "   - ìœ ì‚¬í•œ í–‰ìœ„ëŠ” í•˜ë‚˜ë¡œ í†µí•©\n",
    "   - ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬\n",
    "\n",
    "2. **artifact_ids:**\n",
    "   - ê° Stepì— ê´€ë ¨ëœ ëª¨ë“  ì•„í‹°íŒ©íŠ¸ ID í¬í•¨\n",
    "   - IDëŠ” [ID: xxx] í˜•ì‹ìœ¼ë¡œ í‘œì‹œë¨\n",
    "\n",
    "3. **timestamp:**\n",
    "   - \"ìˆ˜ì§‘ ì‹œê°\" í•„ë“œì—ì„œ ì¶”ì¶œ\n",
    "   - ISO 8601 í˜•ì‹ (ì˜ˆ: \"2025-09-23T04:55:59\")\n",
    "   - ì—¬ëŸ¬ ì•„í‹°íŒ©íŠ¸ë¥¼ í†µí•©í•œ ê²½ìš° ê°€ì¥ ì´ë¥¸ ì‹œê°„ ì‚¬ìš©\n",
    "   - ì•Œ ìˆ˜ ì—†ìœ¼ë©´ null\n",
    "\n",
    "4. **description:**\n",
    "   - ì¤‘í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì‘ì„±\n",
    "   - \"ëˆ„ê°€/ë¬´ì—‡ì„/ì™œ/ì–´ë–»ê²Œ\" ëª…í™•íˆ ì„¤ëª…\n",
    "\n",
    "5. **ì „ì²´ ë³´ê³ ì„œ:**\n",
    "   - name: ê°„ê²°í•œ ì œëª© (15ì ë‚´ì™¸)\n",
    "   - description: ì „ì²´ íë¦„ ìš”ì•½ (2-3ë¬¸ì¥)\n",
    "\n",
    "**ë©”íƒ€ë°ì´í„°:**\n",
    "- job_id: {job_id}\n",
    "- task_id: {task_id}\n",
    "\n",
    "**ì¶œë ¥ í˜•ì‹:**\n",
    "{{{{\n",
    "  \"name\": \"ì•…ì„± íŒŒì¼ ìœ í¬ ë° ì •ë³´ ìœ ì¶œ\",\n",
    "  \"description\": \"ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ ìš”ì•½...\",\n",
    "  \"job_id\": \"{job_id}\",\n",
    "  \"task_id\": \"{task_id}\",\n",
    "  \"steps\": [\n",
    "    {{{{\n",
    "      \"order_no\": 1,\n",
    "      \"timestamp\": \"2025-09-23T04:55:59\",\n",
    "      \"description\": \"í–‰ìœ„ ì„¤ëª…...\",\n",
    "      \"artifact_ids\": [\"id1\", \"id2\", ...]\n",
    "    }}}}\n",
    "  ]\n",
    "}}}}\"\"\"),\n",
    "        (\"human\", \"í•„í„°ë§ëœ ì¤‘ìš” ì•„í‹°íŒ©íŠ¸:\\n\\n{artifacts_detail}\")\n",
    "    ])\n",
    "    \n",
    "    # Large model ì‚¬ìš© (ì •í™•í•œ ë¶„ì„)\n",
    "    load_dotenv(\"../.env\")\n",
    "    structured_llm = llm_large.with_structured_output(ScenarioCreate)\n",
    "    synth_chain = prompt | structured_llm\n",
    "\n",
    "    final_report = synth_chain.invoke({\"artifacts_detail\": artifacts_detail})\n",
    "\n",
    "    return {\"final_report\": final_report}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c192018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- 4. ì œì–´ íë¦„: router --------------------\n",
    "def router(state: AgentState):\n",
    "    if state[\"current_chunk_index\"] < len(state[\"artifact_chunks\"]):\n",
    "        return \"analyze_chunk\"\n",
    "    else:\n",
    "        return \"synthesize_report\"\n",
    "\n",
    "# -------------------- 5. ê·¸ë˜í”„ êµ¬ì„± ë° ì‹¤í–‰ --------------------\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# ê·¸ë˜í”„ ì •ì˜\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"analyze_chunk\", analyze_chunk)\n",
    "workflow.add_node(\"synthesize_report\", synthesize_report)\n",
    "workflow.set_entry_point(\"analyze_chunk\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"analyze_chunk\",\n",
    "    router,\n",
    "    {\n",
    "        \"analyze_chunk\": \"analyze_chunk\",\n",
    "        \"synthesize_report\": \"synthesize_report\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"synthesize_report\", END)\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c89dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b630ba4",
   "metadata": {},
   "source": [
    "## ì²­í¬ ì‚¬ì´ì¦ˆ ìµœì í™” ê°€ì´ë“œ (10ë§Œ ê°œ ì•„í‹°íŒ©íŠ¸ ê¸°ì¤€)\n",
    "\n",
    "### ë¬¸ì œì :\n",
    "- ì²­í¬ 100ê°œ â†’ 1000ë²ˆì˜ LLM í˜¸ì¶œ â†’ ë„ˆë¬´ ëŠë¦¬ê³  ë¹„ìš© ë§ì´ ë°œìƒ\n",
    "- Map ë‹¨ê³„ì—ì„œ Small Modelì´ì§€ë§Œ í˜¸ì¶œ íšŸìˆ˜ê°€ ë§ìœ¼ë©´ ë¹„íš¨ìœ¨ì \n",
    "\n",
    "### ê¶Œì¥ ì²­í¬ ì‚¬ì´ì¦ˆ:\n",
    "\n",
    "**1. ëª¨ë¸ë³„ í† í° ì œí•œ ê³ ë ¤**\n",
    "- Gemini 2.5 Flash Lite: ì…ë ¥ í† í° ~100ë§Œ (ë§¤ìš° í¼)\n",
    "- ì•„í‹°íŒ©íŠ¸ 1ê°œë‹¹ ìš”ì•½ í˜•íƒœ ì•½ 200-500 í† í°\n",
    "- **ê¶Œì¥: 500-1000ê°œ/ì²­í¬** â†’ ì´ 100-200ë²ˆ í˜¸ì¶œ\n",
    "\n",
    "**2. ì‹¤ìš©ì ì¸ ì²­í¬ ì‚¬ì´ì¦ˆ ê²°ì •:**\n",
    "```python\n",
    "total_artifacts = 100_000\n",
    "\n",
    "# ì˜µì…˜ A: ë¹ ë¥¸ ì²˜ë¦¬ (ì¶”ì²œ)\n",
    "chunk_size = 1000  # â†’ 100ë²ˆ Map í˜¸ì¶œ\n",
    "# ì¥ì : ë¹ ë¦„, í•„í„°ë§ íš¨ê³¼ ì¢‹ìŒ\n",
    "# ë‹¨ì : ì²­í¬ë‹¹ í† í° ì‚¬ìš©ëŸ‰ ì¦ê°€\n",
    "\n",
    "# ì˜µì…˜ B: ê· í˜•ì¡íŒ ì²˜ë¦¬\n",
    "chunk_size = 500   # â†’ 200ë²ˆ Map í˜¸ì¶œ\n",
    "# ì¥ì : ì•ˆì •ì , í† í° ê´€ë¦¬ ìš©ì´\n",
    "# ë‹¨ì : Map í˜¸ì¶œ íšŸìˆ˜ ì¦ê°€\n",
    "\n",
    "# ì˜µì…˜ C: ë§¤ìš° í° ì²­í¬ (ëŒ€ë‹´í•œ ì„ íƒ)\n",
    "chunk_size = 2000  # â†’ 50ë²ˆ Map í˜¸ì¶œ\n",
    "# ì¥ì : ìµœì†Œ í˜¸ì¶œ íšŸìˆ˜\n",
    "# ë‹¨ì : í† í° í•œê³„ ê·¼ì ‘ ì‹œ ì—ëŸ¬ ê°€ëŠ¥\n",
    "```\n",
    "\n",
    "**3. ë¹„ìš© ë° ì‹œê°„ ë¹„êµ (10ë§Œ ê°œ ê¸°ì¤€):**\n",
    "| ì²­í¬ ì‚¬ì´ì¦ˆ | Map í˜¸ì¶œ | ì˜ˆìƒ ì‹œê°„ | API ë¹„ìš© (ëŒ€ëµ) |\n",
    "|----------|---------|---------|--------------|\n",
    "| 100      | 1000ë²ˆ  | ~30ë¶„   | $$$$         |\n",
    "| 500      | 200ë²ˆ   | ~7ë¶„    | $$           |\n",
    "| 1000     | 100ë²ˆ   | ~4ë¶„    | $            |\n",
    "| 2000     | 50ë²ˆ    | ~2ë¶„    | $ (ìµœì €)      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c4672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë™ì  ì²­í¬ ì‚¬ì´ì¦ˆ ê²°ì • í•¨ìˆ˜\n",
    "def calculate_optimal_chunk_size(total_artifacts: int, target_chunks: int = 100) -> int:\n",
    "    \"\"\"\n",
    "    ì•„í‹°íŒ©íŠ¸ ì´ ê°œìˆ˜ì— ë”°ë¼ ìµœì ì˜ ì²­í¬ ì‚¬ì´ì¦ˆ ê³„ì‚°\n",
    "    \n",
    "    Args:\n",
    "        total_artifacts: ì „ì²´ ì•„í‹°íŒ©íŠ¸ ê°œìˆ˜\n",
    "        target_chunks: ëª©í‘œ ì²­í¬ ê°œìˆ˜ (ê¸°ë³¸ 100ê°œ)\n",
    "        \n",
    "    Returns:\n",
    "        ìµœì í™”ëœ ì²­í¬ ì‚¬ì´ì¦ˆ\n",
    "    \"\"\"\n",
    "    # ëª©í‘œ ì²­í¬ ê°œìˆ˜ë¡œ ë‚˜ëˆˆ ê°’\n",
    "    calculated_size = total_artifacts // target_chunks\n",
    "    \n",
    "    # ìµœì†Œ/ìµœëŒ€ ì œí•œ ì„¤ì •\n",
    "    min_chunk_size = 500   # ë„ˆë¬´ ì‘ìœ¼ë©´ ë¹„íš¨ìœ¨\n",
    "    max_chunk_size = 2000  # ë„ˆë¬´ í¬ë©´ í† í° ì´ˆê³¼ ìœ„í—˜\n",
    "    \n",
    "    # ë²”ìœ„ ë‚´ë¡œ ì¡°ì •\n",
    "    optimal_size = max(min_chunk_size, min(calculated_size, max_chunk_size))\n",
    "    \n",
    "    # ì‹¤ì œ ì²­í¬ ê°œìˆ˜ ê³„ì‚°\n",
    "    actual_chunks = (total_artifacts + optimal_size - 1) // optimal_size\n",
    "    \n",
    "    print(f\"ğŸ“Š ì²­í¬ ì‚¬ì´ì¦ˆ ìµœì í™” ê²°ê³¼:\")\n",
    "    print(f\"  - ì „ì²´ ì•„í‹°íŒ©íŠ¸: {total_artifacts:,}ê°œ\")\n",
    "    print(f\"  - ì²­í¬ ì‚¬ì´ì¦ˆ: {optimal_size}ê°œ\")\n",
    "    print(f\"  - ì˜ˆìƒ ì²­í¬ ê°œìˆ˜: {actual_chunks}ê°œ\")\n",
    "    print(f\"  - Map ë‹¨ê³„ LLM í˜¸ì¶œ: {actual_chunks}ë²ˆ\")\n",
    "    print(f\"  - ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„: ~{actual_chunks * 2.5:.0f}ì´ˆ (ì•½ {actual_chunks * 2.5 / 60:.1f}ë¶„)\")\n",
    "    \n",
    "    return optimal_size\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "total = len(artifacts) if 'artifacts' in locals() else 100000\n",
    "optimal_chunk_size = calculate_optimal_chunk_size(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da54586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì í™”ëœ ì‹¤í–‰ ë²„ì „\n",
    "from test_artifact_maker import make_test_artifacts\n",
    "\n",
    "task_id = \"session-20251002-052932-151e52e9\"\n",
    "job_id = \"test+job_id\"\n",
    "\n",
    "print(\"ğŸ”„ ì•„í‹°íŒ©íŠ¸ ìƒì„± ì¤‘...\")\n",
    "artifacts = make_test_artifacts(task_id, limit=1000)\n",
    "# artifacts = make_test_artifacts(task_id)\n",
    "\n",
    "print(f\"\\nâœ… ì´ {len(artifacts):,}ê°œ ì•„í‹°íŒ©íŠ¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "\n",
    "# ë™ì ìœ¼ë¡œ ìµœì  ì²­í¬ ì‚¬ì´ì¦ˆ ê³„ì‚°\n",
    "optimal_chunk_size = calculate_optimal_chunk_size(len(artifacts), target_chunks=100)\n",
    "\n",
    "# ì²­í¬ ë¶„í• \n",
    "artifact_chunks = chunk_artifacts(artifacts, chunk_size=optimal_chunk_size)\n",
    "\n",
    "print(f\"\\nğŸš€ ë¶„ì„ ì‹œì‘...\")\n",
    "initial_state = {\n",
    "    \"artifact_chunks\": artifact_chunks,\n",
    "    \"current_chunk_index\": 0,\n",
    "    \"intermediate_results\": [],\n",
    "    \"final_report\": None,\n",
    "    \"job_id\": job_id,\n",
    "    \"task_id\": task_id\n",
    "}\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "recursion_limit = len(artifact_chunks) + 10\n",
    "final_state = app.invoke(initial_state, config={\"recursion_limit\": recursion_limit})\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâ±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {elapsed_time:.1f}ì´ˆ ({elapsed_time/60:.1f}ë¶„)\")\n",
    "print(f\"ğŸ“Š ì²˜ë¦¬ëœ ì²­í¬: {len(artifact_chunks)}ê°œ\")\n",
    "print(f\"ğŸ” í•„í„°ë§ëœ ì¤‘ìš” ì•„í‹°íŒ©íŠ¸: {sum(len(r.important_artifacts) for r in final_state['intermediate_results'])}ê°œ\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"--- ğŸ“‹ ìµœì¢… ë¶„ì„ ê²°ê³¼ ---\")\n",
    "print(\"=\"*80)\n",
    "pretty_print_scenario(final_state[\"final_report\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
