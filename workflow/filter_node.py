from workflow.classes import AgentState, ChunkAnalysisResult, FilterResult
from workflow.prompts import FILTER_PROMPT
from langchain_core.prompts import ChatPromptTemplate
from common.utils import llm_small, chunk_artifacts

from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, cast
import json
import time


def recursive_filter_node(state: AgentState):
    """
    LangGraph ì¡°ê±´ë¶€ ì—£ì§€ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ëŠ” ì¬ê·€ í•„í„°ë§ ë…¸ë“œ.
    ëª©í‘œ ê°œìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ í•„í„°ë§ ê°•ë„ë¥¼ ë†’ì—¬ê°€ë©° ë°˜ë³µ.
    
    Args:
        state: AgentState
    
    Returns:
        ì—…ë°ì´íŠ¸ëœ state
    """
    
    # ì´ˆê¸°í™” ë˜ëŠ” ê¸°ì¡´ ìƒíƒœ ì½ê¸°
    filter_iteration = state.get("filter_iteration", 0)
    target_count = state.get("target_artifact_count", 10000)  # ğŸ†• V2: 10,000ê°œë¡œ ìƒí–¥
    
    # í•„í„°ë§ ê°•ë„ ë ˆë²¨ ì •ì˜ (V2: 3ë‹¨ê³„ë§Œ ì‚¬ìš©)
    strictness_levels = [
        ("very_strict", 0.015),   # 1.5%
        ("strict", 0.025),         # 2.5%
        ("moderate", 0.06),        # 6%
    ]
    
    # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì²´í¬ (V2: 3íšŒë¡œ ì œí•œ)
    max_iterations = 3  # ğŸ†• V2: 5íšŒ â†’ 3íšŒ
    if filter_iteration >= max_iterations:
        print(f"âš ï¸  ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜({max_iterations}) ë„ë‹¬")
        return state
    
    # í˜„ì¬ ë°˜ë³µì˜ ê°•ë„ì™€ ë¹„ìœ¨
    strictness, target_ratio = strictness_levels[filter_iteration]
    
    # ğŸ”„ 1ì°¨ ë°˜ë³µ: ì›ë³¸ artifact_chunks ì‚¬ìš©
    # ğŸ”„ 2ì°¨ ì´ìƒ: ì´ì „ í•„í„°ë§ ê²°ê³¼(intermediate_results) ì‚¬ìš©
    if filter_iteration == 0:
        # ì²« ë²ˆì§¸ ë°˜ë³µ: ì›ë³¸ ì‚¬ìš©
        artifact_chunks_input = state.get("artifact_chunks", [])
        all_artifacts = []
        for chunk in artifact_chunks_input:
            all_artifacts.extend(chunk)
    else:
        # ë‘ ë²ˆì§¸ ì´ìƒ: ì´ì „ í•„í„°ë§ ê²°ê³¼ ì‚¬ìš©
        intermediate_results = state.get("intermediate_results", [])
        all_artifacts = []
        for result in intermediate_results:
            all_artifacts.extend(result.important_artifacts)
    
    total_artifacts = len(all_artifacts)
    
    print(f"\n{'='*70}")
    print(f"ğŸ”„ í•„í„°ë§ ë°˜ë³µ {filter_iteration + 1}/{max_iterations}: {strictness.upper()}")
    print(f"{'='*70}")
    if filter_iteration == 0:
        print(f"  - ì…ë ¥: ì›ë³¸ ì•„í‹°íŒ©íŠ¸")
    else:
        print(f"  - ì…ë ¥: {filter_iteration}ì°¨ í•„í„°ë§ ê²°ê³¼")
    print(f"  - í˜„ì¬ ì•„í‹°íŒ©íŠ¸: {total_artifacts:,}ê°œ")
    print(f"  - í•„í„°ë§ ê°•ë„: {strictness}")
    print(f"  - ëª©í‘œ ë¹„ìœ¨: {target_ratio*100:.1f}%")
    print(f"  - ëª©í‘œ ê°œìˆ˜: {target_count:,}ê°œ")
    
    # ì²­í¬ ë¶„í•  ë° ë°°ì¹˜ ì„¤ì • (TPM ìµœì í™” - Tier 1)
    chunk_size = 300  # ì•„í‹°íŒ©íŠ¸ 300ê°œ/ì²­í¬
    
    # TPM ê¸°ë°˜ ë°°ì¹˜ ì„¤ì • (Tier 1 ìœ ë£Œ)
    # - gemini-2.5-flash-lite Tier 1: ~4M TPM, ~1,000 RPM
    # - ì˜ˆìƒ í† í°/ìš”ì²­: ~10K tokens (300ê°œ ì•„í‹°íŒ©íŠ¸ ìš”ì•½)
    # - ì•ˆì „ ë§ˆì§„: 50% â†’ ì‹¤ì œ ëª©í‘œ ~2M TPM, ~500 RPM
    # - ë°°ì¹˜ë‹¹ ìš”ì²­: 20ê°œ (ë™ì‹œ ì²˜ë¦¬)
    # - ë°°ì¹˜ ê°„ê²©: 30ì´ˆ
    batch_size = 20  # ë°°ì¹˜ë‹¹ ì²­í¬ ìˆ˜
    max_workers_per_batch = 5  # ë°°ì¹˜ ë‚´ ë™ì‹œ ì‹¤í–‰
    batch_delay = 30.0  # ë°°ì¹˜ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    
    artifact_chunks = chunk_artifacts(all_artifacts, chunk_size=chunk_size)
    total_chunks = len(artifact_chunks)
    print(f"  - ì´ ì²­í¬ ìˆ˜: {total_chunks}ê°œ")
    
    # ë°°ì¹˜ ë‹¨ìœ„ í•„í„°ë§
    all_results = []
    num_batches = (total_chunks + batch_size - 1) // batch_size
    
    print(f"\nğŸ“¦ ì´ {num_batches}ê°œ ë°°ì¹˜ë¡œ ì²˜ë¦¬\n")
    
    for batch_num in range(num_batches):
        start_idx = batch_num * batch_size
        end_idx = min(start_idx + batch_size, total_chunks)
        batch_chunks = artifact_chunks[start_idx:end_idx]
        
        print(f"ğŸ“¦ ë°°ì¹˜ {batch_num + 1}/{num_batches} (ì²­í¬ {start_idx + 1}-{end_idx})...")
        
        batch_results = []
        
        with ThreadPoolExecutor(max_workers=max_workers_per_batch) as executor:
            future_to_idx = {}
            for offset, chunk in enumerate(batch_chunks):
                chunk_idx = start_idx + offset
                future = executor.submit(
                    analyze_chunk_simple,
                    chunk,
                    chunk_idx,
                    target_ratio
                )
                future_to_idx[future] = chunk_idx
            
            for future in as_completed(future_to_idx):
                chunk_idx = future_to_idx[future]
                try:
                    chunk_result = future.result(timeout=120)
                    batch_results.append((chunk_idx, chunk_result))
                except TimeoutError:
                    print(f"â±ï¸  ì²­í¬ {chunk_idx + 1}: íƒ€ì„ì•„ì›ƒ")
                    batch_results.append((chunk_idx, ChunkAnalysisResult(
                        important_artifacts=[],
                        chunk_summary="íƒ€ì„ì•„ì›ƒ"
                    )))
                except Exception as e:
                    print(f"âŒ ì²­í¬ {chunk_idx + 1}: {type(e).__name__}")
                    batch_results.append((chunk_idx, ChunkAnalysisResult(
                        important_artifacts=[],
                        chunk_summary=f"ì˜¤ë¥˜: {type(e).__name__}"
                    )))
        
        batch_results.sort(key=lambda x: x[0])
        all_results.extend([r for _, r in batch_results])
        
        batch_artifacts = sum(len(r.important_artifacts) for _, r in batch_results)
        print(f"  âœ… ë°°ì¹˜ {batch_num + 1} ì™„ë£Œ: {batch_artifacts}ê°œ ë°œê²¬")
        
        if batch_num < num_batches - 1:
            time.sleep(batch_delay)
    
    # ê²°ê³¼ ë¶„ì„
    total_filtered = sum(len(r.important_artifacts) for r in all_results)
    
    print(f"\nğŸ“Š ë°˜ë³µ {filter_iteration + 1} ê²°ê³¼:")
    print(f"  - í•„í„°ë§ëœ ê°œìˆ˜: {total_filtered:,}ê°œ")
    print(f"  - ëª©í‘œ ëŒ€ë¹„: {total_filtered}/{target_count} ({total_filtered/target_count*100:.1f}%)")
    
    # ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•œ ìƒíƒœ ì—…ë°ì´íŠ¸
    next_iteration = filter_iteration + 1
    next_strictness_idx = min(next_iteration, len(strictness_levels) - 1)
    next_strictness = strictness_levels[next_strictness_idx][0]
    
    return {
        "intermediate_results": all_results,
        "filter_iteration": next_iteration,
        "current_strictness": next_strictness,
        "target_artifact_count": target_count,
        "job_id": state.get("job_id"),
        "task_id": state.get("task_id"),
        "artifact_chunks": state.get("artifact_chunks"),  # ì›ë³¸ ìœ ì§€
    }


def should_continue_filtering(state: AgentState) -> str:
    """
    í•„í„°ë§ì„ ê³„ì†í• ì§€ ê²°ì •í•˜ëŠ” ì¡°ê±´ í•¨ìˆ˜.
    (V2: ëª©í‘œ 10,000ê°œ, ìµœëŒ€ 3íšŒ ë°˜ë³µ)
    
    Args:
        state: AgentState
    
    Returns:
        "continue": í•„í„°ë§ ë°˜ë³µ
        "synthesize": ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰
    """
    target_count = state.get("target_artifact_count", 10000)  # ğŸ†• V2: 10,000ê°œ
    filter_iteration = state.get("filter_iteration", 0)
    intermediate_results = state.get("intermediate_results", [])
    
    total_filtered = sum(len(r.important_artifacts) for r in intermediate_results)
    
    max_iterations = 3  # ğŸ†• V2: 3íšŒ
    
    # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
    if total_filtered <= target_count:
        print(f"\nâœ… ëª©í‘œ ë‹¬ì„±! ({total_filtered:,}ê°œ >= {target_count:,.0f}ê°œ)")
        print(f"{'='*70}\n")
        return "synthesize"
    
    # ìµœëŒ€ ë°˜ë³µ ë„ë‹¬
    if filter_iteration >= max_iterations:
        print(f"\nâš ï¸  ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬. í˜„ì¬ ê²°ê³¼({total_filtered:,}ê°œ)ë¡œ ì§„í–‰")
        print(f"{'='*70}\n")
        return "synthesize"
    
    # ê³„ì† í•„í„°ë§
    print(f"\nğŸ”„ ëª©í‘œ ë¯¸ë‹¬. í•„í„°ë§ ê°•ë„ë¥¼ ë†’ì—¬ ì¬ì‹œë„...\n")
    return "continue"


def analyze_chunk_simple(
    chunk: List[dict], 
    chunk_idx: int, 
    target_ratio: float = 0.05,
    max_retries: int = 1
):
    """
    ì ìˆ˜ ì—†ì´ ë‹¨ìˆœ í•„í„°ë§ë§Œ ìˆ˜í–‰ (ë¹ ë¥´ê³  ì•ˆì •ì ).
    LLM ì‘ë‹µ ì˜¤ë¥˜ ì‹œ ìë™ ì¬ì‹œë„.
    
    Args:
        chunk: ë¶„ì„í•  ì•„í‹°íŒ©íŠ¸ ë¦¬ìŠ¤íŠ¸
        chunk_idx: ì²­í¬ ì¸ë±ìŠ¤
        strictness: í•„í„°ë§ ê°•ë„ (í˜„ì¬ëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
        target_ratio: ëª©í‘œ ì„ íƒ ë¹„ìœ¨ (0.05 = 5%)
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ 2íšŒ)
    
    Returns:
        ChunkAnalysisResult (ì ìˆ˜ ì—†ìŒ)
    """
    # ì•„í‹°íŒ©íŠ¸ë¥¼ ê°„ëµí•˜ê²Œ í¬ë§·
    artifacts_summary = []
    for idx, artifact in enumerate(chunk):
        artifact_type = artifact.get('artifact_type', 'N/A')
        artifact_id = artifact.get('id', 'N/A')
        
        data = artifact.get('data', {})
        data_summary = {}
        for key, value in data.items():
            if not value:
                continue
            if hasattr(value, 'isoformat'):
                data_summary[key] = value.isoformat()
            else:
                data_summary[key] = str(value)
        
        artifacts_summary.append({
            "index": idx,
            "type": artifact_type,
            "key_data": data_summary,
            "id": artifact_id
        })
    
    artifacts_text = json.dumps(artifacts_summary, ensure_ascii=False, indent=2)
    
    # ğŸ†• í†µí•©ëœ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ (í•„í„°ë§ ê°•ë„ì™€ ë¬´ê´€í•˜ê²Œ ì¼ê´€ëœ ê¸°ì¤€ ì ìš©)
    target_count = max(5, int(len(chunk) * target_ratio))
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", FILTER_PROMPT),
        ("human", "ì•„í‹°íŒ©íŠ¸ ëª©ë¡:\n{artifacts_text}\n\nì²­í¬ í¬ê¸°: {chunk_size}ê°œ")
    ])
    
    structured_llm = llm_small.with_structured_output(FilterResult)
    filter_chain = prompt | structured_llm
    
    chunk_size = len(chunk)
    
    # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ max_retriesíšŒ ì‹œë„)
    for attempt in range(max_retries + 1):
        try:
            filter_result = filter_chain.invoke({
                "artifacts_text": artifacts_text,
                "chunk_size": chunk_size,
                "target_ratio_percent": target_ratio * 100,
                "target_count": target_count
            })
            
            # ì‘ë‹µ ê²€ì¦
            if filter_result is None or not hasattr(filter_result, 'important_indices'):
                if attempt < max_retries:
                    print(f"âš ï¸  ì²­í¬ {chunk_idx + 1}: LLM ì‘ë‹µ ì˜¤ë¥˜ (ì¬ì‹œë„ {attempt + 1}/{max_retries})...")
                    time.sleep(1)
                    continue
                else:
                    print(f"âŒ ì²­í¬ {chunk_idx + 1}: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬ - ë¹ˆ ê²°ê³¼ ë°˜í™˜")
                    return ChunkAnalysisResult(
                        important_artifacts=[],
                        chunk_summary="ë¶„ì„ ì‹¤íŒ¨ (ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼)"
                    )
                
            filter_result = cast(FilterResult, filter_result)

            # ì„ íƒëœ ì•„í‹°íŒ©íŠ¸ì˜ ì›ë³¸ ë°ì´í„° ì¶”ì¶œ
            important_artifacts = [
                chunk[idx] for idx in filter_result.important_indices 
                if 0 <= idx < len(chunk)
            ]
            
            if not important_artifacts:
                print(f"âœ… ì²­í¬ {chunk_idx + 1}: ìœ ì˜ë¯¸í•œ ë°ì´í„° ì—†ìŒ")
                chunk_summary = "ê´€ë ¨ì„± ì—†ëŠ” ë°ì´í„°"
            else:
                chunk_summary = filter_result.chunk_summary
                if attempt > 0:
                    print(f"âœ… ì²­í¬ {chunk_idx + 1}: {len(important_artifacts)}ê°œ ë°œê²¬ (ì¬ì‹œë„ {attempt}íšŒ í›„ ì„±ê³µ)")
                else:
                    print(f"âœ… ì²­í¬ {chunk_idx + 1}: {len(important_artifacts)}ê°œ ë°œê²¬")
            
            return ChunkAnalysisResult(
                important_artifacts=important_artifacts,
                chunk_summary=chunk_summary
            )
        
        except Exception as e:
            if attempt < max_retries:
                print(f"âš ï¸  ì²­í¬ {chunk_idx + 1}: {type(e).__name__} (ì¬ì‹œë„ {attempt + 1}/{max_retries})...")
                time.sleep(1)
                continue
            else:
                print(f"âŒ ì²­í¬ {chunk_idx + 1}: ìµœëŒ€ ì¬ì‹œë„ í›„ ì‹¤íŒ¨ - {str(e)}")
                return ChunkAnalysisResult(
                    important_artifacts=[],
                    chunk_summary=f"ì˜¤ë¥˜: {type(e).__name__}"
                )
