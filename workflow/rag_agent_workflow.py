from typing import Any, Dict

from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode


# ğŸ†• V2: workflow_v2ì˜ í•„í„°ë§ ë¡œì§ ë° í´ë˜ìŠ¤ import
from workflow.filter_node import (
    recursive_filter_node,
    should_continue_filtering
)
from workflow.classes import AgentState, ScenarioCreate
from workflow.database import save_data_node
from workflow.requirements_node import analyze_requirements_node
from workflow.tools import agent_tools, ToolContext, get_metadata_info, format_metadata_section
from workflow.prompts import AGENT_SYSTEM_PROMPT, SCENARIO_GENERATOR_SYSTEM_PROMPT, CLASSIFY_PROMPT
from workflow.utils import llm_large, llm_small

# --------------------------------------------------------------------------
# LLM ë° ë„êµ¬ ì„¤ì •
# --------------------------------------------------------------------------
llm_with_tools = llm_large.bind_tools(agent_tools)

# --------------------------------------------------------------------------
# ê·¸ë˜í”„ ë…¸ë“œ(Nodes) ì •ì˜
# --------------------------------------------------------------------------

def extract_filtered_artifacts(state: AgentState) -> Dict:
    """
    ì¬ê·€ í•„í„°ë§ ì™„ë£Œ í›„, intermediate_resultsì—ì„œ filtered_artifactsë¥¼ ì¶”ì¶œí•˜ê³ 
    ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•´ ë¶ˆí•„ìš”í•œ ì›ë³¸ ë°ì´í„°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
    """
    print("--- ğŸ“¦ Node: í•„í„°ë§ ê²°ê³¼ ì¶”ì¶œ ë° ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘... ---")
    
    # 1. í•„í„°ë§ëœ ì•„í‹°íŒ©íŠ¸ ì¶”ì¶œ
    filtered = []
    for result in state.get("intermediate_results", []):
        filtered.extend(result.important_artifacts)
    
    print(f"  âœ… ì´ {len(filtered)}ê°œ ì•„í‹°íŒ©íŠ¸ ì¶”ì¶œ")
    
    # 2. ë©”ëª¨ë¦¬ ìµœì í™”: ë” ì´ìƒ í•„ìš” ì—†ëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„° ì •ë¦¬
    # - artifact_chunks: ì›ë³¸ ì•„í‹°íŒ©íŠ¸ ì²­í¬ (í•„í„°ë§ ì™„ë£Œ í›„ ë¶ˆí•„ìš”)
    # - intermediate_results: ì¤‘ê°„ ê²°ê³¼ (ì´ë¯¸ filtered_artifactsë¡œ ì¶”ì¶œ)
    
    chunks_count = sum(len(chunk) for chunk in state.get("artifact_chunks", []))
    
    print(f"  ğŸ—‘ï¸  ë©”ëª¨ë¦¬ ì •ë¦¬:")
    print(f"     - artifact_chunks ì œê±°: {chunks_count:,}ê°œ")
    print(f"     - intermediate_results ì œê±°: {len(state.get('intermediate_results', []))}ê°œ ì²­í¬")
    
    print(f"--- âœ… Node: ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ (í•„í„°ë§ëœ {len(filtered)}ê°œë§Œ ìœ ì§€) ---")
    
    return {
        "filtered_artifacts": filtered,
        # ë©”ëª¨ë¦¬ ìµœì í™”: ë¶ˆí•„ìš”í•œ ë°ì´í„° ëª…ì‹œì ìœ¼ë¡œ ì œê±°
        "artifact_chunks": [],
        "intermediate_results": [],
    }

def agent_reasoner(state: AgentState) -> Dict:
    """
    (ì›Œí¬í”Œë¡œìš° 3ë‹¨ê³„) ì—ì´ì „íŠ¸ì˜ ììœ¨ì  ì¶”ë¡  ë° í–‰ë™ ê²°ì •
    
    ì—­í• :
    - ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ì •ë³´ ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½
    - ë„êµ¬ë¥¼ ììœ¨ì ìœ¼ë¡œ ì„ íƒí•˜ì—¬ ì‹¤í–‰ (query_planner, artifact_search, web_search)
    - ë„êµ¬ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ì¶”ê°€ ì¡°ì‚¬ í•„ìš” ì—¬ë¶€ íŒë‹¨
    - ì¶©ë¶„í•œ ì •ë³´ ìˆ˜ì§‘ ì‹œ ìµœì¢… ë³´ê³ ì„œ ìƒì„± ê²°ì •
    
    íë¦„:
    1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸(AGENT_SYSTEM_PROMPT) + ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ ë¡œë“œ
    2. LLMì´ ë‹¤ìŒ í–‰ë™ ê²°ì • (ë„êµ¬ í˜¸ì¶œ ë˜ëŠ” ë³´ê³ ì„œ ìƒì„±)
    3. ë©”ì‹œì§€ë¥¼ stateì— ì¶”ê°€ (ë‹¤ìŒ ë…¸ë“œë¡œ ì „ë‹¬)
    """
    print("--- ğŸ¤” Agent: ì¶”ë¡  ë° í–‰ë™ ê²°ì • ì¤‘... ---")
    
    # 0. ë„êµ¬ ì»¨í…ìŠ¤íŠ¸ ì„¤ì • (ë„êµ¬ê°€ State ì •ë³´ì— ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡)
    collection_name = state.get("collection_name") or "artifacts_collection"
    db_config = state.get("db_config")
    ToolContext.set_context(collection_name, db_config)
    
    # 1. ë©”ì‹œì§€ êµ¬ì„±
    existing_messages = state.get("messages", [])
    messages_to_invoke = list(existing_messages) # type: ignore

    if not messages_to_invoke:
        # ì²« ì‹¤í–‰: ì‹œìŠ¤í…œ ë©”ì‹œì§€ + ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„±
        user_requirements = state.get("analyzed_user_requirements", "ì¼ë°˜ì ì¸ ì •ë³´ìœ ì¶œ ë¶„ì„ì„ ì§„í–‰í•´ ì£¼ì„¸ìš”.")

        metadata_info = get_metadata_info(collection_name, db_config)
        context_info = format_metadata_section(metadata_info)

        messages_to_invoke = [
            SystemMessage(content=AGENT_SYSTEM_PROMPT.format(context_info=context_info)),
            HumanMessage(content=user_requirements or "ì •ë³´ìœ ì¶œ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„")
        ]


    else:
        # ì´í›„ ì‹¤í–‰: ê¸°ì¡´ íˆìŠ¤í† ë¦¬ë§Œ ì‚¬ìš©
        last_message = messages_to_invoke[-1]

        # ToolMessage ì´í›„ì—ëŠ” continuation_promptë¥¼ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
        # (ë„êµ¬ ê²°ê³¼ê°€ ì´ë¯¸ ë©”ì‹œì§€ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ LLMì´ ìë™ìœ¼ë¡œ ë¶„ì„í•¨)
        if isinstance(last_message, AIMessage) and not last_message.tool_calls:
            continuation_prompt = "ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì§„í–‰í•˜ì„¸ìš”. ëª¨ë“  ì •ë³´ê°€ ìˆ˜ì§‘ë˜ì—ˆë‹¤ê³  íŒë‹¨ë˜ë©´ ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê² ë‹¤ê³  ëª…í™•íˆ ì–¸ê¸‰í•´ì£¼ì„¸ìš”."
            messages_to_invoke.append(HumanMessage(content=continuation_prompt))

    # 2. LLM í˜¸ì¶œ
    print(f"  ğŸ“¨ ë©”ì‹œì§€ ê°œìˆ˜: {len(messages_to_invoke)}ê°œ")
    
    try:
        response = llm_with_tools.invoke(messages_to_invoke)

        # 3. ì‘ë‹µ ë¶„ì„ ë° ë¡œê¹…
        tool_calls = getattr(response, 'tool_calls', None)
        if tool_calls:
            print(f"  ğŸ”§ ë„êµ¬ í˜¸ì¶œ: {len(tool_calls)}ê°œ")
            for tool_call in tool_calls:
                print(f"     - {tool_call.get('name', 'unknown')}")
        else:
            content = getattr(response, 'content', '')
            if content:
                # ì²« 100ìë§Œ ì¶œë ¥ (ê°„ê²°í•˜ê²Œ)
                preview = content[:100] + "..." if len(content) > 100 else content
                print(f"  ğŸ’­ ì¶”ë¡ : {preview}")
            else:
                print(f"  âš ï¸  ê²½ê³ : LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
            
            # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            if "ì¶©ë¶„í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤" in content or "ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤" in content:
                print("  âœ… ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ â†’ ë³´ê³ ì„œ ìƒì„± ë‹¨ê³„ë¡œ ì´ë™")
        
        print("--- âœ… Agent: ì¶”ë¡  ì™„ë£Œ ---")
        if len(messages_to_invoke) == 2:
            return {"messages": messages_to_invoke + [response]}
        return {"messages": [response]}
        
    except Exception as e:
        print(f"  âŒ ì—ì´ì „íŠ¸ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ "ì¶©ë¶„í•œ ì •ë³´" í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŒ (ê°€ì§œ ë³´ê³ ì„œ ìƒì„± ë°©ì§€)
        error_response = AIMessage(
            content=f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\në°ì´í„°ë² ì´ìŠ¤ ì ‘ê·¼ ë˜ëŠ” LLM í˜¸ì¶œì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        )
        return {"messages": [error_response], "analysis_failed": True}

def scenario_generator(state: AgentState) -> Dict:
    """
    (ì›Œí¬í”Œë¡œìš° 4ë‹¨ê³„) ëˆ„ì ëœ ëª¨ë“  ì •ë³´(History, ê²€ìƒ‰ ê²°ê³¼)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    ì—­í• :
    - ëŒ€í™” íˆìŠ¤í† ë¦¬ì—ì„œ ìˆ˜ì§‘ëœ ëª¨ë“  ì •ë³´ ë¶„ì„
    - ì •ë³´ìœ ì¶œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹œê°„ìˆœìœ¼ë¡œ ì¬êµ¬ì„±
    - ScenarioCreate ê°ì²´ë¡œ êµ¬ì¡°í™”ëœ ë³´ê³ ì„œ ìƒì„±
    
    ì¶œë ¥:
    - name: ì‹œë‚˜ë¦¬ì˜¤ ì œëª©
    - description: ì‹œë‚˜ë¦¬ì˜¤ ìš”ì•½
    - steps: 5-10ê°œì˜ ë‹¨ê³„ë³„ í–‰ìœ„ ì„¤ëª… (ì‹œê°„ìˆœ)
    """
    print("--- ğŸ“ Node: ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì¤‘ ---")
    
    # 1. ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
    messages = state.get("messages") or []
    job_id = state.get("job_id", "unknown")
    task_id = state.get("task_id", "unknown")
    analysis_failed = state.get("analysis_failed", False)
    
    print(f"  ğŸ“Š ë¶„ì„ ëŒ€ìƒ: {len(messages)}ê°œ ë©”ì‹œì§€")
    
    # 2. ì˜¤ë¥˜ ìƒíƒœ í™•ì¸ (ê°€ì§œ ë³´ê³ ì„œ ìƒì„± ë°©ì§€)
    if analysis_failed:
        print("  âš ï¸  ë¶„ì„ ì‹¤íŒ¨ ìƒíƒœ - ì˜¤ë¥˜ ë³´ê³ ì„œ ìƒì„±")
        error_report = ScenarioCreate(
            name="ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨",
            description=(
                "ë°ì´í„°ë² ì´ìŠ¤ ì ‘ê·¼ ë˜ëŠ” LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ "
                "ì •ë³´ìœ ì¶œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
                "ì‹œìŠ¤í…œ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
            ),
            job_id=job_id,
            task_id=task_id,
            report_detail_id="analysis_failed",
            steps=[]
        )
        return {"final_report": error_report}
    
    # 3. ë„êµ¬ ì‹¤í–‰ ê²€ì¦ (ìµœì†Œ ê²€ìƒ‰ íšŸìˆ˜ í™•ì¸)
    tool_messages = [m for m in messages if hasattr(m, 'name') and getattr(m, 'name', None)]
    if len(tool_messages) == 0:
        print("  âš ï¸  ë„êµ¬ ì‹¤í–‰ ì—†ìŒ - ë°ì´í„° ë¶€ì¡± ë³´ê³ ì„œ ìƒì„±")
        no_search_report = ScenarioCreate(
            name="ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ",
            description=(
                "ë°ì´í„° ê²€ìƒ‰ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì ‘ê·¼ ê¶Œí•œì´ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            ),
            job_id=job_id,
            task_id=task_id,
            report_detail_id="no_search",
            steps=[]
        )
        return {"final_report": no_search_report}
    
    print(f"  ğŸ” ë„êµ¬ ì‹¤í–‰ íšŸìˆ˜: {len(tool_messages)}ê°œ")
    
    # 4. LLM í˜¸ì¶œ (structured output)
    try:
        structured_llm = llm_large.with_structured_output(ScenarioCreate)
        
        # ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ë©”ì‹œì§€ êµ¬ì„±
        scenario_messages = [
            SystemMessage(content=SCENARIO_GENERATOR_SYSTEM_PROMPT),
            HumanMessage(content=f"ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •ë³´ìœ ì¶œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìƒì„±í•˜ì„¸ìš”.\n\në„êµ¬ ì‹¤í–‰ íšŸìˆ˜: {len(tool_messages)}ê°œ"),
            *messages  # ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬í•¨
        ]
        
        result = structured_llm.invoke(scenario_messages)
        
        # Pydantic ëª¨ë¸ë¡œ ë³€í™˜ (dictë¡œ ë°˜í™˜ë  ìˆ˜ ìˆìŒ)
        report: ScenarioCreate
        if isinstance(result, dict):
            report = ScenarioCreate(**result)
        else:
            report = result  # type: ignore
        
        # job_id, task_id ì—…ë°ì´íŠ¸
        report = report.model_copy(update={"job_id": job_id, "task_id": task_id})
        
        print(f"  âœ… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ:")
        print(f"     - ì œëª©: {report.name}")
        print(f"     - ë‹¨ê³„ ìˆ˜: {len(report.steps)}ê°œ")
        
        return {"final_report": report}
        
    except Exception as e:
        print(f"  âŒ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ë³´ê³ ì„œ ë°˜í™˜
        fallback_report = ScenarioCreate(
            name="ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨",
            description=f"LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            job_id=job_id,
            task_id=task_id,
            report_detail_id="llm_error",
            steps=[]
        )
        return {"final_report": fallback_report}

def classify_data(state: AgentState) -> Dict:
    """
    (ì›Œí¬í”Œë¡œìš° 4ë‹¨ê³„) ëˆ„ì ëœ ë©”ì‹œì§€ ì •ë³´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‚˜ì˜¨ ë§ˆì§€ë§‰ ê²°ë¡ ì„ í–‰ìœ„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    """
    print("--- ğŸ“ Node: ë°ì´í„° ë¶„ë¥˜ ì¤‘ ---")
    messages = state.get("messages") or []
    analysis_failed = state.get("analysis_failed", False)

    if analysis_failed:
        context = ""
        return {"context": context}
    
    try:
        response = llm_large.invoke([HumanMessage(content=CLASSIFY_PROMPT), *messages])
        context = response.content
        return {"context": context}
    except Exception as e:
        print(f"  âŒ ë¶„ë¥˜ ë°ì´í„° ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        context = ""
        return {"context": context}


# --------------------------------------------------------------------------
# 5. ì œì–´ íë¦„(Router) ì •ì˜
# --------------------------------------------------------------------------

# ë°ì´í„° ì €ì¥ ê²°ê³¼ì— ë”°ë¼ ë¶„ê¸°í•  ë¼ìš°í„°
def check_save_status(state: AgentState) -> str:
    """data_save_status í•„ë“œì˜ ê°’ì„ í™•ì¸í•˜ì—¬ 'success' ë˜ëŠ” 'failure'ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    status = state.get("data_save_status")
    return status if status else "failure"

def router(state: AgentState) -> str:
    """ì—ì´ì „íŠ¸ì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ ë³´ê³  ë‹¤ìŒì— ì‹¤í–‰í•  ë…¸ë“œë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    messages = state.get("messages", [])
    if not messages:
        return "continue"
    
    # ì˜¤ë¥˜ ìƒíƒœ í™•ì¸ (agent_reasonerì—ì„œ ì„¤ì •)
    if state.get("analysis_failed"):
        print("  âš ï¸  ë¶„ì„ ì‹¤íŒ¨ ìƒíƒœ ê°ì§€ - ì˜¤ë¥˜ ë³´ê³ ì„œ ìƒì„±")
        return "generate_scenario"
    
    # ë¬´í•œ ë£¨í”„ ë°©ì§€: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì²´í¬
    ai_messages = [m for m in messages if isinstance(m, AIMessage)]
    max_iterations = 20
    if len(ai_messages) >= max_iterations:
        print(f"  âš ï¸  ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜({max_iterations}) ë„ë‹¬ - ë³´ê³ ì„œ ìƒì„±")
        return "generate_scenario"
    
    last_message = messages[-1]
    
    # AIMessageì— tool_callsê°€ ìˆìœ¼ë©´ ë„êµ¬ ì‹¤í–‰
    tool_calls = getattr(last_message, 'tool_calls', None)
    if tool_calls:
        return "tools"
    
    # AIMessageì˜ contentë¥¼ ë³´ê³  ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì—¬ë¶€ íŒë‹¨
    content = getattr(last_message, "content", "") or ""
    if not content:
        return "continue"

    prompt_text = (
        "ì•„ë˜ ai messageë¥¼ ë³´ê³  ìµœì¢… ë³´ê³ ì„œ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ True or Falseë¡œ íŒë‹¨í•˜ì„¸ìš”.\n"
        "[ai_message]\n"
        f"{content}\n\n"
        "ì‘ë‹µì€ ë°˜ë“œì‹œ True ë˜ëŠ” Falseë§Œ ë°˜í™˜í•˜ì„¸ìš”."
    )

    structured_llm = llm_small.with_structured_output(bool)
    result = structured_llm.invoke([HumanMessage(content=prompt_text)])

    if check_is_done(result):
        return "generate_scenario"
    
    # ê¸°ë³¸ì ìœ¼ë¡œ ê³„ì† ìƒê°
    return "continue"

def check_is_done(result: bool | dict | Any) -> bool:
    is_done = False
    if isinstance(result, bool):
        is_done = result
    elif isinstance(result, dict):
        bool_vals = [v for v in result.values() if isinstance(v, bool)]
        if bool_vals:
            is_done = bool_vals[0]
        else:
            is_done = str(result).strip().lower() in ("true", "yes", "1")
    else:
        is_done = str(result).strip().lower() in ("true", "yes", "1")
    return is_done

# --------------------------------------------------------------------------
# ê·¸ë˜í”„ êµ¬ì„± ë° ì»´íŒŒì¼
# --------------------------------------------------------------------------

# ë„êµ¬ ë…¸ë“œ ìƒì„± (agent_toolsëŠ” tools.pyì—ì„œ importë¨)
tool_node = ToolNode(agent_tools)

# ê·¸ë˜í”„ ì›Œí¬í”Œë¡œìš° ì •ì˜
workflow = StateGraph(AgentState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("filter_artifacts", recursive_filter_node)  # ì•„í‹°íŒ©íŠ¸ í•„í„°ë§
workflow.add_node("extract_results", extract_filtered_artifacts)  # í•„í„°ë§ ê²°ê³¼ ì¶”ì¶œ
workflow.add_node("save_data", save_data_node)  # ë°ì´í„° ì €ì¥
workflow.add_node("analyze_requirements", analyze_requirements_node)  # ìš”êµ¬ì‚¬í•­ ë¶„ì„
workflow.add_node("agent_reasoner", agent_reasoner)  # ì—ì´ì „íŠ¸ ì¶”ë¡ 
workflow.add_node("execute_tools", tool_node)  # ë„êµ¬ ì‹¤í–‰
workflow.add_node("generate_scenario", scenario_generator)  # ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
workflow.add_node("classify_results", classify_data)  # ê²°ê³¼ ë¶„ë¥˜

# ì—£ì§€(ì—°ê²° íë¦„) ì„¤ì •
workflow.set_entry_point("filter_artifacts")

# ğŸ†• ì¬ê·€ í•„í„°ë§ ì¡°ê±´ë¶€ ì—£ì§€
workflow.add_conditional_edges(
    "filter_artifacts",
    should_continue_filtering,
    {
        "continue": "filter_artifacts",  # í•„í„°ë§ ë°˜ë³µ
        "synthesize": "extract_results"  # ë‹¤ìŒ ë‹¨ê³„ë¡œ
    }
)

workflow.add_edge("extract_results", "save_data")  # ì•„í‹°íŒ©íŠ¸ ì¶”ì¶œ í›„ ë°ì´í„° ì €ì¥

# [ìˆ˜ì •] ë°ì´í„° ì €ì¥ í›„, ê²°ê³¼ì— ë”°ë¼ ë¶„ê¸°í•˜ëŠ” ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€
workflow.add_conditional_edges(
    "save_data",
    check_save_status,
    {
        "success": "analyze_requirements",  # ì„±ê³µ ì‹œ ìš”êµ¬ì‚¬í•­ ë¶„ì„
        "failure": END                      # ì‹¤íŒ¨ ì‹œ ì›Œí¬í”Œë¡œìš° ì¦‰ì‹œ ì¢…ë£Œ
    }
)

workflow.add_edge("analyze_requirements", "agent_reasoner")  # ìš”êµ¬ì‚¬í•­ ë¶„ì„ í›„ ì—ì´ì „íŠ¸ ì‹œì‘

workflow.add_conditional_edges(
    "agent_reasoner",
    router,
    {
        "tools": "execute_tools",
        "generate_scenario": "generate_scenario",
        "continue": "agent_reasoner", # ê³„ì† ìƒê°
    }
)
workflow.add_edge("execute_tools", "agent_reasoner") # ë„êµ¬ ì‚¬ìš© í›„ ë‹¤ì‹œ ìƒê°
workflow.add_edge("generate_scenario", "classify_results") 
workflow.add_edge("classify_results", END) 

# ê·¸ë˜í”„ ì»´íŒŒì¼
app = workflow.compile()

print("âœ… Graph compiled successfully!")